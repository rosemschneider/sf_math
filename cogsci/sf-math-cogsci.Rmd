---
title: "Sources of knowledge in children's acquisition of the successor function"
bibliography: citations.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf } \\  \\  \\ }

abstract: 
    "The successor function -- a recursive function *S* which states that for any natural number *n*, *S(n)* = *n*+1 -- underlies our understanding of the natural numbers as an infinite class. Recent work has found that acquisition of this logical property is surprisingly protracted, occuring several years after children master the counting procedure. While such work links successor knowledge with counting mastery, the exact processes underlying this developmental transition remain unclear. Here, we examined two hypothesized mechanisms: (1) productive counting knowledge, or mastery of the recursive process through which number words are generated, and (2) formally trained arithmetic, specifically the ``+1'' operation. We find that while both productive counting and arithmetic fact mastery predict successor knowledge, mean arithmetic performance was significantly lower for all children, even those at ceiling in implementing the successor function This surprising dissociation suggests children do not generalize the successor function from formally trained mathematics; rather, we find evidence consistent with the hypothesis that successor knowledge is supported by extraction of recursive counting rules."
    
keywords:
    "Number; language; cognitive development"
    
output: cogsci2016::cogsci_paper 
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
require("knitr")
# opts_knit$set(root.dir = "~/Documents/Projects/sf_math/") #this is specific to RMS, change accordingly
library(tidyverse)
library(magrittr)
library(langcog)
library(lme4)
library(stringr)
library(RColorBrewer)
library(ggthemes)

'%!in%' <- function(x,y)!('%in%'(x,y))
```

# Introduction 

One of the most fundamental challenges in learning is extracting a set of limitlessly productive rules from limited experiences. While humans are remarkably adept learners across a multitude of domains, a powerful application of this prodigious capacity is in the acquisition of symbolic number. Although linguistic number input and expression are decidedly finite, nevertheless we know the natural numbers to be *infinite*; we recognize that for every number, its successor can be obtained by simply adding '1,' and even intuitively know that *bajillion-two* follows *bajillion-one*, even though neither words indicate actual numbers. How does such an understanding arise, and what are the causal mechanisms underlying young learners' ability to acquire such an infinitely productive rule from their extremely finite early number input? In the present study, we test two hypothesized sources of knowledge in children's acquisition of the successor function, a logical property of number which states that for every natural number *n*, its successor is defined as *n*+1.

That such an infinitely generative function should exist is far from obvious in both early linguistic number input [@rule2015; @willits2016], as well as children's earliest expressions of number [@fuson1988]. Children's early exposure to the count list is extremely limited, and although most US children are able to recite a portion of the count list by the age of two, it is treated similarly to the "*ABCs.*" That is, children at this age treat the count list as an "unbreakable chain" which must be recited in its entirety, and which lacks any recursive structure. From the ages of about 2 to 3.5 years, children seem to learn the count list independently of the meanings of number words; although most US children have learned the meanings of *one*, *two*, and *three* by about 3.5 years, they do not yet understand the connection between counting and the cardinal meanings of number words [@wynn1990; @wynn1992]. With the acquisition of *four*, however, children's understanding of cardinality and counting seem to finally come into alignment, in that they suddenly seem to understand how the last word said while counting a set indicates the cardinality of that set. 

On some accounts, [@carey2004; @carey2009], children's acquisition of this *Cardinal Principle* (CP, @gelman1978) is accomplished by an analogical mapping: children notice that both the count list and the cardinalities it represents both differ by exactly *one*, leading them to hypothesize that this may hold true for other numbers as well. Critically, such an account of CP-acqusition implicates the successor function, in that children realize that any number's successor is exactly *one* greater than the current number.

@sarnecka2008 provided support for such an account of CP-acquisition using a paradigm called the 'Unit Task.' In this task, children are told a set of *N* objects are in a box, and then observe the addition of 1 item, with the critical question "Are there *N*+1 or *N*+2 in the box now?" To correctly answer this question, children must understand that the addition of one item to an established cardinality necessitates moving up one number in the count list. Consistent with the hypothesis that children acquire the CP through an induction of the successor function, Sarnecka \& Carey found that CP-knowers exhibited significantly greater Unit Task performance in comparison to subset-knowers (children who have not yet acquired the CP). 

Subsequent work has found that, while CP-knowers demonstrate some understanding of the successor function on the Unit Task, such knowledge seems to be only available for numbers to which the child can accurately count. @davidson2012 and @cheung2017 classified CP-knowers according to their counting proficiency, and found that only the most competent counters were able to successfully implement the successor function for all numbers within their count range. In contrast, less able counters performed at chance on the Unit Task, even for numbers to which they were able to count. In conjunction with other recent work which shows a gap between CP knowledge and generalized successor knowledge [@spaepen2018], these results indicate that acquiring the successor function occurs significantly after learning the CP, perhaps by several years [@cheung2017]. 

In the intervening years between learning the CP and acquiring the successor function (between the ages of about 4 and 6), children encounter increasing numerical input and experiences. They continue to master the count routine, and at some point are able to extract recursive counting rules which enable them to count increasingly high. At the same time, children are beginning formal education, and are trained on arithmetic facts such as *5+1=6,* *6+1=7*, and so on. 

@cheung2017 proposed that children may leverage such new and developing numerical knowledge in acquiring the successor function. Specifically, Cheung and colleagues present both recursive counting knowledge and trained "math facts" mastery as possible paths to successor knowledge. On the first alternative, which we will refer to as "Productive Counting," as children master the recursive base-structure of number, they notice that the next number in the base is generated through implementing the successor function. Thus, such an account would predict that children who demonstrate Productive Counting knowledge should be more likely to exhibit generalized successor knowledge. 

The second alternative, which we will refer to as "Math Facts," hypothesizes that the successor function is not inferred, but trained; as children learn arithmetic operations such as *4+1=5,* they may hypothesize that this trained "+1" operation holds for *any* number. Therefore, the "Math Facts" hypothesis predicts that children's successor knowledge should be significantly predicted by their mastery of the "+1" operation. Importantly, this hypothesis also predicts that children at ceiling on the successor task should be similarly at ceiling in solving addition problems with "+1."

In the present work, we test these two hypothesized causal mechanisms in successor function acquisition in a large group of 3.5 -- 6-year-old CP-knowers. We use both children's counting proficiency and ability to generate the successor for an arbitrarily high number to assess their understanding of Productive Counting, and their mean performance on a set of verbally presented arithmetic problems (e.g., "What is 5+1?") to measure their familiarity with trained "Math Facts." In a set of model comparisons predicting Unit Task performance, we find support for both Productive Counting and Math Facts as possible sources of successor knowledge. In a *post-hoc* analysis with children at ceiling on the Unit Task, however, we find significantly lower arithmetic performance in comparison to a recursive counting measure. These findings suggest that, although children who have acquired the successor function are also likely to have received some formal arithmetic training, it may not play a causal role in helping children generalize this logical property. 

```{r setup}
#load data
data.raw <- read.csv("~/Documents/Projects/sf_math/Data/sf_math_data.csv")%>%
  filter(SID != "CopyPasteMe", 
         SID != "?")%>%
  droplevels()%>%
  filter(Correct != "HELP")%>% #temporary while we resolve helps
  mutate(Age = as.numeric(as.character(Age)),
         Correct = as.integer(as.character(Correct)))%>%
  mutate(Age = round(Age, 2), 
         Agegroup = cut(Age, breaks = c(3.49, 4, 4.5, 5, 5.5, 6), 
                        labels = c("3.5-4", "4-4.5", "4.5-5", 
                                   "5-5.5", "5.5-6")))%>% #add agegroups
  dplyr::select(-Response_single, -Response_double)%>% #remove double coding
  dplyr::rename(Response = Response_final) #rename for code

hc.df <- read.csv("~/Documents/Projects/sf_math/Data/sf_math_hc.csv")%>%
  dplyr::select(-IHC_single, - FHC_single, -Special_count, -Notes, -RMS.note)%>%
  filter(Exclude_trial != 1, 
         IHC_final != "HELP", 
         FHC_final != "HELP")%>%
  dplyr::rename(FHC = FHC_final, 
                IHC = IHC_final)%>%
  filter(!is.na(FHC), 
         !is.na(IHC))%>%
  mutate(IHC = ifelse(as.integer(as.character(IHC)) > 120, 120, as.integer(as.character(IHC))), 
         FHC = ifelse(as.integer(as.character(FHC)) > 120, 120, as.integer(as.character(FHC)))) #add cap to IHC and FHC
```

```{r productive, warning = FALSE}
#classify children as productive or nonproductive
hc <- hc.df %>% 
  dplyr::select(SID, Last_successful, IHC, FHC) %>%
  mutate_at(c('Last_successful','IHC','FHC'),
            function(col) as.integer(str_replace_all(col,'\\D',''))) %>% 
  mutate(Last_successful = ifelse(is.na(Last_successful), 120, Last_successful))%>%
  mutate(SID = as.character(SID))
# 
# 
is.productive = function(subject){
  # takes as input the data for a single subject
  # RULES:
  # - counts to 120 unaided = productive
  # - after making first error, counts >= 20 higher, with no more than 3 errors on way
  if(subject$IHC[1] >= 120){
    # if they get to 120 on first try, = productive
    return("Productive")
  } else if(subject$FHC[1] == 120 & nrow(subject) < 4) {
    return("Productive")
  } else if(subject$FHC[1] < 120 & nrow(subject) == 1 
            & subject$FHC[1] == subject$IHC[1]) {
    return("Nonproductive")
  } else if((subject$FHC[1] - subject$IHC[1]) >= 20){
    # if their final is >= 20 larger than their intial...
    if(nrow(subject) < 4){
      # and they've made 3 or fewer total errors, = productive
      return("Productive")
    } 
    else {
      for(i in 1:nrow(subject)){ # start at row 2
        # check if they ever made it >= 20 counts & <= 3 errors after an error
        runLength = 0 # they just made an error, so no post-error successes yet
        numErrors = 0 # first row was an error if it's not finalCount == 120
        prev = subject$Last_successful[i]
        for (j in i+1:nrow(subject)){ # from current row until end...
          numErrors = numErrors + 1 # new row means new error
          runLength = runLength + (subject$Last_successful[j] - prev)
          # ^ add difference between current count and last count to run length
          prev = subject$Last_successful[j] # update last count
          if(runLength >= 20 & numErrors < 4){
            # if at any point the productivity conditions are met...
            return("Productive") # = productive
          }
        }
      }
      # productivity conditions were never met (because we got to this point) so...
      return("Nonproductive") # != productive
    }
  } else {
    # highest is not >= 20 greater than initial
    return("Nonproductive")
  }
}

# 
#make function to run for all participants
unique_SIDs <- as.vector(unique(hc.df$SID))
# 
class_prod <- function(vector) {
  temp_data <- data.frame()
  for (i in vector) {
    prod.class <- data.frame(i, is.productive(subset(hc, SID == i)))
    # print(i) # for debugging
    names(prod.class) <- c("SID", "productive")
    prod.class %<>%
      mutate(SID = as.character(SID), 
             productive = as.character(productive))
    temp_data <- bind_rows(temp_data, prod.class)
  }
  return(temp_data)
}
# 

#get productive classification for every participant
productive <- class_prod(unique_SIDs)%>%
  dplyr::rename(Productive = productive)

#remove last-successful from hc so you can add IHC and FHC to data.raw
hc %<>%
  dplyr::select(-Last_successful)

#add productive classifications
productive <- full_join(productive, hc, by = "SID")%>%
  distinct(SID, IHC, FHC, Productive)

#full join with raw data
data.raw <- full_join(data.raw, productive, by = "SID") 

#made SID and Productive factors again #MSaPFA
data.raw %<>%
  mutate(SID = factor(SID), 
         Productive = factor(Productive))
```

```{r knower_level}
cp.df <- data.raw %>%
  filter(Task == "GiveN")%>%
  group_by(SID)%>%
  dplyr::summarise(sum_correct = sum(Correct, na.rm = TRUE))%>%
  mutate(Knower.level = ifelse(sum_correct >= 4, "CP-knower", "Subset-knower"))%>%
  dplyr::select(-sum_correct)

data.raw <- full_join(data.raw, cp.df, by = "SID")
```

```{r hcnn, warning = FALSE}
#Get kids who failed NN for highest contiguous
failed.nn <- data.raw %>%
  filter(Task == "WCN", 
         Correct == 0, 
         Trial_number == "Training")

failed.nn.sids <- unique(as.vector(failed.nn$SID))

#get unique ids
unique.nn <- data.raw %>%
  filter(Task == "WCN")%>%
  distinct(SID)

unique.nn <- as.vector(unique.nn$SID)
nextnums <- as.vector(c(7, 26, 30, 62, 83, 95, 71, 24))

#this is a function that pulls out the largest number for which a participant had a correct consecutive
get_contiguous <- function(){
  contig <- data.frame()
  for (sub in unique.nn) {
    tmp <- data.raw %>%
      filter(Task == "WCN",
             SID == sub, 
             Correct == 0)%>%
      mutate(Task_item= as.integer(as.character(Task_item)))%>%
      mutate(Task_item = sort(as.integer(as.character(Task_item))))
    if (length(tmp$SID) == 0) {
      highest_contig = 95
      sub_contig <- data.frame(sub, highest_contig) 
      sub_contig %<>%
        mutate(sub = as.character(sub),
                highest_contig = as.character(highest_contig))
      contig <- bind_rows(contig, sub_contig)
    } else if (sub %in% failed.nn.sids) {
      highest_contig = 0
      sub_contig <- data.frame(sub, highest_contig) 
      sub_contig %<>%
        mutate(sub = as.character(sub),
                highest_contig = as.character(highest_contig))
      contig <- bind_rows(contig, sub_contig)
    } else if (length(tmp$Task_item) > 0 & min(as.integer(as.character(tmp$Task_item))) == 7) {
      highest_contig = 1
      sub_contig <- data.frame(sub, highest_contig)
      sub_contig %<>%
        mutate(sub = as.character(sub),
                highest_contig = as.character(highest_contig))
      contig <- bind_rows(contig, sub_contig)
    } else {
      min.nn <- min(as.integer(as.character(tmp$Task_item)))
      prev_correct <- nextnums[nextnums < min.nn]
      highest_contig <- max(prev_correct)
    
      sub_contig <- data.frame(sub,
                             highest_contig) 
      sub_contig %<>%
        mutate(sub = as.character(sub),
                highest_contig = as.character(highest_contig))
      contig <- bind_rows(contig, sub_contig)
    }
  }
  contig %<>%
    mutate(highest_contig = as.character(highest_contig))
  return(contig)
}

highest_contiguous_nn <- get_contiguous()%>%
  dplyr::rename(SID = sub)

#add this to df 
data.raw <- full_join(data.raw, highest_contiguous_nn, by = "SID")

# #how many kids don't have a highest contiguous NN? 
# all.data %>%
#   filter(is.na(Language))
#   filter(is.na(highest_contig))%>%
#   distinct(Language, SID)%>%
#   group_by(Language)%>%
#   summarise(n = n())%>%
#   kable()

#Check - does anyone have NA for HCNN? 
# data.raw %>%
#   filter(is.na(highest_contig))%>%
#   filter(Exclude_analysis == 0)
```

```{r global_exclusions}
#how many kids in total
pre.excl <- data.raw %>%
  distinct(SID)%>%
  summarise(n = n())

#why are kids excluded
excl.reasons <- data.raw %>%
  filter(Exclude_analysis == 1)%>%
  distinct(SID, Exclude_analysis_reason)%>%
  group_by(Exclude_analysis_reason)%>%
  dplyr::summarise(n = n())%>%
  mutate(sum = sum(n))

#exclude these kids from analysis
all.data <- data.raw %>%
  filter(Exclude_analysis != 1)
```

```{r task_exclusions}
# #how many kids are excluded from which tasks
# all.data %>%
#   filter(Exclude_task == 1)%>%
#   distinct(SID, Exclude_task, Excluded_task, Exclude_task_reason)

#exclude
all.data %<>%
  filter(Exclude_task != 1)
```

```{r trial_exclude}
# #how many trials excluded, and for what reason
# all.data %>%
#   filter(Exclude_trial == 1)%>%
#   group_by(Task, Exclude_trial_reason)%>%
#   summarise(n = n())%>%
#   kable()

#exclude these trials
all.data %<>%
  filter(Exclude_trial != 1)
```

```{r training_exclude}
# #how many kids failed training
# all.data %>%
#   filter(Trial_number == "Training", 
#          Correct == 0)%>%
#   group_by(Task)%>%
#   summarise(n = n()) %>%
#   kable()

#filter out training trials 
all.data %<>%
  filter(Trial_number != "Training")

# #how many trials do we have for each task?
# all.data %>%
#   filter(Task == "SF" | 
#          Task == "WCN" |
#            Task == "MF")%>%
#   group_by(Task)%>%
#   summarise(n = n()) %>%
#   kable()
```

```{r within_outside}
#classify trials as within or outside count range
all.data %<>%
  mutate(count_range = ifelse((Task == "SF" | Task == "WCN" | Task == "MF") & as.numeric(as.character(Task_item)) <= IHC, "Within", "Outside")) %>%
  mutate(count_range = factor(count_range, levels = c("Within", "Outside")))
```

```{r hnn}
#highest next number
#Create a lookup table with the highest NN correctly answered
lookup <- all.data %>%
  filter(Task == "WCN")%>%
  filter(Correct == 1)%>%
  group_by(SID)%>%
  dplyr::summarise(max = max(as.integer(as.character(Task_item))))

no.corr.nn <- all.data %>%
  filter(Task == "WCN")%>%
  group_by(SID)%>%
  dplyr::summarise(mean = mean(Correct, na.rm = TRUE))%>%
  filter(mean == 0)

no.corr.nn.sids <- as.vector(unique(no.corr.nn$SID))

#Function that adds the highest NN to a participant's row in the SF dataframe
add_highest_num <- function(df) {
  tmp <- df
  for (row in 1:nrow(tmp)) {
    sub = as.character(tmp[row, "SID"])
    if (sub %in% no.corr.nn.sids) {
      highest_num = 0
      tmp[row, "highest_num"] = highest_num
    } else {
      highest_num = subset(lookup, SID == sub)$max
      tmp[row, "highest_num"] = highest_num
    }
  }
  return(tmp)
}

#run this function on SF dataframe
all.data <- add_highest_num(all.data)
```

```{r prod.gradient}
all.data %<>%
  mutate(delta.hc = FHC-IHC, 
         prod.gradient = delta.hc/(120-IHC), 
         prod.gradient = ifelse(IHC == 120 | IHC == 119, 1, as.numeric(prod.gradient)))
```

```{r mean_indef}
indef.mean <- all.data %>%
  filter(Task == "Indefinite")%>%
  group_by(SID)%>%
  dplyr::summarise(mean.indef = mean(Correct, na.rm = TRUE))

all.data <- full_join(all.data, indef.mean, by = "SID")
```

```{r mean_three}
mf.sum <- all.data %>%
  filter(Task == "MF")%>%
  group_by(SID)%>%
  dplyr::summarise(mean.mf = mean(Correct, na.rm = TRUE))

all.data <- full_join(all.data, mf.sum, by = "SID")

sf.mean <- all.data %>%
  filter(Task == "SF")%>%
  group_by(SID)%>%
  dplyr::summarise(mean.unit = mean(Correct, na.rm = TRUE))

all.data <- full_join(all.data, sf.mean, by = "SID")

nn.mean <- all.data %>%
  filter(Task == "WCN")%>%
  group_by(SID)%>%
  dplyr::summarise(mean.nn = mean(Correct, na.rm = TRUE))

all.data <- full_join(all.data, nn.mean, by = "SID")
```

# Method

This study was preregistered on OSF (\texttt{blinded link to be included here}), and all methodological and analytical choices were as preregistered, unless stated other in-text. A version-controlled repository of the data and analysis script for this study is available at (\texttt{blinded github link to be included here}).

##Participants
```{r demographics}
n.participants <- all.data %>%
  distinct(SID, Age)%>%
  dplyr::summarise(n = n(),
            mean_age = round(mean(Age), 2), 
            sd_age = round(sd(Age), 2))

n.sex <- all.data %>%
  distinct(SID, Sex)%>%
  filter(Sex == "F")%>%
  group_by(Sex)%>%
  dplyr::summarise(n = n())
```
We recruited 225 children between the ages of 3;6 and 5;11. Forty-four children were immediately excluded for not being CP-knowers during our Give-N screening; these children did not complete the rest of the tasks. Six additional children were excluded for failure to count to at least *two* in our counting measure, and did not participate in the remainder of the study. Thirty additional children were excluded due to experimenter error (*n* = 8), missing highest count recording (*n* = 6), failure to complete 80\% of trials (*n* = 4), being out of age range (*n* = 2), or for other reasons, including asking to stop (*n* = 10). 
After these exclusions, our final analyzable sample included 144 participants out of a planned sample of 150 (\emph{M} = `r n.participants$mean_age`, \emph{SD} = `r n.participants$sd_age`, \emph{N} female = `r n.sex$n`).  

##Procedure
Participants completed 5 tasks (Give-N, Highest Count with Prompts, Unit Task, Next Number, and Math Facts) in a fixed order. 

*Give-N*. Only children who had acquired the Cardinal Principle (CP) were included in this study. The experimenter presented children with a set of 10 plastic apples, bears, or bananas, and a plastic plate. Children were asked to place 6, 9, 7, or 5 objects (in that order) on the plate. After the child finished placing items in the plate, the experimenter asked ``Is that *N*? Can you please count it and check?'' If a child said that the number given was not the number requested, they were given the opportunity to fix the set. Children needed to correctly generate sets for all four requested quantities to be considered a CP-knower.

*Highest Count with Prompts.* This task assessed children's rote and productive counting knowledge. The experimenter introduced the game by saying, "This is a counting game. In this game, I want to you count as high as you can. Go ahead!" If a child made a mistake, or forgot the next number in the count list, the experimenter asked "What comes after *N*?" If the child was unable to correct their mistake, or still did not know the next number, the experimenter provided a prompt, saying "Actually, what comes after *N* is *N*+1. Can you keep counting?" Children were given a maximum of 12 such prompts; the maximum count in this task was 120, meaning that even children who required a prompt at every decade transition would be able to reach the highest number. 

If a child was unable to continue after receiving a prompt, or made an error, the task was ended. Children received a maximum of 3 prompts within a single decade, and could not make more than 3 consecutive errors (i.e., prompted counts separated by only one unprompted number).

*Unit Task*. This task was adapted from [@sarnecka2008]. 

(To be added - remainder of methods, in progress).

##Productivity measures

Different models of productive counting knowledge make different predictions about the best indicator of recursive counting knowledge [@hartnett1998; @rule2015; @yang2016], but an empirical comparison of these predictors and their relationship to the successor function has yet to be conducted. Thus, we preregistered four measures of interest: Initial Highest Count (IHC), the highest number to which a child is able to count prior to making an error; Final Highest Count (FHC), the highest number reached at the conclusion of the Highest Count task; Highest Contiguous Next Number (HCNN)\footnote{Although we initially preregistered simply the highest next number reached as a measure of productivity, we later adapted this measure to account for accuracy. We redefined this measure prior to collection and analysis of the complete dataset.}, the highest number for which a child is able to generate a successor, provided all previous items were correct; and a Productive/Nonproductive classification, a categorical distinction we derived, with a Productive counter defined as a child who is able to count at least two decades past an error in the Highest Count task without making more than three errors. 

The nonindependence of these measures, along with the fact that they are designed to capture the same underlying construct, motivated our decision to use model comparison in our primary analyses, which are described below. 

# Results

Our primary question in this work is whether children's acquisition of the successor function is motivated by recursive counting knowledge, or mastery of trained arithmetic operations. First, we explore children's counting behavior, and describe differences between children classified as Productive and Nonproductive counters. Then, we explore how differences in Productivity manifest across the Unit, Next Number, and Math Facts tasks. Finally, we turn to our critical question by constructing several models to test whether Unit Task performance is best predicted by (1) Productivity; (2) IHC; (3) FHC; (4) HCNN; or (5) mean "Math Facts" performance.

##Highest Count

```{r hc desc}
productivity.pal <- c("#666666","#00b8e6")

hc.table <- all.data %>%
  distinct(SID, Age, Productive, IHC, FHC)%>%
  group_by(Productive)%>%
  dplyr::summarise(n = n(),
            m_ihc = round(mean(IHC)), 
            sd_ihc = round(sd(IHC), 2), 
            median_ihc = round(median(IHC)), 
            m_fhc = round(mean(FHC)), 
            sd_fhc = round(sd(FHC), 2), 
            median_fhc = round(median(FHC)))

error.freq <- full_join(hc.df, productive, by = "SID")%>%
  filter(!is.na(Last_successful))%>%
  filter(Last_successful != 120)%>%
  mutate(Error_type = ifelse(Last_successful %% 10 == 9 , "Decade end", 
                             ifelse(Last_successful %% 10 == 0, "Decade beginning", "Mid-decade")))

#mean number of prompts by Productivity
mean.error <- error.freq %>%
  filter(Last_successful != 120)%>%
  group_by(SID, Productive)%>%
  dplyr::summarise(n = n())%>%
  group_by(Productive)%>%
  dplyr::summarise(mean_prompts = round(mean(n, na.rm = TRUE), 2),
            sd_prompts = round(sd(n, na.rm = TRUE), 2))

#predicting IHC from Productivity 
ihc.fhc.test <- all.data %>%
  distinct(SID, Productive, IHC, FHC)

ihc.ttest <- t.test(subset(ihc.fhc.test, Productive == "Productive")$IHC, subset(ihc.fhc.test, Productive == "Nonproductive")$IHC, 
       var.equal = TRUE)


#what kinds of errors?
mean_prompts.type <- error.freq %>%
  group_by(Productive, Error_type)%>%
  dplyr::summarise(n = n())%>%
  group_by(Productive)%>%
  mutate(total.n = sum(n), 
            prop = n/total.n)


# tab1 <- xtable::xtable(hc.table, 
#                       caption = "This table prints across one column.")
# 
# print(tab1, type="latex", comment = F, table.placement = "H")
```

\begin{table}[b]
\centering
\begin{tabular}{c c c c } 
 \hline
 Productivity & N & Mean IHC (SD) & Mean FHC (SD) \\
 \hline
 Productive & `r hc.table$n[2]` & `r hc.table$m_ihc[2]` (`r hc.table$sd_ihc[2]`) & `r hc.table$m_fhc[2]` (`r hc.table$sd_fhc[2]`)  \\
 Nonproductive & `r hc.table$n[1]` & `r hc.table$m_ihc[1]` (`r hc.table$sd_ihc[1]`) & `r hc.table$m_fhc[1]` (`r hc.table$sd_fhc[1]`) \\ 
 \hline
\end{tabular}
\caption{Highest Count performance by Productivity classification.}
\label{tab:hc}
\end{table} 

Using our Productivity classification, we identified 73 children as Productive counters, and the remaining 71 as Nonproductive counters. A summary of Initial and Final Highest Counts by Productivity is shown in Table \ref{tab:hc}. Productive counters had significantly higher IHCs (*t*(`r ihc.ttest$parameter`) = `r round(ihc.ttest$statistic, 2)`, *p* = .0003) than Nonproductive counters, indicating a higher overall level of counting mastery. Critically, Productive counters used more prompts than Nonproductive counters (an average of `r mean.error$mean_prompts[2]` v. `r mean.error$mean_prompts[1]`) to reach a higher FHC. Thus, although children received a prompt at every error, only Productive counters were able to leverage their knowledge of the base-system in order to continue counting.

Additionally, Productive counters made a higher proportion of their errors at decade-transitions (`r round(mean_prompts.type$prop[5], 2)`) than either mid-decade (`r round(mean_prompts.type$prop[6], 2)`) or at the beginning of a decade (`r round(mean_prompts.type$prop[4], 2)`), indicating that their counting difficulties stemmed from memorizing decade labels, rather than from the base-system. In contrast, Nonproductive counters made errors roughly equally at both decade transitions (`r round(mean_prompts.type$prop[2], 2)`) and mid-decade (`r round(mean_prompts.type$prop[3], 2)`). 

```{r initial_final, fig.pos = "t", fig.width=3.2, fig.height=3.2, fig.cap = "Scatterplot of Initial and Final Highest Counts by Productivity classification. Points are jittered slightly to avoid overplotting."}
initial_final <- all.data %>%
  filter(!is.na(Productive))%>%
  distinct(SID, IHC, FHC, Productive, prod.gradient)%>%
  mutate(IHC = as.numeric(IHC), 
         FHC = as.numeric(FHC))

initial_final %>%
ggplot(aes(x = IHC, y = FHC, 
                          color = Productive)) +
  geom_point(size = .25) + geom_jitter(width = .1) +
  labs(x = "Initial highest count", y = "Final highest count", 
                      color = "", title = "") +
  scale_x_continuous(breaks = seq(0, 140, 10)) + 
  scale_y_continuous(breaks = seq(0, 140, 10)) + 
  theme_bw() +
  theme(panel.grid.minor = element_blank(), 
        legend.position = "bottom", 
        legend.text = element_text(size = 6), 
        axis.text = element_text(size = 6), 
        axis.title = element_text(size = 7)) +
  scale_colour_manual(values = productivity.pal) +
  theme(legend.position = "bottom") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

##Accuracy by task

```{r 3_tasks, fig.pos = "t", fig.width=3.5, fig.height=2.5, fig.cap = "Mean performance for Unit, Next Number, and Math Facts tasks by Productivity. Bars represent mean performance, with error bars indicating mean standard error. Violin plots indicate density distribution of responses."}
three.pal <- c("#7570B3", "#E7298A", "#66A61E")

all.data %>%
  filter(Task == "SF" |
           Task == "WCN" |
           Task == "MF")%>%
  mutate(Task = factor(Task, levels = c("SF", "WCN", "MF"), 
                       labels = c("Unit", "Next Number", "Math Facts"))) %>% 
  group_by(SID, Task, Productive)%>%
  dplyr::summarise(mean = mean(as.numeric(as.character(Correct), na.rm=TRUE)),
            sd = sd(as.numeric(as.character(Correct), na.rm=TRUE))) %>%
  ggplot(aes(x = Task, y = mean, fill=Task)) +
  stat_summary(fun.y = mean, position = position_dodge(width = .95), 
                      geom="bar", alpha = .5, colour = "black") +
  geom_violin(alpha = .1) +
  stat_summary(fun.data = mean_se, geom="errorbar", 
               position = position_dodge(width=0.90), width = 0.2)+
  ylab("Mean correct") + 
  xlab('') + 
  theme_bw() + 
  theme(legend.position = "bottom") +
  theme(text = element_text(size = 12), 
        panel.grid.minor = element_blank(), 
        panel.grid.major = element_blank(), 
        legend.title = element_blank(),  
        axis.text = element_text(size = 6), 
        axis.title = element_text(size = 7), 
        strip.text.x = element_text(size = 6)) +
  ylim(0, 1.0) +
  scale_fill_manual(values = three.pal, guide = "none") +
  scale_colour_manual(values = productivity.pal, guide = "none") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  facet_grid(~Productive)

##predicting accuracy by task and productivity - t-tests
#Unit task 
unit.t <- all.data %>%
  filter(Task == "SF")%>%
  group_by(SID, Productive)%>%
  dplyr::summarise(mean = mean(Correct))

unit.test <- t.test(subset(unit.t, Productive == "Productive")$mean, 
       subset(unit.t, Productive == "Nonproductive")$mean, var.equal = TRUE)

nn.t <- all.data %>%
  filter(Task == "WCN")%>%
  group_by(SID, Productive)%>%
  dplyr::summarise(mean = mean(Correct))

nn.test <- t.test(subset(nn.t, Productive == "Productive")$mean, 
       subset(nn.t, Productive == "Nonproductive")$mean, var.equal = TRUE)

mf.t <- all.data %>%
  filter(Task == "MF")%>%
  group_by(SID, Productive)%>%
  dplyr::summarise(mean = mean(Correct))

mf.test <- t.test(subset(mf.t, Productive == "Productive")$mean, 
       subset(mf.t, Productive == "Nonproductive")$mean, var.equal = TRUE)

#Is mean performance significantly lower by task for MF for NN by productivity? 
simple.model <- all.data %>%
  filter(Task == "SF" | 
           Task == "WCN" | 
           Task == "MF")%>%
  group_by(SID, Task, Age, Productive)%>%
  dplyr::summarise(mean = mean(Correct))

lm <- lm(mean ~ Task + Productive + Age, data = simple.model)
```

```{r}
model.df <- all.data %>%
  mutate(highest_contig = as.integer(highest_contig), 
         SID = factor(SID), 
         age.c = as.vector(scale(Age, center = TRUE, scale=TRUE)), #scale and center continuous variables for model fit
         fhc.c = as.vector(scale(FHC, center = TRUE, scale = TRUE)), 
         ihc.c = as.vector(scale(IHC, center = TRUE, scale = TRUE)),
         highest_contig.c = as.vector(scale(highest_contig, center = TRUE, scale=TRUE)), 
         highest_num.c = as.vector(scale(highest_num, center = TRUE, scale=TRUE)),
         mean.mf.c = as.vector(scale(mean.mf, center = TRUE, scale=TRUE)),
         starting_num.c = as.vector(scale(as.numeric(as.character(Task_item)), center = TRUE, scale = TRUE)))%>%
  filter(Task == "SF")
```

```{r}
#base
sf.base <- glmer(Correct ~ count_range  +age.c + (1|SID), 
                 family = "binomial", data = model.df)
#highest nn
sf.highest_nn <- glmer(Correct ~ highest_num.c + count_range + age.c + (1|SID), 
                 family = "binomial", data = model.df)

#ihc
sf.ihc <- glmer(Correct ~ ihc.c + count_range + age.c + (1|SID), 
                 family = "binomial", data = model.df)

#fhc
sf.fhc <- glmer(Correct ~ fhc.c + count_range + age.c + (1|SID), 
                 family = "binomial", data = model.df)

#Productive
sf.prod <- glmer(Correct ~ Productive + count_range + age.c + (1|SID), 
                 family = "binomial", data = model.df)

#Highest contiguous NN
sf.highest_contig <- glmer(Correct ~ highest_contig.c + count_range + age.c + (1|SID), 
                 family = "binomial", data = model.df)

#Productivity gradient
sf.prod.gradient <- glmer(Correct ~ prod.gradient + count_range + age.c + (1|SID), 
                 family = "binomial", data = model.df)
```

```{r, include = FALSE}
#base v. highest nn
anova(sf.base, sf.highest_nn, test = 'LRT')
#base v. ihc
anova(sf.base, sf.ihc, test = 'LRT')
#base v. fhc
anova(sf.base, sf.fhc, test = 'LRT')
#base v. productive
anova(sf.base, sf.prod, test = 'LRT')
#base v. highest_contig
anova(sf.base, sf.highest_contig, test = 'LRT')
#base. v. prod. gradient
anova(sf.base, sf.prod.gradient, test = 'LRT')
```

```{r, include = FALSE}
#HCNN
sf.large.base <- glmer(Correct ~ highest_contig.c + count_range + age.c + (1|SID), 
                       family = "binomial", data = model.df)

#FHC + HCNN 
sf.large.plusfhc <- glmer(Correct ~ fhc.c + highest_contig.c + count_range + age.c + (1|SID), 
                       family = "binomial", data = model.df)
prod.chi <- anova(sf.large.base, sf.large.plusfhc, test = 'LRT') #FHC adds to model

#IHC + FHC + HCNN 
sf.large.plusfhc.ihc <- glmer(Correct ~ ihc.c + fhc.c + highest_contig.c + count_range + age.c + (1|SID), 
                       family = "binomial", data = model.df)
anova(sf.large.base, sf.large.plusfhc, sf.large.plusfhc.ihc, test = 'LRT') #IHC does not add to model

##Productivity + FHC +  HCNN
sf.large.plusprod <- glmer(Correct ~ Productive + fhc.c + highest_contig.c + count_range + age.c + (1|SID), 
                       family = "binomial", data = model.df)
anova(sf.large.base, sf.large.plusfhc, sf.large.plusprod, test = 'LRT') #productivity does not add to this model 
```

```{r, include = FALSE}
#make math facts model 
sf.mf <- glmer(Correct ~ mean.mf.c + count_range + age.c + (1|SID), 
               family = "binomial", data = model.df)

#test
math.chi <- anova(sf.base, sf.mf, test = 'LRT') #math facts significant predictor
```

```{r, include = FALSE}
##add Math Facts to large productivity model 
sf.large.plusfhc <- glmer(Correct ~ fhc.c + highest_contig.c + count_range + age.c + (1|SID), 
                       family = "binomial", data = model.df)

sf.large.plusfhc.plusmf <- glmer(Correct ~ mean.mf.c + fhc.c + highest_contig.c + count_range + age.c + (1|SID), 
                       family = "binomial", data = model.df)

critical.chi <- anova(sf.large.plusfhc, sf.large.plusfhc.plusmf, test = 'LRT') # math facts significantly addes to model
summary(sf.large.plusfhc.plusmf)
```

```{r prod_mod_vis, fig.pos = "h", fig.width=3.5, fig.height=2.5, fig.cap = "Estimates of predictor coefficients for the full model of Unit Task performance. Continuous predictors (Math Facts, HCNN, FHC, and Age) are scaled and center. Values above 0 indicate increased likelihood of an accurate Unit Task response. Error bars indicate 95\\%  confidence intervals."}
# library(dotwhisker)
#visualizing regressions
full.model <- summary(sf.large.plusfhc.plusmf)

AIC <- as.numeric(full.model$AICtab[1])

full.model.df <- data.frame(coef(full.model)[,0:4])
full.model.df <- add_rownames(full.model.df, "Parameter")

full.model.df %<>%
  mutate(Parameter = ifelse(Parameter == "highest_contig.c", "HCNN",
                            ifelse(Parameter == "fhc.c", "FHC",
                                   ifelse(Parameter == "count_rangeOutside", "Beyond IHC",
                                          ifelse(Parameter == "age.c", "Age", 
                                                 ifelse(Parameter == "mean.mf.c", "Math Facts", "Intercept")))))) %>%
  filter(Parameter != "Intercept")

full.model.df %>%
ggplot(aes(x = Parameter, y = Estimate)) +
  geom_pointrange(aes(ymin = Estimate - 1.96 * Std..Error,
                      ymax = Estimate + 1.96 * Std..Error,
                      colour = Parameter)) +
  geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
  theme_bw() +
  theme(axis.text = element_text(size = 8),
        axis.title = element_text(size = 9)) +
  coord_flip() +
  scale_colour_brewer(palette = "Dark2", guide = "none") +
  annotate("text", x =  .75 , y = .5, label = paste("AIC =", round(AIC,2)),
           size = 3) +
  xlab("") +
  scale_y_continuous(name = "Coefficient Estimate (log likelihood)")

```

Next, we tested whether Productivity was significantly related to children's performance across the Unit, Next Number, and Math Facts tasks. We found that Productive counters had sigbnificantly greater mean performance for all three tasks in independent sample *t*-tests (all *p*s < .002) (Figure \ref{fig:3_tasks}). Further, this difference persisted for every number queried in each task (Figure \ref{fig:item}), indicating that Productive counters are more likely to have a generalized understanding of the successor function (Unit Task), recursion (Next Number Task), and addition with "+1" (Math Facts). 

```{r item, fig.env = "figure*", fig.pos = "t", fig.width = 6.8, fig.align = "center", set.cap.width = T, num.cols = 2, fig.cap = "Mean performance for Unit, Next Number, and Math Facts by item and Productivity"}
all.data %>%
  filter(Task == "MF" | 
         Task == "WCN" | 
           Task == "SF")%>%
  mutate(Task_item = factor(Task_item, levels = c("5", "6", "7", "15", "20", 
                                                  "21", "24", "26", "30", "32", 
                                                  "34", "46", "51", "57", "60", "62", "64", 
                                                  "71", "73", "81", "83", "84", "86",
                                                  "93", "95")), 
         Task = factor(Task, levels = c("SF", "WCN", "MF"), labels = c("Unit", 
                                                                       "Next Number", 
                                                                       "Math Facts")))%>%
  group_by(Productive, Task, Task_item)%>%
  dplyr::summarise(mean = mean(Correct, na.rm = TRUE), 
            n = n(), 
            sd = sd(Correct, na.rm = TRUE), 
            se = sd/sqrt(n)) %>%
  ggplot(aes(x = Task_item, y = mean, colour = Productive, group = Productive)) +
  geom_point(size = 1) + 
  geom_line() +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), 
                width = .1) +
  theme_bw() + 
  facet_grid(~Task, scale = "free_x") +
  scale_colour_manual(values = productivity.pal) +
  theme(legend.position = "bottom", 
        legend.title = element_blank(), 
        axis.text = element_text(size = 6), 
        axis.title = element_text(size = 7), 
        strip.text.x = element_text(size = 6), 
        legend.text = element_text(size = 6), 
        strip.text = element_text(size = 8)) +
  labs(x = "Number queried", y = "Mean performance") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  

#is this difference significant for every number? Figure out this analysis
```


While Productive counters exhibited higher performance across all three tasks, their performance on Math Facts was significantly lower than mean performance on the Unit and Next Number tasks. A linear regression predicting mean performance from task and controlling for the effects of Productivity and age revealed significant effects of task, with higher mean performance for both the Unit ($\beta$ = `r round(lm$coefficients[2], 2)`, $p < .0001$) and Next Number tasks ($\beta$ = `r round(lm$coefficients[3], 2)`, $p < .0001$). In addition to a main effect of Productivity ($\beta$ = `r round(lm$coefficients[4], 2)`, $p < .0001$), this model also revealed a significant effect of age ($\beta$ = `r round(lm$coefficients[5], 2)`, $p < .0001$).

Taken together, our analyses of children's performance across the Unit, Next Number, and Math Facts tasks indicate two things. First, our broad classification of Productivity captures a meaningful distinction not only in children's counting ability, but also in their general numerical knowledge, as evidenced by significantly greater performance for Productive counters across all three tasks. Second, children's proficiency with formally trained arithmetic operations seems to lag their conceptual understanding of these procedures, as shown by the significant decrease in performance on Math Facts in comparison to the Unit Task -- even for numbers that were repeated across both tasks. We will return to this difference in the discussion.

##Predictors of successor knowledge

Our main question in this work was whether children acquired the successor function through mastery of recursion in the count list (Productive counting knowledge), or through trained arithmetic operations (Math Facts). To test these two hypothesized causal mechanisms, we constructed five individual logistics mixed effects models\footnote{All mixed effects models were fit in \texttt{R} using the \texttt{lme4} package. The model specification was: \texttt{Correct $\sim$ [Productivity/IHC/FHC/HCNN/mean Math Facts] + Within/Outside IHC + Age + ( 1 | subject)}. IHC, FHC, HCNN, and age were centered and scaled in order to fit the model.}, predicting accuracy on the Unit Task as a function of (1) Productivity; (2) Initial Highest Count; (3) Final Highest Count; (4) Highest Contiguous Next Number; and (5) mean Math Facts performance. Continuous predictors (IHC, FHC, HCNN, mean Math Facts, and age) were all scaled and center to allow for direct comparisons. 

In constructing our productivity models, we first tested whether Productivity, IHC, FHC, or HCNN individually predicted children's performance on the Unit Task. Likelihood ratio tests indicated that the addition of each term significantly improved the fit of the model, in comparison to the base (*p*s < .0001 for all tests). Because these productivity predictors may explain overlapping variance, we constructed our final productivity model hierarchically, using likelihood ratio tests to assess whether an individual predictor explained unique variance. This process of model comparison revealed HCNN and FHC as the best predictors of children's performance on the Unit Task ($\chi^2$(1) = `r round(prod.chi$Chisq[2], 2)`, *p* $=$ .003). 

Next, we tested whether mean performance on the Math Facts task was a significant predictor of successor knowledge. A likelihood ratio test indicated that mean Math Facts performance independently predicted Unit Task performance in comparison to a base model ($\chi^2$(1) = `r round(math.chi$Chisq[2], 2)`, *p* $<$ .0001). 

Finally, we added mean Math Facts performance to our large productivity model containing both FHC and HCNN. Interestingly, mean Math Facts performance significantly improved the fit of the model, ($\chi^2$(1) = `r round(critical.chi$Chisq[2], 2)`, $p$ $=$ .0001). The parameter estimates for the full model are shown in Figure \ref{fig:prod_mod_vis}. FHC is no longer a significant predictor when mean Math Facts performance is added to the model, indicating that both terms explain overlapping variance. HCNN, however, remains significant, and is a stronger predictor ($\beta$ $=$ .52, $p$ < .0001) of Unit Task performance than mean Math Facts performance ($\beta$ $=$ .4, $p$ $=$ .0001). 

Taken together, the results of our full model suggest that mastery of both the recursive nature of the count list, as well as trained arithmetic operations, may be sources of knowledge that children leverage in acquiring the successor function. We found that greater accuracy on the Unit Task was associated with both the ability to generate the successor for an arbitrarily high number (HCNN), as well as higher mean Math Facts performance. 

##Do children generalize the successor function from arithmetic code?

```{r}
#add quartiles for unit task
tmp <- all.data %>%
  mutate(sf.quartile = cut(mean.unit, 
                                breaks=quantile(mean.unit, na.rm=TRUE), 
                                include.lowest=TRUE))%>%
  mutate(mf.ceiling = ifelse(mean.mf >= .75, ">= 75% correct", "< 75% correct"))

#predicting performance for kids at ceiling for MF and WCN
ms1 <- tmp %>%
  filter(sf.quartile == "(0.875,1]", 
         Task=="MF" | 
           Task == "WCN")%>%
  mutate(starting_num.c = as.vector(scale(as.numeric(as.character(Task_item)), center = TRUE, scale = TRUE)))

post_hoc <- summary(lmer(Correct ~ Task + starting_num.c + Age + (1|SID), 
              data = ms1))
```

While both Next Number and Math Facts performance produced the best fit for children's Unit Task performance, we found that performance was significantly lower for Math Facts than for both Next Number and the Unit Task. Recall that, if children have acquired the successor function through a generalization of the "+1" operation, we should find that children who are at ceiling on the Unit Task should similarly be at ceiling on the Math Facts task. While our full model indicates significant effects of both recursive counting and arithmetic mastery, it is unclear whether such effects indicate causation, or merely correlation. 

Thus, in a *post-hoc* analysis, we tested whether mean performance differed between the Next Number and Math Facts tasks for children who were at ceiling in the Unit Task ($\geq$ 87.5\%). If children are drawing more heavily upon recursive counting or arithmetic knowledge in acquiring the successor function, that should be evidenced by higher mean performance for that task. We built a linear mixed effects model predicting response from Task, controlling for effects of item magnitude and age, with a random effect of subject. Surprisingly, we found that Math Facts performance was still signficantly lower than Next Number performance for these children ($\beta$ -.32, $p$ $<$ .0001). 

# General Discussion

In the course of acquiring a full understanding of number, young children must make "infinite use of finite means" (citation here). Somehow children are able to extract the set of principles and rules governining symbolic number, and are ultimately able to go beyond their initially limited numerical knowledge. The process by which children learn the logical properties of number, and the sources of knowledge they may leverage, are currently unclear. 

In this work, we tested two pathways to successor function acquisition: recursive counting knowledge and arithmetic fact mastery. While we find that successor knowledge is related to both hypothesized causal mechanisms, our results suggest that children do not draw upon formally trained arithmetic operations in generalizing the successor function. Rather, we find that children's Unit Task performance is more closely linked to the Next Number Task, a measure which captures children's understanding of the count list's recursive structure. 

Our analyses further revealed that our Productive classification captures a meaningful distinction in recursive counting knowledge, with significant differences in mean performance between Productive and Nonproductive counters observed across the Unit, Next Number, and Math Facts tasks. While continuous measures such as Final Highest Count and Highest Contiguous Next Number produced the best fit to Unit Task performance data, these results suggest that using such a categorical measure may be useful in quantifying differences in children's numerical knowledge. Future work should the process by which children become Productive counters, and the role of linguistic input in supporting the extraction of recursive counting rules.

Interestingly, we found significantly lower performance on our Math Facts task than on the Unit or Next Number tasks for all children, regardless of Productivity. This surprising dissociation was especially striking in children who were at ceiling in implementing the successor function. Despite demonstrating mastery of the concepts underlying addition (and even mapping them onto symbolic number), nevertheless these children had not mapped these procedures to formal arithmetic code, even when the same number was queried across both tasks.

This finding is consistent with previous literature suggesting that children's conceptual understanding of arithmetic operatures may be in place prior to learning the language of mathematics [@hughes1981; @huttenlocher1994]. Our data further suggest that children treat statements such as "1+1=2" and "2+1=3" as holistic entities, rather than as the sum of their component parts. We found that almost all children were able to spontaneously and accurately answer "What is 1+1?", but many expressed a great deal of uncertainty for even modestly harder questions by saying things like "I can't do it for harder numbers." Children's performance on this measure strongly suggests that mapping arithmetic code to an underlying concept is an early bottleneck in mathematical ability. 

# Acknowledgements

This work was supported by a NSF GRFP to (first author) and NSF to (last author) (grant #s removed for blinded review). Additional acknowlegdments to (to be added after blinded review).  

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
