---
title: "SF-Math analysis"
author: "Rose Schneider"
date: "5/13/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
rm(list = ls())
require("knitr")
opts_knit$set(root.dir = "~/Documents/Projects/sf_math/") #this is specific to RMS, change accordingly
library(tidyverse)
library(magrittr)
library(langcog)
library(lme4)
library(stringr)
library(RColorBrewer)
library(ggthemes)
```

#Setup

##Loading data
Each task is stored as a separate CSV. Need to tidy and check each sheet individually.
```{r}
#First load Give N
given.raw <- read.csv("Data/GiveN.csv")

#Then load Highest Count
highestcount.raw <- read.csv("Data/HighestCount.csv")
#Then load SF
sf.raw <- read.csv("Data/SF.csv") %>%
  dplyr::rename(num_queried = starting_num) #to make life easier for within/outside

#Then load Next Number
nextnumber.raw <- read.csv("Data/NextNumber.csv")

#Then load Math Facts
mathfacts.raw <- read.csv("Data/MathFacts.csv")

#Then load Indefinite
indefinite.raw <- read.csv("Data/Indefinite.csv")

#combine for looping
tasks <- list(given.raw, highestcount.raw, sf.raw, 
              nextnumber.raw, mathfacts.raw, indefinite.raw)

highestcount.raw %<>%
  mutate(final_highest = as.numeric(as.character(final_highest))) #for some reason was being read in as a factor
```

##Tidy data
Remove all exclusions. Make sure that anyone who is excluded by Give N is not included in other analyses.
```{r}
#make a df of total exclusions across all tasks
check_exclusions <- function(df){
    exclusions <- df %>%
      filter(exclude == 1)%>%
      distinct(SID, age, sex, experimenter, location, exclude, exclude_reason)
    
    exclusions$task = deparse(substitute(df))
    if (length(exclusions$SID) > 0) {
    return(exclusions)
    } else {
      return("No exclusions")
    }
}

#check each task
given.exclusions <- check_exclusions(given.raw)
highestcount.exclusions <- check_exclusions(highestcount.raw)
sf.exclusions <- check_exclusions(sf.raw)
nextnumber.exclusions <- check_exclusions(nextnumber.raw)
mathfacts.exclusions <- check_exclusions(mathfacts.raw)
indefinite.exclusions <- check_exclusions(indefinite.raw) #deal with this later

#bind these together
all_exclusions <- bind_rows(given.exclusions, highestcount.exclusions, 
                            sf.exclusions, nextnumber.exclusions, 
                            mathfacts.exclusions)
exclude <- all_exclusions %>%
  distinct(SID, exclude_reason)%>%
  group_by(exclude_reason)%>%
  summarise(n = n())
# all_exclusions
```

Once you have reviewed these guys manually, run the below code
```{r}
##once you have reviewed these guys manually, run the below code
exclusion_SIDs <- as.vector(all_exclusions$SID)

'%!in%' <- function(x,y)!('%in%'(x,y))

exclude <- function(df){
  df %<>%
    filter(SID %!in% exclusion_SIDs)
}

given.df <- exclude(given.raw)
highestcount.df <- exclude(highestcount.raw)
nextnumber.df <- exclude(nextnumber.raw)
sf.df <- exclude(sf.raw)
mathfacts.df <- exclude(mathfacts.raw)
```

Change the age for participants with incorrect age.
```{r}
change_ages <- function(df) { #update this manually
  df %<>%
     mutate(age = ifelse(SID == "040418-LK", 5.727584, age), 
         age = ifelse(SID == "022818-BH", 5.278576, age), 
         age = ifelse(SID == "032218-LM", 4.175222, age), 
         age = ifelse(SID == "042418-ML", 5.615332, age))
  return(df)
}

given.df <- change_ages(given.df)
sf.df <- change_ages(sf.df)
highestcount.df <- change_ages(highestcount.df)
nextnumber.df <- change_ages(nextnumber.df)
mathfacts.df <- change_ages(mathfacts.df)

```

###General filtering
Remove pilots, empty values, etc. 
```{r}
general_exclusions <- function(df) {
  df %<>%
    filter(location != "Lab", 
           !is.na(SID), 
           !is.na(age))
  return(df)
}

given.df <- general_exclusions(given.df)
highestcount.df <- general_exclusions(highestcount.df) %>%
  mutate(exclude_trial = as.numeric(exclude_trial))
sf.df <- general_exclusions(sf.df)
nextnumber.df <- general_exclusions(nextnumber.df)
mathfacts.df <- general_exclusions(mathfacts.df)
```

##Sanity checks
Do we have the same number (and same participants) for each task? Check unique ids across tasks, make sure there's a match.
```{r}
#get unique IDs
get_unique_SID <- function(df){
  unique <- df %>%
    distinct(SID, age)
  
  unique$task = deparse(substitute(df))
  return(unique)
}

unique.given <- get_unique_SID(given.df)
unique.highestcount <- get_unique_SID(highestcount.df)
unique.sf <- get_unique_SID(sf.df)
unique.nextnumber <- get_unique_SID(nextnumber.df)
unique.mathfacts <- get_unique_SID(mathfacts.df)

#bind all unique IDs and ages together
all_unique <- bind_rows(unique.given, unique.highestcount, unique.sf, 
          unique.nextnumber, unique.mathfacts)%>%
  mutate(SID = factor(SID))

#dumb way to do this, but there need to be 5 tasks per participant
#find all participants that do not have 5 tasks
unique.check <- all_unique %>%
  group_by(SID, age)%>%
  summarise(n = n())%>%
  filter(n != 5)

##NB - you must manally review these. 
##To-do for RMS: You should automate whether these are included or excluded

#Do we have the same number of participants in all tasks
#note, this is necessary, but should be a red flag to check data more carefully if not
check_unique <- function() {
  if (length(unique.given$SID) == length(unique.highestcount$SID) &
      length(unique.sf$SID) & length(unique.nextnumber$SID) & length(unique.mathfacts$SID)) {
    print("Same number of subjects in each task")
  } else {
        print("WARNING: Differing numbers of participants in each task")
      }
}

check_unique()

```

Round age
```{r}
round_age <- function(df) {
  df %<>%
    mutate(age = round(age, 2))
  return(df)
}

sf.df <- round_age(sf.df)
given.df <- round_age(given.df)
nextnumber.df <- round_age(nextnumber.df)
mathfacts.df <- round_age(mathfacts.df)
highestcount.df <- round_age(highestcount.df)
```

##Coding check

Is everything marked as correct actually correct? Recode.
```{r}
sf.df %<>%
  mutate(correct_check = ifelse(num_answer == (num_queried + 1), 1, 0))%>%
  mutate(correct = correct_check)

#because nextnumber and mathfacts have IDK as answers, convert these to numbers
##NB RMS: Do NOT forget to exclude these numbers from analyses

nextnumber.df %<>%
  mutate(num_answer = ifelse(num_answer == "IDK", -1000,
                             as.numeric(as.character(num_answer))))%>%
  mutate(num_answer = ifelse(is.na(num_answer), -2000, 
                             as.numeric(as.character(num_answer))))%>%
  mutate(num_answer = as.numeric(as.character(num_answer)))%>%
  mutate(correct_check = ifelse(num_answer == (num_queried + 1), 1, 0), 
         correct = correct_check)

#math facts
#first need to get starting number
mathfacts.df %<>%
  mutate(num_queried = substr(as.character(problem), 1,2))

mathfacts.df$num_queried <- as.numeric(str_replace(mathfacts.df$num_queried, "[+]", ""))

#now I need to standardize the responses
#there are a lot of answers here that don't make a ton of sense (text and numbers)
mathfacts.df %<>%
  mutate(num_answer_standard = ifelse(num_answer == "IDK", -1000, 
                                      ifelse(num_answer == "idk", -1000, as.character(num_answer))))

starting_num <- c(1, 20, 21, 32, 5, 57, 64, 86, 93)
answer <- c(2, 21, 22, 33, 6, 58, 65, 87, 94)
lookup <- data.frame(starting_num, answer)

check_math_facts <- function() {
  for (row in 1:nrow(mathfacts.df)) {
    num = mathfacts.df[row, "num_queried"]
    answer = subset(lookup, starting_num == num)$answer
    if (answer != mathfacts.df[row, "num_answer_standard"]) {
      mathfacts.df[row, "correct"] = 0
    } else {
     mathfacts.df[row, "correct"] = 1 
    }
  }
  return(mathfacts.df)
}

mathfacts.df <- check_math_facts()

```

##Productivity

Productivity is manually coded for now because it's a bear to do programmatically. Here, spreading manual productivity classification to other tasks.
```{r}
#NB, productivity is manually coded for now, will need to be done programmatically in the future

#performance by productivity
#extract productivity classifications, assign
productive_SIDS <- highestcount.df %>%
  filter(productive == "productive")%>%
  distinct(SID)

productive_SIDS <- as.vector(productive_SIDS$SID)

productivity_classification <- function(df) {
  tmp <- df %>%
    mutate(productive = ifelse(SID %in% productive_SIDS, "productive", "nonproductive"))
  return(tmp)
}

#do across tasks
sf.df <- productivity_classification(sf.df)
nextnumber.df <- productivity_classification(nextnumber.df)
mathfacts.df <- productivity_classification(mathfacts.df)
```

##Within and outside of count range

Trials for SF, NN, and MF need to be classified as within or outside of count range for each participant
```{r}
#first, get initial highest count for each kiddo
lookup <- highestcount.df %>%
  distinct(SID, initial_highest)

#I need to determine whether an indivudal number in a df for a participant is within or outside a given range 

determine_count_range <- function(df) {
  tmp <- df
  for (row in 1:nrow(tmp)) {
    sub = as.character(tmp[row, "SID"])
    count_range = subset(lookup, SID == sub)$initial_highest
    tmp[row, "highest_count"] = count_range
    if (tmp[row, "num_queried"] > count_range) {
      tmp[row, "count_range"] = "outside"
    } else {
      tmp[row, "count_range"] = "within"
    }
  }
  return(tmp)
}

sf.df <- determine_count_range(sf.df)
nextnumber.df <- determine_count_range(nextnumber.df)
mathfacts.df <- determine_count_range(mathfacts.df)
```

##Highest next number

Find the highest next number answered correctly.
```{r}
lookup <- nextnumber.df %>%
  filter(correct == 1)%>%
  group_by(SID)%>%
  summarise(max = max(num_queried))

add_highest_num <- function(df) {
  tmp <- df
  for (row in 1:nrow(tmp)) {
    sub = as.character(tmp[row, "SID"])
    highest_num = subset(lookup, SID == sub)$max
    tmp[row, "highest_num"] = highest_num
  }
  return(tmp)
}

sf.df <- add_highest_num(sf.df)
```

#Demographics
```{r}
sf.df %>%
  mutate(age = round(age, 2))%>%
  distinct(SID, age, sex)%>%
  group_by(sex)%>%
  summarise(n = n(), mean_age = round(mean(age,2)), sd_age = round(sd(age),2), median_age = round(median(age),2))
```


#Planned analyses

##Make model dataframe with all of the relevant variables
```{r}
#add highest nextnumber
#now get initial and final highest counts for each participant
lookup <- highestcount.df %>%
  distinct(SID, initial_highest, final_highest)

add_counts <- function(df) {
  tmp <- df
  for (row in 1:nrow(tmp)) {
    sub = as.character(tmp[row, "SID"])
    initial = subset(lookup, SID == sub)$initial_highest
    final = subset(lookup, SID == sub)$final_highest
    tmp[row, "initial_highest"] = initial
    tmp[row, "final_highest"] = final
  }
  return(tmp)
}

model.df <- add_counts(sf.df)

#mean math facts performance
lookup <- mathfacts.df %>%
  group_by(SID)%>%
  summarise(mean = mean(correct))%>%
  mutate(SID = as.character(SID))

add_mathmean <- function(df) {
  tmp <- df
  for (row in 1:nrow(tmp)) {
    sub = as.character(tmp[row, "SID"])
    mean_math = subset(lookup, SID == sub)$mean
    tmp[row, "mf_mean"] = mean_math
  }
  return(tmp)
}


model.df <- add_mathmean(model.df)

model.df %<>%
  mutate(final_highest = as.integer(final_highest), 
         productive = factor(productive), 
         correct = as.integer(correct), 
         count_range = factor(count_range))
```

Counting Productivity & Successor Task Performance: 
Simple models: 
Model 1: Successor.Performance ~ Highest.Next.Number  + Within/Outside range + Age + (1|subject)
Model 2: Successor.Performance ~ Initial.Count + Within/Outside range + Age + (1|subject)
Model 3: Successor.Performance ~ Final.Count + Within/Outside range + Age + (1|subject)
Model 4: Successor.Performance ~ Productivity+ Within/Outside range + Age + (1|subject)

After running these first four models, any predictor that significantly (p <.05) predicted Successor Performance will be added into Model 5, which will be our “Large” model (containing all predictors that significantly predicted Successor Performance in the simple models).

We will construct model 5 hierarchically. Model comparisons will be performed at each stage by running a likelihood ratio test between reduced and full models, with significant effects retained in the full model (Model 5). Model selection will be done on the basis of AIC evaluation and significant Chi-square statistic.

Math Facts & Successor Task Performance: To identify whether there is connection between math fact knowledge and Successor Task performance, we will conduct one primary analysis, which predict Successor Task performance from performance on the Math Facts task. 
Model 6: Successor.Performance ~ Math.Facts + Within/Outside range + Age + (1|subject)

After running this model, if there is a significant relationship between Math Facts and Successor performance, we will construct Model 7, which includes both Math Facts and all of the significant predictors from Model 5 (above). 

To test whether Model 7 (containing Math Facts) fits the data significantly better than the reduced Model 5 (without Math Facts), we will conduct a likelihood ratio test of the two models, using AIC and Chi-square statistic to assess whether Math Facts performance significantly contributes to predicting SF performance. 

17.5 Correlation between all continuous measures and all other continuous measures. 

Error classification

##Models
###Model 1: Highest Next Number
```{r}
#model 1
#Successor.Performance ~ Highest.Next.Number  + Within/Outside range + Age + (1|subject)
model.1 <- glmer(correct ~ highest_num + count_range + age + (1|SID), 
                 family = "binomial", data = model.df)
summary(model.1)
```

###Model 2: Initial Highest Count
```{r}
#model 2
#Successor.Performance ~ Initial.Count + Within/Outside range + Age + (1|subject)
model.2 <- glmer(correct ~ initial_highest + count_range + age + (1|SID), 
                                  family = "binomial", data = model.df)
summary(model.2)
```

###Model 3: Final Highest Count
```{r}
#model 3
#Successor.Performance ~ Final.Count + Within/Outside range + Age + (1|subject)
model.3 <- glmer(correct ~ final_highest + count_range + age + (1|SID), 
                 family = "binomial", data = model.df)
summary(model.3)
```

###Model 4: Productive Classification
```{r}
#model 4
#Successor.Performance ~ Productivity+ Within/Outside range + Age + (1|subject)
model.4 <- glmer(correct ~ productive + count_range + age + (1|SID), 
                 family = "binomial", data = model.df)
summary(model.4)

```

###Model 5: Large Model, all significant from 1-4
```{r}
##Any predictor that significantly (p <.05) predicted Successor Performance will be added into Model 5, which will be our “Large” model (containing all predictors that significantly predicted Successor Performance in the simple models).
#Model 1: Highest next number (p = .0183)
#Model 2: Initial highest (p = .64)
#Model 3: Final highest (p < .0001)
#Model 4: Productive (p < .0001)

model.5.base <- glmer(correct ~ count_range + age + (1|SID), 
                      family = "binomial", data = model.df)
#add final, compare
model.5.final <- glmer(correct ~ final_highest + count_range + age + (1|SID), 
                 family = "binomial", data = model.df)
anova(model.5.base, model.5.final, test = 'LRT')

##add highest
model.5.highest_nn <- glmer(correct ~ highest_num + final_highest + count_range + age + (1|SID), 
                 family = "binomial", data = model.df)

model.5.prod <- glmer(correct ~ productive + final_highest + count_range + age + (1|SID), 
                 family = "binomial", data = model.df)

anova(model.5.highest_nn,  model.5.prod,model.5.final, test = 'LRT')

summary(model.5.final)
#final_highest seems to be doing the most work here, AIC and Chisq. don't seem to justify us retaining highest nn or productive in the model 

#model unstable, check and see if convergence warning is legit
# with(model.5.final_highest@optinfo$derivs,max(abs(solve(Hessian,gradient)))<2e-3) #true, we've converged adequately
```

###Model 6 - math facts
Is mean performance on the math facts task predictive of SF performance?
Not a significant predictor at this time
```{r}
#Model 6: Successor.Performance ~ Math.Facts + Within/Outside range + Age + (1|subject)
model.6 <- glmer(correct ~ mf_mean + count_range + age + (1|SID), 
                 family = "binomial", data = model.df)
model.6.base <- glmer(correct ~ count_range + age + (1|SID), 
                 family = "binomial", data = model.df)
anova(model.6, model.6.base, test = 'LRT')

```

##Correlation between all continuous variables and all other continuous variables
```{r}
#NB, for task correlations, do we want mean task performance?
#continuous variables = age, highest counts
cor.test(model.df$age, model.df$initial_highest)
cor.test(model.df$age, model.df$final_highest)
cor.test(model.df$initial_highest, model.df$initial_highest)
```

##Error classification on Highest count
```{r}
decades <- c("9", "19", "29", "39", "49", "59", "69", 
             "79", "89", "99", "109", "119")
beginning_decade <- c("10", "20", "30", "40", "50", "60", 
                      "70", "80", "90", "100", "110")

errors <- highestcount.df %>%
  mutate(error_type = ifelse(as.character(last_successful) %in% decades, "Decade-final",
                             ifelse(is.na(last_successful), "NaN",
                                    ifelse(as.character(last_successful) %in% beginning_decade, 
                                           "Decade-beginning", "Mid-decade"))))%>%
  filter(error_type != "NaN")%>%
  group_by(productive, error_type)%>%
  dplyr::summarise(n = n())%>%
  group_by(productive)%>%
  mutate(total = sum(n), 
         prop = n/total)

productivity.colors <- c("#4daf4a","#984ea3")

#visualization of error types by productivity
ggplot(errors, aes(x = error_type, y = prop, fill = factor(productive, 
                                                        levels = c("productive", "nonproductive"), 
                                                        labels = c("Productive", "Nonproductive")))) + 
  geom_bar(stat = "identity", position = position_dodge()) +
  theme_bw() + 
  scale_fill_manual(values = productivity.colors) +
  labs(x = "Error type", y = "Proportion of errors", fill = "Productivity classification", 
       title = "Proportion of error types by productivity group")
```

#Individual task visualizations
1. Initial highest count and final highest count, grouped by productivity
2. Performance on SF, NN, and MF, within/outside of range, grouped by productivity
3. Scatterplot of relation between tasks, SF, NN, and MF
4. Mean performance on novel/same numbers on SF/NN/MF, grouped by productivity


##Highest count
###Scatterplot

Relation between initial and final highest count, grouped by productivity
```{r}
#relation between initial and final
initial_final <- highestcount.df %>%
  distinct(SID, age, initial_highest, final_highest, productive)%>%
  mutate(initial_highest = as.numeric(initial_highest), 
         initial_final = as.numeric(final_highest))

highest_count <- ggplot(initial_final, aes(x = initial_highest, y = final_highest, 
                          color = factor(productive, levels = c("productive","nonproductive"), labels = c("Productive", "Nonproductive")))) +
  geom_point(size = 2) + geom_jitter(width = .1) +
  labs(x = "Initial highest count", y = "Final highest count",
                      title = "Initial and final highest counts by productivity classification", 
                      color = "Productivity Classification") +
  theme_bw() + 
  scale_x_continuous(breaks = seq(0, 120, 10)) + 
  scale_y_continuous(breaks = seq(0, 120, 10)) + 
  theme(panel.grid.minor = element_blank()) + 
  scale_color_manual(values = productivity.colors)
highest_count
```

###Histograms of initial and final highest count distributions, grouped by productivtiy
```{r}
#distribution of initial
initial_hist <- ggplot(initial_final, aes(x = initial_highest, 
                          fill = factor(productive, levels = c("productive", "nonproductive"), 
                                   labels = c("Productive", "Nonproductive")))) + 
  geom_histogram(colour = "black") +
  scale_x_continuous(name="Initial highest count", breaks=seq(0,120,10)) +
  theme_bw() + 
  labs(y = "Count", title = "Distribution of initial highest counts", 
       fill = "Productivity Classification") + 
  theme(panel.grid.minor = element_blank()) +
  scale_fill_manual(values = productivity.colors)
initial_hist

final <- ggplot(initial_final, aes(x = final_highest, 
                          fill = factor(productive, levels = c("productive", "nonproductive"), labels = c("Productive", "Nonproductive")))) + 
  geom_histogram(colour = "black") +
  scale_x_continuous(name="Final highest count", breaks=seq(0,120,10)) +
  theme_bw() + 
  labs(y = "Count", title = "Distribution of final highest counts", 
       fill = "Productivity Classification") + 
  theme(panel.grid.minor = element_blank()) +
  scale_fill_manual(values = productivity.colors)
final
```

##SF

This is a sanity check to make sure that performance doesn't significantly differ between experiment halves (i.e., checking to make sure that performance isn't dropping because kids are getting bored.)
```{r}
#is there a significant accuracy difference between first and second half of experiment? 
sf.df %<>%
  mutate(half = ifelse(trial == 1 | trial == 2, "practice",
                       ifelse(trial > 2 & trial < 11, "first", "second")))


#significant difference between first and second half?
t.test(subset(sf.df, half == "first")$correct, subset(sf.df, half == "second")$correct, 
       var.equal = TRUE)

sf.model <- sf.df %>%
  filter(half != "practice")

#use mem to account for subject-level variability
m1 <- glmer(correct ~ half + (1 | SID), data=sf.model, 
            family=binomial, control=glmerControl(optimizer="bobyqa",
                            optCtrl=list(maxfun=2e5)))
m0 <- glmer(correct ~  1  + (1 | SID), data=sf.model, 
            family=binomial, control=glmerControl(optimizer="bobyqa",
                            optCtrl=list(maxfun=2e5)))
anova(m0,m1)
```

###Accuracy by within/outside range by productivity
```{r}
sf.range.ms <- sf.df %>%
  filter(half != "practice") %>%
  group_by(count_range, productive)%>%
  multi_boot_standard("correct", na.rm = TRUE)


countrange.colors <- c("#e41a1c","#377eb8")

ggplot(sf.range.ms, aes(x = factor(productive, levels = c("productive", "nonproductive"), 
                                   labels = c("Productive", "Nonproductive")), y = mean, 
                        fill = factor(count_range, levels = c("within", "outside"), 
                                      labels = c("Within initial highest count", "Outside initial highest count")))) + 
  geom_bar(stat = "identity", position= position_dodge()) + 
  geom_linerange(aes(ymin = ci_lower,
                      ymax = ci_upper),
                  size = .3,
                  show.legend = FALSE,
                 position=position_dodge(width = 0.9)) +
  theme_bw() +
  labs(x = "Productivity", y = "Mean Correct",  
       title = "Accuracy by count range and productivity: Successor Function", 
       fill = "Within/outside count range") +
  scale_fill_manual(values = countrange.colors) 
```

##Next number
###Accuracy within/outside by productivity
```{r}
nn.range.ms <- nextnumber.df %>%
  filter(num_queried != 1) %>%
  group_by(count_range, productive)%>%
  multi_boot_standard("correct", na.rm = TRUE)

ggplot(nn.range.ms, aes(x = factor(productive, levels = c("productive", "nonproductive"), 
                                                          labels = c("Productive", "Nonproductive")), y = mean, 
                        fill = factor(count_range, levels = c("within", "outside"), 
                                      labels = c("Within initial highest count", "Outside initial highest count")))) + 
  geom_bar(stat = "identity", position= position_dodge()) + 
  geom_linerange(aes(ymin = ci_lower,
                      ymax = ci_upper),
                  size = .3,
                  show.legend = FALSE,
                 position=position_dodge(width = 0.9)) +
  theme_bw() +
  labs(x = "Productivity", y = "Mean Correct",  
       title = "Accuracy by count range and productivity: Next Number", 
       fill = "Within/outside count range") +
  scale_fill_manual(values = countrange.colors)
```

##Math facts
###Accuracy by productivity and range
```{r}
mf.range.ms <- mathfacts.df %>%
  filter(problem != "1+1")%>%
  group_by(count_range, productive)%>%
  multi_boot_standard("correct", na.rm = TRUE)

ggplot(mf.range.ms, aes(x = factor(productive, levels = c("productive", "nonproductive"), 
                                                          labels = c("Productive", "Nonproductive")), y = mean, 
                        fill = factor(count_range, levels = c("within", "outside"), 
                                      labels = c("Within initial highest count", "Outside initial highest count")))) + 
  geom_bar(stat = "identity", position= position_dodge()) + 
  geom_linerange(aes(ymin = ci_lower,
                      ymax = ci_upper),
                  size = .3,
                  show.legend = FALSE,
                 position=position_dodge(width = 0.9)) +
  theme_bw() +
  labs(x = "Productivity", y = "Mean Correct",  
       title = "Accuracy by count range and productivity: Math Facts", 
       fill = "Within/outside count range") +
  scale_fill_manual(values = countrange.colors)
```

##SF, NN, MF
###Difference in performance on same/novel numbers by task
```{r}
#first, get a vector of all the numbers used in each task
sf_nums <- sf.df %>%
  distinct(num_queried)

sf_nums <- as.vector(sf_nums$num_queried)

math_facts_nums <- mathfacts.df %>%
  distinct(num_queried)

mf_nums <- as.vector(math_facts_nums$num_queried)

nn_nums <- nextnumber.df %>%
  distinct(num_queried)

nn_nums <- as.vector(nn_nums$num_queried)

##make a function to label novel or same
novel_same <- function(df) {
  tmp <- df
  for (row in 1:nrow(tmp)) {
    num = tmp[row, "num_queried"]
    if (num %!in% sf_nums) {
     tmp[row, "novel_same"] = "novel" 
    } else {
      tmp[row, "novel_same"] = "same"
    }
  }
  return(tmp)
}

math_nn <- function() {
  for (row in 1:nrow(sf.df)) {
    num = sf.df[row, "num_queried"]
    if (num %in% mf_nums) {
      sf.df[row, "novel_same"] = "same"
    } else if (num %in% nn_nums) {
      sf.df[row, "novel_same"] = "same"
    } else {
      sf.df[row, "novel_same"] = "novel"
    }
  }
  return(sf.df)
}

#remove training
sf.df <- math_nn()%>%
  filter(num_queried != 1,
         num_queried != 3)
nextnumber.df <- novel_same(nextnumber.df) %>%
  filter(num_queried != 1)
mathfacts.df <- novel_same(mathfacts.df)  %>%
  filter(problem != "1+1")

novel.nn <- nextnumber.df %>%
  mutate(productive = factor(productive, 
                             levels = c("productive","nonproductive"), 
                             labels = c("Productive", "Nonproductive")), 
         count_range = factor(count_range, levels = c("within", "outside"),
                              labels = c("Within", "Outside")))%>%
  group_by(SID, age, productive, novel_same, count_range) %>%
  multi_boot_standard("correct", na.rm = TRUE)%>%
  mutate(task = "next number")

novel.mf <- mathfacts.df %>%
  mutate(productive = factor(productive, 
                             levels = c("productive","nonproductive"), 
                             labels = c("Productive", "Nonproductive")), 
         count_range = factor(count_range, levels = c("within", "outside"),
                              labels = c("Within", "Outside")))%>%
  group_by(SID, age, productive, novel_same, count_range)%>%
  multi_boot_standard("correct", na.rm = TRUE)%>%
  mutate(task = "math_facts")

novel.sf <- sf.df %>%
  mutate(productive = factor(productive, 
                             levels = c("productive","nonproductive"), 
                             labels = c("Productive", "Nonproductive")), 
         count_range = factor(count_range, levels = c("within", "outside"),
                              labels = c("Within", "Outside")))%>%
  group_by(SID, age, productive, novel_same, count_range)%>%
  multi_boot_standard("correct", na.rm = TRUE)%>%
  mutate(task = "successor")

tmp <- bind_rows(novel.nn, novel.mf)
all.novel <- bind_rows(tmp, novel.sf)


all.novel.ms <- all.novel %>%
  group_by(novel_same, task, productive, count_range)%>%
  multi_boot_standard("mean", na.rm = TRUE)


task.colors = c("#9E0142", "#3288BD", "#5E4FA2")
#plot
novel <- ggplot(all.novel.ms, aes(x = factor(novel_same, levels = c("novel", "same"), 
                                    labels = c("Novel", "Same")), y = mean, 
                         fill = factor(task, levels = c("math_facts", "next number", "successor"), 
                                       labels = c("Math Facts", "Next Number", "Successor")), 
                         alpha = count_range)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  geom_linerange(aes(ymin = ci_lower,
                      ymax = ci_upper),
                  size = .3,
                  show.legend = FALSE,
                 position=position_dodge(width = 0.9))+
  theme_bw() +
  facet_wrap(~productive) + 
  scale_fill_manual(values = task.colors) + scale_alpha_discrete(range = c(1, .5))+
  labs(x = "Novel/Same", y = "Mean performance", fill = "Task", 
       alpha = "Within/Outside count range")
novel
```


##Scatterplot by task (SF/NN/MF) 
```{r}
#first, compute mean performance for each kiddo for each task, so that you have one value per kid

#then put all of these together for the scatterplot

sf.mean <- sf.df %>%
  group_by(SID, age, productive)%>%
  summarise(mean.sf = mean(correct))

nn.mean <- nextnumber.df %>%
  group_by(SID)%>%
  summarise(mean.nn = mean(correct))

mf.mean <- mathfacts.df %>%
  group_by(SID)%>%
  summarise(mean.mf = mean(correct))

tmp <- right_join(sf.mean, nn.mean, by = "SID")
all.mean <- right_join(tmp, mf.mean, by = "SID")

##Individual scatterplots for now, until I learn how to combine them all into one plot
ggplot(all.mean, aes(x = mean.sf, y = mean.nn, 
                     color = factor(productive, levels = c("productive", "nonproductive"), 
                                    labels = c("Productive", "Nonproductive")))) +
  geom_point(size = 2)  +
  labs(x = "Mean performance: SF", y = "Mean performance: NN",
                      title = "Mean performance on SF and NN", 
                      color = "Productivity Classification") +
  theme_bw() +
  theme(panel.grid.minor = element_blank()) + 
  scale_color_manual(values = productivity.colors)

#nn and mf
ggplot(all.mean, aes(x = mean.mf, y = mean.nn, 
                     color = factor(productive, levels = c("productive", "nonproductive"), 
                                    labels = c("Productive", "Nonproductive")))) +
  geom_point(size = 2)  +
  labs(x = "Mean performance: MF", y = "Mean performance: NN",
                      title = "Mean performance on NN and MF", 
                      color = "Productivity Classification") +
  theme_bw() +
  theme(panel.grid.minor = element_blank()) + 
  scale_color_manual(values = productivity.colors)

#mf and sf
ggplot(all.mean, aes(x = mean.mf, y = mean.sf, 
                     color = factor(productive, levels = c("productive", "nonproductive"), 
                                    labels = c("Productive", "Nonproductive")))) +
  geom_point(size = 2)  +
  labs(x = "Mean performance: MF", y = "Mean performance: SF",
                      title = "Mean performance on MF and SF", 
                      color = "Productivity Classification") +
  theme_bw() +
  theme(panel.grid.minor = element_blank()) + 
 scale_color_manual(values = productivity.colors)


#now melt?
all.melt <- gather(all.mean, task, mean, mean.sf:mean.mf)

#scatterplot with three - to come in the future
```

#Exploratory analyses
##Partial correlation, SF/MF/NN performance, controlling for age
Exploratory data - To explore whether children’s performance on the Successor Task is related to their ability to generalize the Successor to all numbers, we will run a partial correlation between performance on both tasks, controlling for age.
```{r}
library(ppcor)

sf.nn.pcor <- pcor.test(all.mean$mean.sf, all.mean$mean.nn, all.mean$age) 
sf.mf.pcor <- pcor.test(all.mean$mean.sf, all.mean$mean.mf, all.mean$age)
```

##T-tests by productivity on SF/MF/NN
Using the Productive/Non-Productive categorical classification outlined above, we will compare mean performance between both groups on all tasks (Next Number, Successor, and Math Facts) in t-tests.
```{r}
#next number
nn.t.test <- t.test(subset(all.mean, productive == "productive")$mean.nn, 
                    subset(all.mean, productive == "nonproductive")$mean.nn, var.equal = TRUE)

#successor function
sf.t.test <- t.test(subset(all.mean, productive == "productive")$mean.sf, 
                    subset(all.mean, productive == "nonproductive")$mean.sf, var.equal = TRUE)

#math facts
mf.t.test <- t.test(subset(all.mean, productive == "productive")$mean.mf, 
                    subset(all.mean, productive == "nonproductive")$mean.mf, var.equal = TRUE)

```
Exploratory modeling of our counting data to test the predictions of Yang (2017). Here, we plan to use cluster analysis to identify distinct groups (or categories) of counters from our highest count data, assessing the likelihood of Successor Knowledge from cluster membership.

#Post-hoc analyses
1. Accuracy ~ Problem * Task
```{r}
#make the appropriate df with SID, age, productive, correct, task, novel_same, count range
sf.limited <- sf.df %>%
  dplyr::select(SID, age, productive, num_queried, correct, novel_same, count_range)%>%
  mutate(task = "Successor")

nn.limited <- nextnumber.df %>%
  dplyr::select(SID, age, productive, num_queried, correct, novel_same, count_range)%>%
  mutate(task = "Next Number")

mf.limited <- mathfacts.df %>%
  dplyr::select(SID, age, productive, num_queried, correct, novel_same, count_range)%>%
  mutate(task = "Math Facts")

all.limited <- bind_rows(sf.limited, nn.limited, mf.limited)%>%
  mutate(SID = factor(SID), 
         productive = factor(productive), 
         correct = as.integer(correct), 
         novel_same = factor(novel_same), 
         count_range = factor(count_range), 
         task = factor(task, levels = c("Successor", "Next Number", "Math Facts")))

##Is accuracy mediated by task? 
##Correct ~ Task*novel_same + Within/outside + Age + (1|SID)

model.post1 <- glmer(correct ~ task*novel_same + count_range + age + (1|SID), 
                     family = "binomial", data = all.limited)

#model fails to converge - is this a big deal? check
with(model.post1@optinfo$derivs,max(abs(solve(Hessian,gradient)))<2e-3) #false, take this with a grain of salt for now

summary(model.post1)
```
