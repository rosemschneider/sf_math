---
title: "SF-Math analysis"
author: "Rose Schneider"
date: "5/13/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
rm(list = ls())
require("knitr")
opts_knit$set(root.dir = "~/Documents/Projects/sf_math/") #this is specific to RMS, change accordingly
library(tidyverse)
library(magrittr)
library(langcog)
library(lme4)
library(stringr)
library(RColorBrewer)
library(ggthemes)
```

#Setup
###Loading data
Each task is stored as a separate CSV. Need to tidy and check each sheet individually.
```{r}
#First load Give N
given.raw <- read.csv("Data/GiveN.csv")

#Then load Highest Count
highestcount.raw <- read.csv("Data/HighestCount.csv")
#Then load SF
sf.raw <- read.csv("Data/SF.csv") %>%
  dplyr::rename(num_queried = starting_num) #to make life easier for within/outside

#Then load Next Number
nextnumber.raw <- read.csv("Data/NextNumber.csv")

#Then load Math Facts
mathfacts.raw <- read.csv("Data/MathFacts.csv")

#Then load Indefinite
indefinite.raw <- read.csv("Data/Indefinite.csv")

##Small data-type fix
highestcount.raw %<>%
  mutate(final_highest = as.numeric(as.character(final_highest))) #for some reason was being read in as a factor
```

###Tidy data
Remove all exclusions. Make sure that anyone who is excluded by Give N is not included in other analyses.
```{r}
##This is a function that checks a dataframe for anyone marked as "excluded", pulls out those participants, and puts them into a dataframe
check_exclusions <- function(df){
    exclusions <- df %>%
      filter(exclude == 1)%>%
      distinct(SID, age, sex, experimenter, location, exclude, exclude_reason)
    
    exclusions$task = deparse(substitute(df))
    if (length(exclusions$SID) > 0) {
    return(exclusions)
    } else {
      return("No exclusions")
      return(exclusions)
    }
}

#check each task individually - this will return exclusion dfs for each tasks
given.exclusions <- check_exclusions(given.raw)%>%
  filter(exclude == "1")%>%
  mutate(exclude = as.numeric(as.character(exclude)))
highestcount.exclusions <- check_exclusions(highestcount.raw)%>%
  filter(exclude == "1")%>%
  mutate(exclude = as.numeric(as.character(exclude)))
sf.exclusions <- check_exclusions(sf.raw)%>%
  filter(exclude == "1")%>%
  mutate(exclude = as.numeric(as.character(exclude)))
nextnumber.exclusions <- check_exclusions(nextnumber.raw)%>%
  filter(exclude == "1")%>%
  mutate(exclude = as.numeric(as.character(exclude)))
mathfacts.exclusions <- check_exclusions(mathfacts.raw)%>%
  filter(exclude == "1")%>%
  mutate(exclude = as.numeric(as.character(exclude)))
indefinite.exclusions <- check_exclusions(indefinite.raw) %>%
  filter(exclude == "1")%>%
  mutate(exclude = as.numeric(as.character(exclude)))

#bind all of these individual exclusion dfs together. NB, this will include duplicate SIDs, because an excluded participant might appear in more than one task
all_exclusions <- bind_rows(given.exclusions, highestcount.exclusions, 
                            sf.exclusions, nextnumber.exclusions, 
                            mathfacts.exclusions, indefinite.exclusions)

#pull out the distinct SIDs that need to be excluded
exclude <- all_exclusions %>%
  distinct(SID, exclude_reason)%>%
  group_by(SID, exclude_reason)%>%
  dplyr::summarise(n = n())
# all_exclusions
```

**IMPORTANT** You now have two dataframes that contain SIDs that have been flagged for exclusion from analyses. You need to review these SIDs manually to determine whether their exclusion is warranted.
```{r}
##once you have reviewed these SIDs manually, run the below code
#make a vector of all the SIDs that should be excluded
exclusion_SIDs <- as.vector(all_exclusions$SID)

#this is a function to exclude elements not in a vector
'%!in%' <- function(x,y)!('%in%'(x,y))

#function for excluding SIDs
exclude <- function(df){
  df %<>%
    dplyr::filter(SID %!in% exclusion_SIDs)
  return(df)
}

#run this function on every task df to exclude the participants that have been flagged
given.df <- exclude(given.raw)
highestcount.df <- exclude(highestcount.raw)
nextnumber.df <- exclude(nextnumber.raw)
sf.df <- exclude(sf.raw)
mathfacts.df <- exclude(mathfacts.raw)
indefinite.df <- exclude(indefinite.raw)
```

###Manual changes
Change the age for participants with incorrect age.
```{r}
#function for changing ages. This is fairly flexible, and could be updated to fix any other coding errors in the datasheet.
change_ages <- function(df) { #update this manually
  df %<>%
     mutate(age = ifelse(SID == "040418-LK", 5.727584, age), 
         age = ifelse(SID == "022818-BH", 5.278576, age), 
         age = ifelse(SID == "032218-LM", 4.175222, age), 
         age = ifelse(SID == "042418-ML", 5.615332, age))
  return(df)
}

#make manual changes for each task df
given.df <- change_ages(given.df)
sf.df <- change_ages(sf.df)
highestcount.df <- change_ages(highestcount.df)
nextnumber.df <- change_ages(nextnumber.df)
mathfacts.df <- change_ages(mathfacts.df)
indefinite.df <- change_ages(indefinite.df)

#final highest count has a cap at 120
highestcount.df %<>%
  mutate(final_highest = ifelse(final_highest > 120, 120, final_highest), 
         initial_highest = ifelse(initial_highest > 120, 120, initial_highest))

```

###General filtering
Remove pilots, empty values, etc. 
```{r}
#This is a function for excluding participants run in the lab (pilots), or empty values - this is also very flexible
general_exclusions <- function(df) {
  df %<>%
    filter(!is.na(SID), 
           !is.na(age))
  return(df)
}

#run on each individual task df
given.df <- general_exclusions(given.df)

highestcount.df <- general_exclusions(highestcount.df) %>%
  mutate(exclude_trial = ifelse(is.na(exclude_trial), 0, 1))%>%
  filter(exclude_trial != 1)

sf.df <- general_exclusions(sf.df) %>%
  mutate(exclude_trial = ifelse(is.na(exclude_trial), 0, 1))%>%
  filter(exclude_trial != 1)

nextnumber.df <- general_exclusions(nextnumber.df)%>%
  mutate(exclude_trial = ifelse(is.na(exclude_trial), 0, 1))%>%
  filter(exclude_trial != 1)

mathfacts.df <- general_exclusions(mathfacts.df)%>%
  mutate(exclude_trial = ifelse(is.na(exclude_trial), 0, 1))%>%
  filter(exclude_trial != 1)

indefinite.df <- general_exclusions(indefinite.df)%>%
  filter(!is.na(correct))%>%
  mutate(exclude_trial = ifelse(is.na(exclude_trial), 0, 1))%>%
  filter(exclude_trial != 1)
```

###Sanity checks
Do we have the same number (and same participants) for each task? Check unique ids across tasks, make sure there's a match.
```{r}
#this is a function for getting unique IDs in each task
get_unique_SID <- function(df){
  unique <- df %>%
    distinct(SID, age)
  
  unique$task = deparse(substitute(df))
  return(unique)
}

#FOR NOW (change later): Filter this SID out
given.df %<>%
  filter(SID != "062518-SG")

highestcount.df %<>%
  filter(SID != "062518-SG")

#get unique IDs for each task df
unique.given <- get_unique_SID(given.df)
unique.highestcount <- get_unique_SID(highestcount.df)
unique.sf <- get_unique_SID(sf.df)
unique.nextnumber <- get_unique_SID(nextnumber.df)
unique.mathfacts <- get_unique_SID(mathfacts.df) #NB RMS: you'll have to deal with indefinite here

#bind all unique IDs and ages together in a df
all_unique <- bind_rows(unique.given, unique.highestcount, unique.sf, 
          unique.nextnumber, unique.mathfacts)%>%
  mutate(SID = factor(SID))

#dumb way to do this, but there need to be 5 tasks per participant
#find all participants that do not have 5 tasks
unique.check <- all_unique %>%
  group_by(SID, age)%>%
  summarise(n = n())%>%
  filter(n != 5)


```

**IMPORTANT**You now have a dataframe with unique SIDs that are flagged as not completing 5 tasks. To be included in our analyses, a child MUST at least participate in each task. You need to review these participants individually, because there might be some other reason that they are showing up here (incorrectly entered age, SID, etc.)
```{r}
#Do we have the same number of participants in all tasks
#This is a function which checks whether the number of participants is equal in each task

###NB RMS: I actually want to do this a little differently, where I'm checking to make sure we have the same participants in each task, and not just the same number
check_unique <- function() {
  if (length(unique.given$SID) == length(unique.highestcount$SID) &
      length(unique.given$SID) == length(unique.sf$SID) & 
      length(unique.given$SID) == length(unique.nextnumber$SID) & 
      length(unique.given$SID) == length(unique.mathfacts$SID)) {
    print("Same number of subjects in each task")
  } else {
        print("WARNING: Differing numbers of participants in each task")
      }
}

check_unique()

#find which SIDs are not in the tasks
check <- unique.mathfacts %>%
  filter(SID %!in% unique.given$SID)

#Below, I'm making sure that we have the same SIDs in each task
given.sids <- as.vector(unique(given.df$SID))
hc.sids <- as.vector(unique(highestcount.df$SID))
sf.sids <- as.vector(unique(sf.df$SID))
nn.sids <- as.vector(unique(nextnumber.df$SID))
mf.sids <- as.vector(unique(mathfacts.df$SID))

#I need to check this for EVERY df
check_unique_sids <- function() {
  if(length(which(given.sids %in% hc.sids)) == length(given.sids)) {
    given.hc <- TRUE
    given.all <- TRUE
  } else if(length(which(given.sids %in% sf.sids)) == length(given.sids)) {
    given.sf <- TRUE
    given.all <- TRUE
  } else if(length(which(given.sids %in% nn.sids)) == length(given.sids)) {
    given.nn <- TRUE
    given.all <- TRUE
  } else if(length(which(given.sids %in% mf.sids)) == length(given.sids)) {
    given.mf <- TRUE
    given.all <- TRUE
  } else {
    given.all <- FALSE
  }
  if(given.all == FALSE) {
    print("WARNING: CHECK SIDS, MISMATCH")
  }
  else {
    print("Same SIDS in each task")
  }
}
check_unique_sids()
```

###Round age
```{r}
#this is a function for rounding age
round_age <- function(df) {
  df %<>%
    mutate(age = round(age, 2))
  return(df)
}

#round age for every task df
sf.df <- round_age(sf.df)
given.df <- round_age(given.df)
nextnumber.df <- round_age(nextnumber.df)
mathfacts.df <- round_age(mathfacts.df)
highestcount.df <- round_age(highestcount.df)
```

***

##Data manipulations
Checking coding, adding productivity, highest count, within/outside count range, etc.

###Coding check
Is everything marked as correct actually correct? I'm recoding here.
```{r}
#SF task
#This one is easy - just check whether the number answered is one more than the number prompted
sf.df %<>%
  mutate(correct_check = ifelse(num_answer == (num_queried + 1), 1, 0))%>%
  mutate(correct = correct_check)

#NN and MF are a little more difficult, because there are non-number answers
#Convert these to nonplausible answers
#NB: These will be marked as incorrect, but you MUST exclude them in you are doing any kind of analyses with the actual numeric answers being given
nextnumber.df %<>%
  mutate(num_answer = ifelse(num_answer == "IDK", -1000,
                             as.numeric(as.character(num_answer))))%>% #IDK = -1000
  mutate(num_answer = ifelse(is.na(num_answer), -2000, 
                             as.numeric(as.character(num_answer))))%>% #NA = -2000
  mutate(num_answer = as.numeric(as.character(num_answer)))%>%
  mutate(correct_check = ifelse(num_answer == (num_queried + 1), 1, 0), #if answer = 1+ prompt, correct
         correct = correct_check)

#math facts
#this is a little more difficult because we need to pull out the starting number
mathfacts.df %<>%
  mutate(num_queried = substr(as.character(problem), 1,2)) #pull out the first two numbers

mathfacts.df$num_queried <- as.numeric(str_replace(mathfacts.df$num_queried, "[+]", "")) #remove the plus, convert to numeric

#Okay, you have the number queried - now standardize answers
#there are a lot of answers here that don't make a ton of sense (text and numbers)
mathfacts.df %<>%
  mutate(num_answer_standard = ifelse(num_answer == "IDK", -1000, 
                                      ifelse(num_answer == "idk", -1000, as.character(num_answer)))) #IDK = -1000

#because it would be crazy to convert every answer to something intelligible, create a lookup table so we can check whether the answer is valid based on the number prompted
starting_num <- c(1, 20, 21, 32, 5, 57, 64, 86, 93)
answer <- c(2, 21, 22, 33, 6, 58, 65, 87, 94)
lookup <- data.frame(starting_num, answer) #create a lookup table

#this is a function that looks at every row for every problem, and then checks whether the answer given is correct based on the lookup table
check_math_facts <- function() {
  for (row in 1:nrow(mathfacts.df)) {
    num = mathfacts.df[row, "num_queried"]
    answer = subset(lookup, starting_num == num)$answer
    if (answer != mathfacts.df[row, "num_answer_standard"]) {
      mathfacts.df[row, "correct"] = 0
    } else {
     mathfacts.df[row, "correct"] = 1 
    }
  }
  return(mathfacts.df)
}

#run this function on math facts df
mathfacts.df <- check_math_facts()

```

###Productivity
```{r}
hc <- highestcount.df %>% # replace with your local path
  select(SID, last_successful, initial_highest, final_highest) %>%
  mutate_at(c('last_successful','initial_highest','final_highest'),
            function(col) as.integer(str_replace_all(col,'\\D',''))) %>% # some of these included '?', so i remove any char thats not a digit
  mutate(last_successful = ifelse(is.na(last_successful), 120, last_successful))
# 
# 
is.productive = function(subject){
  # takes as input the data for a single subject
  # RULES:
  # - counts to 120 unaided = productive
  # - after making first error, counts >= 20 higher, with no more than 3 errors on way
  if(subject$initial_highest[1] >= 120){
    # if they get to 120 on first try, = productive
    return(TRUE)
  } else if(subject$final_highest[1] == 120 & nrow(subject) < 4) {
    return(TRUE)
  } else if(subject$final_highest[1] < 120 & nrow(subject) == 1 
            & subject$final_highest[1] == subject$initial_highest[1]) {
    return(FALSE)
  } else if((subject$final_highest[1] - subject$initial_highest[1]) >= 20){
    # if their final is >= 20 larger than their intial...
    if(nrow(subject) < 4){
      # and they've made 3 or fewer total errors, = productive
      return(TRUE)
    } 
    else {
      for(i in 1:nrow(subject)){ # start at row 2
        # check if they ever made it >= 20 counts & <= 3 errors after an error
        runLength = 0 # they just made an error, so no post-error successes yet
        numErrors = 0 # first row was an error if it's not finalCount == 120
        prev = subject$last_successful[i]
        for (j in i+1:nrow(subject)){ # from current row until end...
          numErrors = numErrors + 1 # new row means new error
          runLength = runLength + (subject$last_successful[j] - prev)
          # ^ add difference between current count and last count to run length
          prev = subject$last_successful[j] # update last count
          if(runLength >= 20 & numErrors < 4){
            # if at any point the productivity conditions are met...
            return(TRUE) # = productive
          }
        }
      }
      # productivity conditions were never met (because we got to this point) so...
      return(FALSE) # != productive
    }
  } else {
    # highest is not >= 20 greater than initial
    return(FALSE)
  }
}

# #issues: not working for multiple errors
# #last successful doesn't not always equal final highest count
# 
#make function to run for all participants
unique_SIDs <- as.vector(unique.highestcount$SID)
# 
class_prod <- function(vector) {
  temp_data <- data.frame()
  for (i in vector) {
    prod.class <- data.frame(i, is.productive(subset(hc, SID == i)))
    names(prod.class) <- c("SID", "productive")
    temp_data <- bind_rows(temp_data, prod.class)
  }
  return(temp_data)
}
# 

productive <- class_prod(unique_SIDs)%>%
  rename(check_prod = productive)%>%
  mutate(check_prod = ifelse(check_prod == TRUE, "productive", "nonproductive"))

productive_SIDS <- productive %>%
  filter(check_prod == "productive")%>%
  distinct(SID)

#assign these to a vector
prod_SIDS <- as.vector(productive_SIDS$SID)

#This is a function that checks a SID against the productive SIDs in the vector above, and then classifies a participant as Productive or Nonproductive
productivity_classification <- function(df) {
  tmp <- df %>%
    mutate(productive = ifelse(SID %in% prod_SIDS, "productive", "nonproductive"))
  return(tmp)
}

#Assign productivity across all tasks
sf.df <- productivity_classification(sf.df)
nextnumber.df <- productivity_classification(nextnumber.df)
mathfacts.df <- productivity_classification(mathfacts.df)
indefinite.df <- productivity_classification(indefinite.df)

```

###Within and outside of count range
Trials for SF, NN, and MF need to be classified as within or outside of count range for each participant
```{r}
#first, get initial highest count for each kiddo
#Make a lookup table with SID and initial highest count
lookup <- highestcount.df %>%
  distinct(SID, initial_highest)

#This is a function that, for each trial, checks the number queried. If number queried is above the child's initial highest count, marks that trial as beyond count range.
determine_count_range <- function(df) {
  tmp <- df
  for (row in 1:nrow(tmp)) {
    sub = as.character(tmp[row, "SID"])
    count_range = subset(lookup, SID == sub)$initial_highest
    tmp[row, "highest_count"] = count_range
    if (tmp[row, "num_queried"] > count_range) {
      tmp[row, "count_range"] = "outside"
    } else {
      tmp[row, "count_range"] = "within"
    }
  }
  return(tmp)
}

#Run for each task
sf.df <- determine_count_range(sf.df)
nextnumber.df <- determine_count_range(nextnumber.df)
mathfacts.df <- determine_count_range(mathfacts.df)
```

###Highest next number
Find the highest next number answered correctly for each participant.
```{r}
#Create a lookup table with the highest NN correctly answered
lookup <- nextnumber.df %>%
  filter(correct == 1)%>%
  group_by(SID)%>%
  summarise(max = max(num_queried))

#Function that adds the highest NN to a participant's row in the SF dataframe
add_highest_num <- function(df) {
  tmp <- df
  for (row in 1:nrow(tmp)) {
    sub = as.character(tmp[row, "SID"])
    highest_num = subset(lookup, SID == sub)$max
    tmp[row, "highest_num"] = highest_num
  }
  return(tmp)
}

#run this function on SF dataframe
sf.df <- add_highest_num(sf.df)
```

***

#Demographics
```{r}
sf.df %>%
  mutate(age = round(age, 2))%>%
  distinct(SID, age, sex)%>%
  group_by(sex)%>%
  summarise(n = n(), mean_age = round(mean(age,2)), sd_age = round(sd(age),2), median_age = round(median(age),2))

mean(sf.df$age)
sd(sf.df$age)

hist(sf.df$age)
```

```{r}
###This was exploratory, but no need to run
# errors <- highestcount.df %>%
#   filter(!is.na(error), 
#          !is.na(prompt), 
#          productive == "productive")
# 
# final_highest <- highestcount.df%>%
#   filter(!is.na(error), 
#          !is.na(prompt), 
#          productive == "productive")%>%
#   distinct(SID, final_highest)
# 
# final_highest.x <- highestcount.df %>%
#   filter(!is.na(error), 
#          !is.na(prompt), 
#          productive == "productive", 
#          error_num < 8)
# 
# limited_fhc <- function() {
#   subs <- as.vector(unique(final_highest.x$SID))
#   
#   fhcs <- data.frame()
#   for(sub in subs){
#     x <- final_highest.x %>%
#       dplyr::filter(SID == sub)
#     
#     sub_x_max_error <- max(x$error_num)
#     
#     x %<>%
#       dplyr::filter(error_num == sub_x_max_error)
#     fhcs <- bind_rows(fhcs, x)
#   }
#   return(fhcs)
# }
# 
# final_limited <- limited_fhc() %>%
#   select(SID, productive, last_successful)%>%
#   dplyr::rename(limited_final_highest = last_successful)%>%
#   mutate(limited_final_highest = ifelse(limited_final_highest > 120, 120, limited_final_highest))
# 
# tmp <- right_join(final_highest, final_limited, by = "SID")%>%
#   mutate(diff.final = final_highest - limited_final_highest)
# 
# mean(tmp$diff.final)
# sd(tmp$diff.final)
# hist(tmp$diff.final)
# x <- tmp %>%
#   filter(diff.final == 0)
# 
# #spread limited final to model.df

```

***

#Planned analyses
Our main planned analyses are model comparisons between generalized linear models predicting SF performance from Productivity and Math Facts. We also preregistered correlations between continuous variables.  

##Models
Make dataframe for model analyses with all the relevant info.
```{r}
#Need to add initial and final highest counts for each participant
#Create a lookup df for this
lookup <- highestcount.df %>%
  distinct(SID, initial_highest, final_highest)

check <- lookup %>%
  group_by(SID)%>%
  summarise(n = n())

#this is a function that adds initial and final highest counts to the model df
add_counts <- function(df) {
  tmp <- df
  for (row in 1:nrow(tmp)) {
    sub = as.character(tmp[row, "SID"])
    initial = subset(lookup, SID == sub)$initial_highest
    final = subset(lookup, SID == sub)$final_highest
    tmp[row, "initial_highest"] = initial
    tmp[row, "final_highest"] = final
  }
  return(tmp)
}

#build model df from SF df
model.df <- add_counts(sf.df)

#Now add mean math facts performance
#create a lookup with their SID and mean MF performance
lookup <- mathfacts.df %>%
  group_by(SID)%>%
  filter(trial != 1)%>%
  summarise(mean = mean(correct))%>%
  mutate(SID = as.character(SID))

#this is a function for adding mean MF performance to the model df
add_mathmean <- function(df) {
  tmp <- df
  for (row in 1:nrow(tmp)) {
    sub = as.character(tmp[row, "SID"])
    mean_math = subset(lookup, SID == sub)$mean
    tmp[row, "mf_mean"] = mean_math
  }
  return(tmp)
}

#Add mean MF performance to model df
model.df <- add_mathmean(model.df)

#some data structures manipulation
model.df %<>%
  mutate(final_highest = as.integer(final_highest), #make sure this is an integer
         productive = factor(productive), #change from character
         correct = as.integer(correct), #change to integer
         count_range = factor(count_range)) %>%
  filter(trial != 1, 
         trial != 2)

# #add final_limited
# lookup <- final_limited 


```

####Planned models
Counting and productivity models:
To identify whether there is connection between counting experience and Successor Task performance, we will conduct three initial analyses, predicting Successor Task performance from either (1) Initial Highest Count, (2) Productivity (defined above), (3) final highest count, or (4) performance on the Next Number task. 

All models will be logistic mixed effects models, predicting performance on a trial as a function of the following predictors, with a random intercept for subject. In R, the formula will be glmer(SF_correct ~ (predictor) + age + (1|subject), family = binomial).

Simple models: 
Model 1: Successor.Performance ~ Highest.Next.Number  + Within/Outside range + Age + (1|subject)
Model 2: Successor.Performance ~ Initial.Count + Within/Outside range + Age + (1|subject)
Model 3: Successor.Performance ~ Final.Count + Within/Outside range + Age + (1|subject)
Model 4: Successor.Performance ~ Productivity+ Within/Outside range + Age + (1|subject)

After running these first four models, any predictor that significantly (p <.05) predicted Successor Performance will be added into Model 5, which will be our “Large” model (containing all predictors that significantly predicted Successor Performance in the simple models).

We will construct model 5 hierarchically. Model comparisons will be performed at each stage by running a likelihood ratio test between reduced and full models, with significant effects retained in the full model (Model 5). Model selection will be done on the basis of AIC evaluation and significant Chi-square statistic.

Math Facts & Successor Task Performance: 
To identify whether there is connection between math fact knowledge and Successor Task performance, we will conduct one primary analysis, which predict Successor Task performance from performance on the Math Facts task. 
Model 6: Successor.Performance ~ Math.Facts + Within/Outside range + Age + (1|subject)

After running this model, if there is a significant relationship between Math Facts and Successor performance, we will construct Model 7, which includes both Math Facts and all of the significant predictors from Model 5 (above). 

To test whether Model 7 (containing Math Facts) fits the data significantly better than the reduced Model 5 (without Math Facts), we will conduct a likelihood ratio test of the two models, using AIC and Chi-square statistic to assess whether Math Facts performance significantly contributes to predicting SF performance. 

###Model 1: Highest Next Number
NB that all model coefficients are in log odds.
```{r}
#model 1
#Successor.Performance ~ Highest.Next.Number  + Within/Outside range + Age + (1|subject)
base <- glmer(correct ~ count_range + age + (1|SID), 
                 family = "binomial", data = model.df)
model.1 <- glmer(correct ~ highest_num + count_range + age + (1|SID), 
                 family = "binomial", data = model.df)

#we have convergence issues with model 1, check if there is a failure to converge
with(model.1@optinfo$derivs,max(abs(solve(Hessian,gradient)))<2e-3) #true, converged correctly

#compare model with and without Highest NN
anova(base, model.1, test = 'LRT')
base.AIC <- anova(base, model.1, test = 'LRT')$AIC[1]
highestNN.AIC <- anova(base, model.1, test = 'LRT')$AIC[2]

#summary
summary(model.1)
```

###Model 1 Results
In comparison to base model, there is a significant effect of Highest Next Number (Chisq(5) = `r round(anova(base, model.1, test = 'LRT')$Chisq[2], 2)`, p = `r round(anova(base, model.1, test = 'LRT')$Pr[2], 2)`.)

###Model 2: Initial Highest Count
```{r}
#model 2
#Successor.Performance ~ Initial.Count + Within/Outside range + Age + (1|subject)
model.2 <- glmer(correct ~ initial_highest + count_range + age + (1|SID), 
                                  family = "binomial", data = model.df)

#compare model with and without IHC
anova(base, model.2)
IHC.AIC<- anova(base, model.2)$AIC[2]

#summary
summary(model.2)
```

###Model 2 Results
Interestingly, we do not find a signficant effect of Initial Highest Count. In comparison to base model, adding IHC does not significantly improve the fit of the model (Chisq(5) = `r round(anova(base, model.2)$Chisq[2], 2)`, p = `r round(anova(base, model.2)$Pr[2], 2)`.)

###Model 3: Final Highest Count
```{r}
#model 3
#Successor.Performance ~ Final.Count + Within/Outside range + Age + (1|subject)
model.3 <- glmer(correct ~ final_highest + count_range + age + (1|SID), 
                 family = "binomial", data = model.df)

#convergence issues - is this an issue?
with(model.3@optinfo$derivs,max(abs(solve(Hessian,gradient)))<2e-3) #true, we've converged correctly

#compare model with and without FHC
anova(base, model.3, test = 'LRT')
FHC.AIC <- anova(base, model.3, test = 'LRT')$AIC[2]

summary(model.3)
```

###Model 3 Results
Final highest count is a significant predictor of Successor performance. In comparison to base model, the model fit is significantly improved by adding FHC (Chisq(5) = `r round(anova(base, model.3, test = 'LRT')$Chisq[2], 2)`, p = `r round(anova(base, model.3, test = 'LRT')$Pr[2], 2)`.)

###Model 4: Productive Classification
```{r}
#model 4
#Successor.Performance ~ Productivity+ Within/Outside range + Age + (1|subject)
model.4 <- glmer(correct ~ productive + count_range + age + (1|SID), 
                 family = "binomial", data = model.df)

#compare model with and without productivity
anova(base, model.4, test = 'LRT')
Productive.AIC <- anova(base, model.4, test = 'LRT')$AIC[2]

summary(model.4)
with(model.4@optinfo$derivs,max(abs(solve(Hessian,gradient)))<2e-3)
```

###Model 4 Results
Our productive classification is a significant predictor of Successor task performance. In comparison to base model, adding Productive classification significantly improves the fit of the model (Chisq(5) = `r round(anova(base, model.4, test = 'LRT')$Chisq[2], 2)`, p = `r round(anova(base, model.4, test = 'LRT')$Pr[2], 2)`.)

###Model 5: Large Model, all significant from 1-4
Because we have more than one signficant predictor, we are putting all significant Productivity predictors into one large model. We are usinga stepwise process to build this model, with comparisons performed at each step using a Likelihood Ratio Test. Model selection is done on the basis of AIC and significant (p < .05) Chisq. 
```{r}
##Any predictor that significantly (p <.05) predicted Successor Performance will be added into Model 5, which will be our “Large” model (containing all predictors that significantly predicted Successor Performance in the simple models).
#Model 1: Highest next number 
#Model 2: Initial highest 
#Model 3: Final highest 
#Model 4: Productive 

#Base model for comparisons
model.5.base <- glmer(correct ~ count_range + age + (1|SID), 
                      family = "binomial", data = model.df)
#add final, compare
model.5.final <- glmer(correct ~ final_highest + count_range + age + (1|SID), 
                 family = "binomial", data = model.df)

#convergence issues, check
with(model.5.final@optinfo$derivs,max(abs(solve(Hessian,gradient)))<2e-3) #true, we're ok

#compare FHC to base without FHC
anova(model.5.base, model.5.final, test = 'LRT')
```

Step 1: Add FHC, compare to base model. Significantly improves model fit (Chisq(5) = `r round(anova(model.5.base, model.5.final, test = 'LRT')$Chisq[2], 2)`, p = `r round(anova(model.5.base, model.5.final, test = 'LRT')$Pr[2], 2)`.)


Now add Highest Next Number, IHC, and Productivity, and compare to FHC model fit.
```{r}
##add highest
model.5.highest_nn <- glmer(correct ~ highest_num + final_highest + count_range + age + (1|SID), 
                 family = "binomial", data = model.df)

#model unstable, check and see if convergence warning is legit
with(model.5.highest_nn@optinfo$derivs,max(abs(solve(Hessian,gradient)))<2e-3) #true, we've converged adequately

#now test
anova(model.5.final, model.5.highest_nn, test = 'LRT')
NN.FHC.AIC <- anova(model.5.highest_nn, model.5.final, test = 'LRT')$AIC[2]

#PRODUCTIVITY
#add to final highest
model.5.prod <- glmer(correct ~ productive + final_highest + count_range + age + (1|SID), 
                 family = "binomial", data = model.df)

#convergence warning issues, check to see if this is legit
with(model.5.prod@optinfo$derivs,max(abs(solve(Hessian,gradient)))<2e-3)#true, we're ok

#now test
anova(model.5.final, model.5.prod, test = 'LRT')
Prod.FHC.AIC <- anova(model.5.prod,model.5.final, test = 'LRT')$AIC[2]

##IHC
#add IHC
model.5.IHC <- glmer(correct ~ initial_highest + final_highest + count_range + age + (1|SID), 
                 family = "binomial", data = model.df)

#convergence warnings - is this an issue?
with(model.5.IHC@optinfo$derivs,max(abs(solve(Hessian,gradient)))<2e-3)#true, we've adequately converged

#now test
anova(model.5.final, model.5.IHC, test = 'LRT')
IHC.FHC.AIC <- anova(model.5.IHC,model.5.final, test = 'LRT')$AIC[2]
```

###Model 5 Results
Final highest count seems to be doing the best job predicting SF performance. In model comparisons:
FHC alone vs. FHC + Highest NN: Chisq(6) = `r round(anova(model.5.highest_nn, model.5.final, test = 'LRT')$Chisq[2], 2)`, p = `r round(anova(model.5.highest_nn, model.5.final, test = 'LRT')$Pr[2], 2)`.
FHC alone vs. FHC + Productivity: Chisq(6) = `r round(anova(model.5.prod,model.5.final, test = 'LRT')$Chisq[2], 2)`, p = `r round(anova(model.5.prod,model.5.final, test = 'LRT')$Pr[2], 2)`.
FHC alone vs. FHC + IHC: Chisq(6) = `r round(anova(model.5.IHC,model.5.final, test = 'LRT')$Chisq[2], 2)`, p = `r round(anova(model.5.IHC,model.5.final, test = 'LRT')$Pr[2], 2)`.

###Model 6 - math facts
Is mean performance on the math facts task predictive of SF performance?
```{r}
#Model 6: Successor.Performance ~ Math.Facts + Within/Outside range + Age + (1|subject)
model.6 <- glmer(correct ~ mf_mean + count_range + age + (1|SID), 
                 family = "binomial", data = model.df)
model.6.base <- glmer(correct ~ count_range + age + (1|SID), 
                 family = "binomial", data = model.df)
#compare model with and without math facts
anova(model.6, model.6.base, test = 'LRT')
MF.AIC <- anova(model.6, model.6.base, test = 'LRT')$AIC[2]

```

###Model 6 Results
Math Facts *is* a significant predictor of SF performance. Model comparison with base indicates that adding Math Facts does significantly improve model fit (Chisq(5) = `r round(anova(model.6, model.6.base, test = 'LRT')$Chisq[2], 2)`, p = `r round(anova(model.6, model.6.base, test = 'LRT')$Pr[2], 2)`.)

###Model 7: Productivity vs. Math Facts
Because Math Facts is a significant predictor of SF performance, we're adding it to our Productivity model, which currently contains just Final Highest count, and doing a model comparison to see if it significantly improves the fit of the model. 
```{r}
model.7.base <- glmer(correct ~ final_highest + count_range + age + (1|SID), 
                 family = "binomial", data = model.df)

#convergence warnings - is this an issue?
with(model.7.base@optinfo$derivs,max(abs(solve(Hessian,gradient)))<2e-3)#true, we're okay

model.7.mf <- glmer(correct ~ mf_mean + final_highest + count_range + age + (1|SID), 
                 family = "binomial", data = model.df)
#convergence warnings
with(model.7.mf@optinfo$derivs,max(abs(solve(Hessian,gradient)))<2e-3) #true we're ok

#compare model with and without math facts
anova(model.7.base, model.7.mf, test = 'LRT')
FHC.MF.AIC <- anova(model.7.base, model.7.mf, test = 'LRT')$AIC[2]
```

###Model 7 Results
Math Facts does significantly improve the model fit (Chisq(6) = `r round(anova(model.7.base, model.7.mf, test = 'LRT')$Chisq[2], 2)`, p = `r round(anova(model.7.base, model.7.mf, test = 'LRT')$Pr[2], 2)`.) We have about half of our data currently, so we will see what happens here!

###Model visualization: AIC
It's a little hard to keep track of all of the various model fits, so I'm plotting a visualization of the AIC - we chose models on the basis of a lower AIC and significant Chisq statistic. 
```{r}
Models <- c("Base", "Highest NN", "IHC", "FHC", "Productivity", "FHC + Highest NN", "FHC + Productivity", "IHC + FHC", "Math Facts", "FHC + Math Facts")
AIC <- c(base.AIC, highestNN.AIC, IHC.AIC, FHC.AIC, Productive.AIC, NN.FHC.AIC, Prod.FHC.AIC, IHC.FHC.AIC, MF.AIC, FHC.MF.AIC)

full.AIC <- data.frame(Models, AIC)
full.AIC %<>%
  mutate(Models = factor(full.AIC$Models , 
                         levels = full.AIC[order(full.AIC$AIC), 1]))

full <- ggplot(full.AIC, aes (x = Models, y = AIC, fill = Models)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  theme_bw(base_size = 13) +
  theme(panel.grid.minor = element_blank(), 
        panel.grid.major = element_blank(), 
        axis.text.x = element_text(angle = 45, hjust = 1)) + 
  labs(fill = "") + guides(fill = FALSE) +
  langcog::scale_fill_solarized("AIC") +
  coord_cartesian(ylim=c(1300, 1450))
full

##for FYP
limited.AIC <- full.AIC %>%
  filter(Models == "Base" | Models == "FHC" | Models == "Math Facts" | Models == "FHC + Math Facts")

limited.AIC %<>%
  mutate(Models = factor(Models, levels = c("Base", "FHC", "Math Facts", "FHC + Math Facts")))

model.pal <- c("#268BD2", "#3E7570", "#51AEA5", "#C0DCD8")

fyp.aic <- ggplot(limited.AIC, aes (x = Models, y = AIC, fill = Models)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  theme_bw(base_size = 15) +
  theme(panel.grid.minor = element_blank(), 
        panel.grid.major = element_blank(), 
        axis.text.x = element_text(angle = 25, hjust = 1)) + 
  labs(fill = "") + guides(fill = FALSE) +
  scale_fill_manual(values = model.pal) +
  coord_cartesian(ylim=c(1300, 1450))
fyp.aic
```

###Exploratory model: Highest contiguous NN
We did not account for accuracy in highest NN. Thus, it is possible for a child to answer a very high NN by chance, which would add some noise to this measure. Below, I'm pulling out the children's highest *contiguous* NN.

Note that there are two ways to define contiguous - contiguous in terms of the magnitude of the numbers, and contiguous WRT to the number's position in the task. The below is pulling out numbers WRT magnitude.
```{r}
unique.nn <- nextnumber.df %>%
  distinct(SID)

unique.nn <- as.vector(unique.nn$SID)
nextnums <- as.vector(c(7, 24, 26, 30, 62, 71, 83, 95))

get_contiguous <- function(){
  contig <- data.frame()
  for (sub in unique.nn) {
    tmp <- nextnumber.df %>%
      filter(SID == sub, 
             correct == 0)%>%
      mutate(num_queried = sort(num_queried))
    if (length(tmp$SID) == 0) {
      highest_contig = 95
      sub_contig <- data.frame(sub, highest_contig)
      contig <- bind_rows(contig, sub_contig)
    } else if (length(tmp$num_queried) > 0 & min(tmp$num_queried) == 7) {
      highest_contig = 1
      sub_contig <- data.frame(sub, highest_contig)
      contig <- bind_rows(contig, sub_contig)
    } else {
      min.nn <- min(tmp$num_queried)
      prev_correct <- nextnums[nextnums < min.nn]
      highest_contig <- max(prev_correct)
    
      sub_contig <- data.frame(sub,
                             highest_contig)
      contig <- bind_rows(contig, sub_contig)
    }
  }
  return(contig)
}

highest_contiguous_nn <- get_contiguous()%>%
  dplyr::rename(SID = sub)

model.df <- right_join(model.df, highest_contiguous_nn, by = "SID")

#compare to FHC
model.exp.final <- glmer(correct ~ highest_contig + final_highest + count_range + age + (1|SID), 
                         family = "binomial", data = model.df)
model.exp.final.base <- glmer(correct ~ final_highest + count_range + age + (1|SID), 
                         family = "binomial", data = model.df)
#compare to model with FHC
anova(model.exp.final, model.exp.final.base, test = 'LRT')

#convergence warnings - check if it is an issue
with(model.exp.final@optinfo$derivs,max(abs(solve(Hessian,gradient)))<2e-3)
summary(model.exp.final)

#add highest contiguous 
model.exp.highest_contig <- glmer(correct ~ highest_contig + mf_mean + final_highest +
                                    count_range + age + (1|SID), 
                                  family = "binomial", data = model.df)
model.exp.contig.base <- glmer(correct ~ highest_contig + final_highest + count_range +
                                 age + (1|SID), family = "binomial", data = model.df)

anova(model.exp.contig.base , model.exp.highest_contig, test = 'LRT')

#######
model.exp.final.nn <- glmer(correct ~ highest_num + highest_contig + final_highest + count_range + age + (1|SID), 
                         family = "binomial", data = model.df)

anova(model.exp.final, model.exp.final.nn, test = 'LRT')

```

###Exploratory Model Results
The addition of Highest Contiguous NN significantly improves the fit of the model (Chisq(6) = `r round(anova(model.exp.final, model.exp.final.base, test = 'LRT')$Chisq[2], 2)`, p = `r round(anova(model.exp.final, model.exp.final.base, test = 'LRT')$Pr[2], 2)`).

###Exploratory model: Productivity*Count range, sf task
```{r}
#is the interaction significant?
tmp <- model.df %>%
  mutate(productive = factor(productive, levels = c("productive", "nonproductive")))

model.exp.base <- glmer(correct ~ productive + count_range + age + (1|SID), 
                        family = "binomial", data = model.df)
model.exp.int <- glmer(correct ~ productive*count_range + age + (1|SID), 
                       family = "binomial", data = model.df)
anova(model.exp.base, model.exp.int, test = 'LRT')
summary(model.exp.int)
```

###Exploratory model: Predicting SF from indefinite number performance
```{r}
#need to pull mean indefinite number into model.df
#First, get all participants who completed indefinite number
indefinite.subs <- indefinite.df %>%
  distinct(SID, age)

indefinite.subs <- as.vector(indefinite.subs$SID)

#Now, get only those SIDs from the model df
indef.model.df <- model.df %>%
  filter(SID %in% indefinite.subs)

#get mean indefinite performance and spread
lookup <- indefinite.df %>%
  group_by(SID)%>%
  filter(trial != 1)%>%
  summarise(mean = mean(correct, na.rm = TRUE))%>%
  mutate(SID = as.character(SID))

#this is a function for adding mean MF performance to the model df
add_indefinite <- function(df) {
  tmp <- df
  for (row in 1:nrow(tmp)) {
    sub = as.character(tmp[row, "SID"])
    mean_indefinite = subset(lookup, SID == sub)$mean
    tmp[row, "indef.mean"] = mean_indefinite
  }
  return(tmp)
}

indef.model.df <- add_indefinite(indef.model.df)

#predict SF from indefinite performance, compare to base
exp.id.model.base <- glmer(correct ~ count_range + age + (1|SID), 
                      family = "binomial", data = indef.model.df)
exp.id.model.indef <- glmer(correct ~ indef.mean + count_range + age + (1|SID), 
                      family = "binomial", data = indef.model.df)
anova(exp.id.model.base, exp.id.model.indef, test = 'LRT') #significant, compare to final highest count

exp.id.model.final <- glmer(correct ~ final_highest + count_range + age + (1|SID), 
                      family = "binomial", data = indef.model.df)
#convergence warnings, check if this is an issue
with(exp.id.model.final@optinfo$derivs,max(abs(solve(Hessian,gradient)))<2e-3) #true, we're ok
exp.id.model.both <- glmer(correct ~ final_highest + indef.mean + count_range + age + (1|SID), 
                      family = "binomial", data = indef.model.df)
#convergence warnings - check if this is an issue
with(exp.id.model.both@optinfo$derivs,max(abs(solve(Hessian,gradient)))<2e-3)#true, we're ok

#test FHC and indefinite performance
anova(exp.id.model.both, exp.id.model.final, test = 'LRT')


```

#Exploratory model: Predicting probability from highest count 
```{r}
model.df %<>%
  mutate(productive.log = ifelse(productive == "productive", 1, 0))

model.prod <- model.df %>%
  distinct(SID, age, productive.log, initial_highest, final_highest)

model.prod.prob <- glmer(productive.log ~ initial_highest + age + (1|SID), 
                    family = "binomial", data = model.prod)

model.prod.prob <- glmer(productive.log ~ final_highest + age + (1|SID), 
                    family = "binomial", data = model.prod)
model.prod.prob.base <- glmer(productive.log ~  age + (1|SID), 
                    family = "binomial", data = model.prod)
with(model.prod.prob@optinfo$derivs,max(abs(solve(Hessian,gradient)))<2e-3)
anova(model.prod.prob, model.prod.prob.base, test = 'LRT')
```
***

##Correlation between all continuous variables and all other continuous variables
Here, we have correlations between the following variables:
1. Age
2. Final Highest Count
3. Initial Highest Count 
4. Mean Successor performance
5. Mean Next Number performance
6. Mean Math Facts performance
```{r}
#get mean performance for SF, MF, and NN
sf.mean <- model.df %>%
  filter(trial != 1, 
         trial != 2) %>%
  group_by(SID, age, final_highest, initial_highest)%>%
  dplyr::summarise(mean.sf = mean(correct))

mf.mean <- mathfacts.df %>%
  filter(trial != 1)%>%
  group_by(SID)%>%
  summarise(mean.mf = mean(correct))

nextnumber.mean <- nextnumber.df %>%
  filter(trial != 1)%>%
  group_by(SID)%>%
  summarise(mean.nn = mean(correct))

tmp <- right_join(sf.mean, mf.mean, by = "SID")
cor.mean <- right_join(tmp, nextnumber.mean, by = "SID") #dataframe for correlations
```

###Age
```{r}
cor.test(cor.mean$age, cor.mean$initial_highest)
cor.test(cor.mean$age, cor.mean$final_highest)
```
####Significant correlation:
1. Age and IHC: r = `r round(as.numeric(cor.test(cor.mean$age, cor.mean$initial_highest)$estimate), 2)`, p = `r round(as.numeric(cor.test(cor.mean$age, cor.mean$initial_highest)$p.value), 2)`
2. Age and FHC: r = `r round(as.numeric(cor.test(cor.mean$age, cor.mean$final_highest)$estimate), 2)`, p = `r round(as.numeric(cor.test(cor.mean$age, cor.mean$final_highest)$p.value), 2)`

###Successor task
```{r}
#SF
cor.test(cor.mean$mean.sf, cor.mean$age)
cor.test(cor.mean$mean.sf, cor.mean$initial_highest)
cor.test(cor.mean$mean.sf, cor.mean$final_highest)
cor.test(cor.mean$mean.sf, cor.mean$mean.mf)
cor.test(cor.mean$mean.sf, cor.mean$mean.nn)
```
####Significant correlations between:
1. SF and age: r = `r round(as.numeric(cor.test(cor.mean$mean.sf, cor.mean$age)$estimate), 2)`, p = `r round(as.numeric(cor.test(cor.mean$mean.sf, cor.mean$age)$p.value), 2)`
2. SF and IHC: r = `r round(as.numeric(cor.test(cor.mean$mean.sf, cor.mean$initial_highest)$estimate), 2)`, p = `r round(as.numeric(cor.test(cor.mean$mean.sf, cor.mean$initial_highest)$p.value), 2)`
3. SF and FHC: r = `r round(as.numeric(cor.test(cor.mean$mean.sf, cor.mean$final_highest)$estimate), 2)`, p = `r round(as.numeric(cor.test(cor.mean$mean.sf, cor.mean$final_highest)$p.value), 2)`
4. SF and MF: r = `r round(as.numeric(cor.test(cor.mean$mean.sf, cor.mean$mean.mf)$estimate), 2)`, p = `r round(as.numeric(cor.test(cor.mean$mean.sf, cor.mean$mean.mf)$p.value), 2)`
5. SF and NN:  r = `r round(as.numeric(cor.test(cor.mean$mean.nn, cor.mean$mean.mf)$estimate), 2)`, p = `r round(as.numeric(cor.test(cor.mean$mean.sf, cor.mean$mean.nn)$p.value), 2)`

###Math Facts
```{r}
#MF
cor.test(cor.mean$mean.mf, cor.mean$age)
cor.test(cor.mean$mean.mf, cor.mean$initial_highest)
cor.test(cor.mean$mean.mf, cor.mean$final_highest)
cor.test(cor.mean$mean.mf, cor.mean$mean.sf)
cor.test(cor.mean$mean.mf, cor.mean$mean.nn)
```
####Significant correlations between: 
1. MF and age: r = `r round(as.numeric(cor.test(cor.mean$mean.mf, cor.mean$age)$estimate), 2)`, p = `r round(as.numeric(cor.test(cor.mean$mean.mf, cor.mean$age)$p.value), 2)`
2. MF and IHC: r = `r round(as.numeric(cor.test(cor.mean$mean.mf, cor.mean$initial_highest)$estimate), 2)`, p = `r round(as.numeric(cor.test(cor.mean$mean.mf, cor.mean$initial_highest)$p.value), 2)`
3. MF and FHC: r = `r round(as.numeric(cor.test(cor.mean$mean.mf, cor.mean$final_highest)$estimate), 2)`, p = `r round(as.numeric(cor.test(cor.mean$mean.mf, cor.mean$final_highest)$p.value), 2)`
4. MF and SF: r = `r round(as.numeric(cor.test(cor.mean$mean.mf, cor.mean$mean.sf)$estimate), 2)`, p = `r round(as.numeric(cor.test(cor.mean$mean.mf, cor.mean$mean.sf)$p.value), 2)`
5. MF and NN: r = `r round(as.numeric(cor.test(cor.mean$mean.mf, cor.mean$mean.nn)$estimate), 2)`, p = `r round(as.numeric(cor.test(cor.mean$mean.mf, cor.mean$mean.nn)$p.value), 2)`

###Next Number
```{r}
#NN
cor.test(cor.mean$mean.nn, cor.mean$age)
cor.test(cor.mean$mean.nn, cor.mean$initial_highest)
cor.test(cor.mean$mean.nn, cor.mean$final_highest)
cor.test(cor.mean$mean.nn, cor.mean$mean.sf)
cor.test(cor.mean$mean.nn, cor.mean$mean.mf)
```
####Significant correlations between: 
1. NN and age: r = `r round(as.numeric(cor.test(cor.mean$mean.nn, cor.mean$age)$estimate), 2)`, p = `r round(as.numeric(cor.test(cor.mean$mean.nn, cor.mean$age)$p.value), 2)`
2. NN and IHC: r = `r round(as.numeric(cor.test(cor.mean$mean.nn, cor.mean$initial_highest)$estimate), 2)`, p = `r round(as.numeric(cor.test(cor.mean$mean.nn, cor.mean$initial_highest)$p.value), 2)`
3. NN and FHC: r = `r round(as.numeric(cor.test(cor.mean$mean.nn, cor.mean$final_highest)$estimate), 2)`, p = `r round(as.numeric(cor.test(cor.mean$mean.nn, cor.mean$final_highest)$p.value), 2)`
4. NN and SF: r = `r round(as.numeric(cor.test(cor.mean$mean.nn, cor.mean$mean.sf)$estimate), 2)`, p = `r round(as.numeric(cor.test(cor.mean$mean.nn, cor.mean$mean.sf)$p.value), 2)`
5. NN and MF: r = `r round(as.numeric(cor.test(cor.mean$mean.nn, cor.mean$mean.mf)$estimate), 2)`, p = `r round(as.numeric(cor.test(cor.mean$mean.nn, cor.mean$mean.mf)$p.value), 2)`

***

##Error classification on Highest count
We did not plan any analyses here. Below is the total numbers of each error type (decade-beginning, decade-final, mid-decade); note that there are more errors for Productive rather than Nonproductive because Productive errors as a group counted higher, and were more likely to make errors).
```{r}
decades <- c("9", "19", "29", "39", "49", "59", "69", 
             "79", "89", "99", "109", "119")
beginning_decade <- c("10", "20", "30", "40", "50", "60", 
                      "70", "80", "90", "100", "110")

errors <- highestcount.df %>%
  mutate(error_type = ifelse(as.character(last_successful) %in% decades, "Decade-final",
                             ifelse(is.na(last_successful), "NaN",
                                    ifelse(as.character(last_successful) %in% beginning_decade, 
                                           "Decade-beginning", "Mid-decade"))))%>%
  filter(error_type != "NaN")%>%
  group_by(productive, error_type)%>%
  dplyr::summarise(n = n())%>%
  group_by(productive)%>%
  mutate(total = sum(n), 
         prop = n/total)

# productivity.colors <- c("#4daf4a","#984ea3")
productivity.color <- c('#0083B2', '#8061A6')

#visualization of error types by productivity
error_plot <- ggplot(errors, aes(x = error_type, y = n, fill = factor(productive, 
                                                        levels = c("productive", "nonproductive"), 
                                                        labels = c("Productive", "Nonproductive")))) + 
  geom_bar(stat = "identity", position = position_dodge()) +
  theme_bw(base_size = 13) + 
  scale_fill_manual(values = productivity.colors) +
  labs(x = "Error type", y = "Number of errors", fill = "")
error_plot
```

***

#Exploratory analyses
##Partial correlation between Indefinite and SF, controlling for age
Exploratory data - To explore whether children’s performance on the Successor Task is related to their ability to generalize the Successor to all numbers, we will run a partial correlation between performance on both tasks, controlling for age.
```{r}
#First, get all participants who completed indefinite number
indefinite.subs <- indefinite.df %>%
  distinct(SID, age)

indefinite.subs <- as.vector(indefinite.subs$SID)

#Now, get only those SIDs from the SF df
indef.sf <- sf.df %>%
  filter(SID %in% indefinite.subs)

#compute means for both tasks
indef.mean <- indefinite.df %>%
  group_by(SID, age)%>%
  summarise(mean.indef = mean(correct, na.rm = TRUE))

indef.sf.mean <- indef.sf %>%
  filter(trial != 1, 
         trial != 2)%>%
  group_by(SID)%>%
  summarise(mean.sf = mean(correct, na.rm = TRUE))

#join the two dfs together
indef.pcor <- right_join(indef.mean, indef.sf.mean, by = "SID")

library(ppcor)
partial <- pcor.test(indef.pcor$mean.indef, indef.pcor$mean.sf, indef.pcor$age)
partial
```
Significant correlation between mean performance on SF and Indefinite tasks when controlling for age (r = `r round(as.numeric(partial$estimate), 2)`, p = `r round(as.numeric(partial$p.value), 2)`). 

##T-tests by productivity on SF/MF/NN
Using the Productive/Non-Productive categorical classification outlined above, we will compare mean performance between both groups on all tasks (Next Number, Successor, and Math Facts) in t-tests.
```{r}
##create a dataframe for mean task performance for each subject for each task
#next number
mean.nn <- nextnumber.df %>%
  filter(trial != 1) %>%
  group_by(SID, age, productive) %>%
  summarise(mean.nn = mean(correct, na.rm = TRUE))

#math facts
mean.mf <- mathfacts.df %>%
  filter(trial != 1) %>%
  group_by(SID) %>%
  summarise(mean.mf = mean(correct, na.rm = TRUE))

#successor
mean.sf <- sf.df %>%
  filter(trial != 1, 
         trial != 2)%>%
  group_by(SID) %>%
  summarise(mean.sf = mean(correct, na.rm = TRUE))

#bind these together
tmp <- right_join(mean.nn, mean.mf, by = "SID")
all.mean <- right_join(tmp, mean.sf, by = "SID")


#t-tests
#next number
nn.t.test <- t.test(subset(all.mean, productive == "productive")$mean.nn,
                    subset(all.mean, productive == "nonproductive")$mean.nn, var.equal = TRUE)
nn.t.test
#successor function
sf.t.test <- t.test(subset(all.mean, productive == "productive")$mean.sf,
                    subset(all.mean, productive == "nonproductive")$mean.sf, var.equal = TRUE)
sf.t.test
#math facts
mf.t.test <- t.test(subset(all.mean, productive == "productive")$mean.mf,
                    subset(all.mean, productive == "nonproductive")$mean.mf, var.equal = TRUE)
mf.t.test
```
There is a significant difference between Productive and Nonproductive counters on all tasks:
Next Number: t(51) = `r round(as.numeric(nn.t.test$statistic), 2)`, p = `r round(as.numeric(nn.t.test$p.value), 2)`
Successor: t(51) = `r round(as.numeric(sf.t.test$statistic), 2)`, p = `r round(as.numeric(sf.t.test$p.value), 2)`
Math Facts: t(51) = `r round(as.numeric(mf.t.test$statistic), 2)`, p = `r round(as.numeric(mf.t.test$p.value), 2)`

##Accuracy ~ Problem * Task: Is performance mediated by task?
```{r}
##This is determining whether the numbers queried were novel or same
##Same = numbers seen in more than one task (either SF + NN, or SF + MF)
##Novel = numbers only appeared in that task
#first, get a vector of all the numbers used in each task
sf_nums <- sf.df %>%
  distinct(num_queried)

sf_nums <- as.vector(sf_nums$num_queried)

math_facts_nums <- mathfacts.df %>%
  distinct(num_queried)

mf_nums <- as.vector(math_facts_nums$num_queried)

nn_nums <- nextnumber.df %>%
  distinct(num_queried)

nn_nums <- as.vector(nn_nums$num_queried)

##make a function to label novel or same
novel_same <- function(df) {
  tmp <- df
  for (row in 1:nrow(tmp)) {
    num = tmp[row, "num_queried"]
    if (num %!in% sf_nums) {
     tmp[row, "novel_same"] = "novel" 
    } else {
      tmp[row, "novel_same"] = "same"
    }
  }
  return(tmp)
}

math_nn <- function() {
  for (row in 1:nrow(sf.df)) {
    num = sf.df[row, "num_queried"]
    if (num %in% mf_nums) {
      sf.df[row, "novel_same"] = "same"
    } else if (num %in% nn_nums) {
      sf.df[row, "novel_same"] = "same"
    } else {
      sf.df[row, "novel_same"] = "novel"
    }
  }
  return(sf.df)
}

#remove training
sf.df <- math_nn()%>%
  filter(num_queried != 1,
         num_queried != 3)
nextnumber.df <- novel_same(nextnumber.df) %>%
  filter(num_queried != 1)
mathfacts.df <- novel_same(mathfacts.df)  %>%
  filter(problem != "1+1")

#make the appropriate df with SID, age, productive, correct, task, novel_same, count range
sf.limited <- sf.df %>%
  dplyr::select(SID, age, productive, num_queried, correct, novel_same, count_range)%>%
  mutate(task = "Successor")

nn.limited <- nextnumber.df %>%
  dplyr::select(SID, age, productive, num_queried, correct, novel_same, count_range)%>%
  mutate(task = "Next Number")

mf.limited <- mathfacts.df %>%
  dplyr::select(SID, age, productive, num_queried, correct, novel_same, count_range)%>%
  mutate(task = "Math Facts")

all.limited <- bind_rows(sf.limited, nn.limited, mf.limited)%>%
  mutate(SID = factor(SID), 
         productive = factor(productive), 
         correct = as.integer(correct), 
         novel_same = factor(novel_same), 
         count_range = factor(count_range), 
         task = factor(task, levels = c("Successor", "Next Number", "Math Facts")))

##Is accuracy mediated by task? 
##Correct ~ Task*novel_same + Within/outside + Age + (1|SID)

model.post1 <- glmer(correct ~ task*novel_same + count_range + age + (1|SID), 
                     family = "binomial", data = all.limited)
model.post.base <- glmer(correct ~ task + novel_same + count_range + age + (1|SID), 
                     family = "binomial", data = all.limited)

#model fails to converge - is this a big deal? check
with(model.post1@optinfo$derivs,max(abs(solve(Hessian,gradient)))<2e-3) 

#LRT to compare whether interaction is significant
anova(model.post1, model.post.base, test = 'LRT')

summary(model.post1)
```
Currently, it does not appear that the interaction of task and problem type (Novel/Same) is significant (Chisq(9) = `r round(anova(model.post1, model.post.base, test = 'LRT')$Chisq[2], 2)`, p = `r round(anova(model.post1, model.post.base, test = 'LRT')$Pr[2], 2)`.)

At the moment, it appears that we have a main effect of task, such that performance on Math Facts is significantly less likely to be accurate than Successor task (Beta = `r round(summary(model.post1)$coeff[3], 2)`, p = `r round(summary(model.post1)$coeff[4,3], 2)`). There are also significant effects of age and count range.

##Indefinite number
First, visualizing indefinite number performance. Then seeing whether productivity is a significant predictor of indefinite.
```{r}
indef.sub <- indefinite.df %>%
  distinct(SID, productive)%>%
  group_by(productive)%>%
  summarise(n = n())

indef <- indefinite.df %>%
  group_by(productive)%>%
  langcog::multi_boot_standard("correct", na.rm = TRUE)

productivity.colors <- c("#cb4b16","#6c71c4")
indef.plot <- ggplot(indef, aes(x = factor(productive, 
                             levels = c("productive", "nonproductive"), 
                             labels = c("Productive", "Nonproductive")), 
                             y = mean, fill= factor(productive, 
                             levels = c("productive", "nonproductive"), 
                             labels = c("Productive", "Nonproductive")))) + 
  geom_bar(stat = "identity", position = position_dodge()) +
  geom_linerange(aes(ymin = ci_lower,
                      ymax = ci_upper),
                  size = .3,
                  show.legend = FALSE,
                 position=position_dodge(width = 0.9)) +
  theme_bw(base_size = 15) +
  labs(x = "Productivity", y = "Mean indefinite performance") + guides(fill = FALSE) +
  scale_fill_manual(values = productivity.colors)
indef.plot

base <- glmer(correct ~ age + (1|SID), family = "binomial", data = indefinite.df)
model.1.indef <- glmer(correct ~ productive + age + (1|SID), family = "binomial", data = indefinite.df)
anova(base, model.1.indef, test = 'LRT')
summary(model.1.indef)
```
Productivity is a significant predictor of Indefinite number task performance (Chisq(4) = `r round(anova(base, model.1.indef, test = 'LRT')$Chisq[2], 2)`, p = `r round(anova(base, model.1.indef, test = 'LRT')$Pr[2], 2)`).

###Exploratory: Cluster analysis
Exploratory modeling of our counting data to test the predictions of Yang (2017). Here, we plan to use cluster analysis to identify distinct groups (or categories) of counters from our highest count data, assessing the likelihood of Successor Knowledge from cluster membership.

**NB**I am working on this, but still learning. So far, it looks like I can identify some clusters, but that's pretty much it. There's a lot of tweaking that can be done, I think. Note that this seems to be working fairly well for the nonproductive-low counters!

```{r}
library(mclust)
clust <- highestcount.df %>%
  dplyr::select(initial_highest, final_highest)

BIC <- mclustBIC(clust)
plot(BIC)
summary(BIC)
mod1 <- Mclust(clust, x = BIC)
summary(mod1, parameters = TRUE)
plot(mod1, what = "classification")
```

***

#Individual task visualizations

##Highest count
```{r}
#Highest count descriptives
highestcount.df %>%
  group_by(productive)%>%
  summarise(m_age = mean(age), 
            sd_age = sd(age),
            mean_initial = mean(initial_highest), 
            sd_initial = sd(initial_highest), 
            median_initial = median(initial_highest), 
            mean_final = mean(final_highest), 
            sd_final = sd(final_highest), 
            median_final = median(final_highest))

highestcount.df %>% 
  distinct(SID, productive)%>%
  group_by(productive)%>%
  summarise(n = n())

mean(highestcount.df$initial_highest)
mean(highestcount.df$final_highest)

```
While Productive and Nonproductive counters seem to have somewhat similar mean IHC at this point (although Productive counters are able to count, on average, somewhat higher), the really interesting difference is in their FHC; Productive counters have a much higher mean FHC than Nonproductive counters.

###Scatterplot of IHC and FHC.
Relation between initial and final highest count, grouped by productivity
```{r}
#relation between initial and final
initial_final <- highestcount.df %>%
  distinct(SID, age, initial_highest, final_highest, productive)%>%
  mutate(initial_highest = as.numeric(initial_highest), 
         initial_final = as.numeric(final_highest))

productivity.color <- c('#5185CE', '#005A52')
highest_count <- ggplot(initial_final, aes(x = initial_highest, y = final_highest, 
                          color = factor(productive, levels = c("productive","nonproductive"), labels = c("Productive", "Nonproductive")))) +
  geom_point(size = 2) + geom_jitter(width = .1) +
  labs(x = "Initial highest count", y = "Final highest count", 
                      color = "", title = "") +
  theme_bw(base_size = 15) + 
  scale_x_continuous(breaks = seq(0, 120, 10)) + 
  scale_y_continuous(breaks = seq(0, 120, 10)) + 
  theme(panel.grid.minor = element_blank(), 
        legend.position = "bottom", 
        legend.text = element_text(size = 10)) + 
  scale_color_manual(values = productivity.color)
highest_count
```
A visualization of IHC and FHC. We can see that our productivity measure is doing a pretty good job capturing meaningful differences between kids, with only a few potentially misclassified high and low counters.

###Histograms of initial and final highest count distributions, grouped by productivtiy
```{r}
#distribution of initial
initial_hist <- ggplot(initial_final, aes(x = initial_highest, 
                          fill = factor(productive, levels = c("productive", "nonproductive"), 
                                   labels = c("Productive", "Nonproductive")))) + 
  geom_histogram(colour = "black") +
  scale_x_continuous(name="Initial highest count", breaks=seq(0,120,10)) +
  theme_bw() + 
  labs(y = "Count", title = "Distribution of initial highest counts", 
       fill = "Productivity Classification") + 
  theme(panel.grid.minor = element_blank()) 
  scale_fill_manual(values = productivity.colors)
initial_hist

final <- ggplot(initial_final, aes(x = final_highest, 
                          fill = factor(productive, levels = c("productive", "nonproductive"), labels = c("Productive", "Nonproductive")))) + 
  geom_histogram(colour = "black") +
  scale_x_continuous(name="Final highest count", breaks=seq(0,120,10)) +
  theme_bw() + 
  labs(y = "Count", title = "Distribution of final highest counts", 
       fill = "Productivity Classification") + 
  theme(panel.grid.minor = element_blank()) +
  scale_fill_manual(values = productivity.colors)
final
```

##Successor Task
This is a sanity check to make sure that performance doesn't significantly differ between experiment halves (i.e., checking to make sure that performance isn't dropping because kids are getting bored.)
```{r}
#is there a significant accuracy difference between first and second half of experiment? 
sf.df %<>%
  mutate(half = ifelse(trial == 1 | trial == 2, "practice",
                       ifelse(trial > 2 & trial < 11, "first", "second")))


#significant difference between first and second half?
t.test(subset(sf.df, half == "first")$correct, subset(sf.df, half == "second")$correct, 
       var.equal = TRUE)

sf.model <- sf.df %>%
  filter(half != "practice")

#use mem to account for subject-level variability
m1 <- glmer(correct ~ half + (1 | SID), data=sf.model, 
            family=binomial, control=glmerControl(optimizer="bobyqa",
                            optCtrl=list(maxfun=2e5)))
m0 <- glmer(correct ~  1  + (1 | SID), data=sf.model, 
            family=binomial, control=glmerControl(optimizer="bobyqa",
                            optCtrl=list(maxfun=2e5)))
anova(m0,m1)
```

No significant effect of half, meaning that performance is not significantly lower in the second half of the Successor Task. 

###Accuracy by within/outside range by productivity
```{r}
sf.range.ms <- sf.df %>%
  filter(half != "practice") %>%
  group_by(SID, count_range, productive)%>%
  multi_boot_standard("correct", na.rm = TRUE)

sf <- ggplot(sf.range.ms, aes(x = factor(count_range, levels = c("within", "outside"), 
                                      labels = c("Within", "Outside")), y = mean, 
                        fill = factor(productive, levels = c("productive", "nonproductive"), 
                                   labels = c("Productive", "Nonproductive")))) + 
  geom_boxplot() +
  theme_bw(base_size = 15) +
  theme(legend.text = element_text(size = 15), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  labs(x = "Within/outside unprompted count range", y = "Mean Successor performance",  
       fill = "") +
  scale_fill_manual(values = productivity.color)
sf

```

##Next number
###Accuracy within/outside by productivity
```{r}
nn.range.ms <- nextnumber.df %>%
  filter(num_queried != 1) %>%
  group_by(count_range, productive)%>%
  multi_boot_standard("correct", na.rm = TRUE)

nn <- ggplot(nn.range.ms, aes(x = factor(productive, levels = c("productive", "nonproductive"), 
                                   labels = c("Productive", "Nonproductive")), y = mean, 
                        fill = factor(count_range, levels = c("within", "outside"), 
                                      labels = c("Within initial highest count", "Outside initial highest count")))) + 
  geom_bar(stat = "identity", position= position_dodge()) + 
  geom_linerange(aes(ymin = ci_lower,
                      ymax = ci_upper),
                  size = .3,
                  show.legend = FALSE,
                 position=position_dodge(width = 0.9)) +
  theme_bw(base_size = 13) +
  theme(legend.text = element_text(size = 10), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  labs(x = "Productivity", y = "Mean Next Number performance",  
       fill = "") +
  langcog::scale_fill_solarized("Within/outside count range")
nn
```

##Math facts
###Accuracy by productivity and range
```{r}
mf.range.ms <- mathfacts.df %>%
  filter(problem != "1+1")%>%
  group_by(count_range, productive)%>%
  multi_boot_standard("correct", na.rm = TRUE)

mf <- ggplot(mf.range.ms, aes(x = factor(productive, levels = c("productive", "nonproductive"), 
                                   labels = c("Productive", "Nonproductive")), y = mean, 
                        fill = factor(count_range, levels = c("within", "outside"), 
                                      labels = c("Within initial highest count", "Outside initial highest count")))) + 
  geom_bar(stat = "identity", position= position_dodge()) + 
  geom_linerange(aes(ymin = ci_lower,
                      ymax = ci_upper),
                  size = .3,
                  show.legend = FALSE,
                 position=position_dodge(width = 0.9)) +
  theme_bw(base_size = 13) +
  theme(legend.text = element_text(size = 10), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  labs(x = "Productivity", y = "Mean Math Facts performance",  
       fill = "") +
  langcog::scale_fill_solarized("Within/outside count range")
mf
```

***

##SF/NN/MF - visualizations of all three tasks together
These are essentially the same graphs as above - I just combined all the tasks so we can make comparisons more easily. 
```{r}
##This is determining whether the numbers queried were novel or same
##Same = numbers seen in more than one task (either SF + NN, or SF + MF)
##Novel = numbers only appeared in that task
#first, get a vector of all the numbers used in each task
sf_nums <- sf.df %>%
  distinct(num_queried)

sf_nums <- as.vector(sf_nums$num_queried)

math_facts_nums <- mathfacts.df %>%
  distinct(num_queried)

mf_nums <- as.vector(math_facts_nums$num_queried)

nn_nums <- nextnumber.df %>%
  distinct(num_queried)

nn_nums <- as.vector(nn_nums$num_queried)

##make a function to label novel or same
novel_same <- function(df) {
  tmp <- df
  for (row in 1:nrow(tmp)) {
    num = tmp[row, "num_queried"]
    if (num %!in% sf_nums) {
     tmp[row, "novel_same"] = "novel" 
    } else {
      tmp[row, "novel_same"] = "same"
    }
  }
  return(tmp)
}

math_nn <- function() {
  for (row in 1:nrow(sf.df)) {
    num = sf.df[row, "num_queried"]
    if (num %in% mf_nums) {
      sf.df[row, "novel_same"] = "same"
    } else if (num %in% nn_nums) {
      sf.df[row, "novel_same"] = "same"
    } else {
      sf.df[row, "novel_same"] = "novel"
    }
  }
  return(sf.df)
}

#remove training
sf.df <- math_nn()%>%
  filter(num_queried != 1,
         num_queried != 3)
nextnumber.df <- novel_same(nextnumber.df) %>%
  filter(num_queried != 1)
mathfacts.df <- novel_same(mathfacts.df)  %>%
  filter(problem != "1+1")


```

###Mean task performance
```{r}
#get mean perf by subject for each task
mean.nn <- nextnumber.df %>%
  filter(trial != 1) %>%
  group_by(SID) %>%
  summarise(mean = mean(correct, na.rm = TRUE))%>%
  mutate(task = "next number")

mean.mf <- mathfacts.df %>%
  filter(trial != 1) %>%
  group_by(SID) %>%
  summarise(mean = mean(correct, na.rm = TRUE))%>%
  mutate(task = "math_facts")

mean.sf <- sf.df %>%
  filter(trial != 1, 
         trial != 2) %>%
  group_by(SID) %>%
  summarise(mean = mean(correct, na.rm = TRUE))%>%
  mutate(task = "successor")

#bind these together
tmp <- bind_rows(mean.nn, mean.mf)
all.mean <- bind_rows(tmp, mean.sf)

#take mean of means
all.mean.ms <- all.mean %>%
  group_by(task)%>%
  multi_boot_standard("mean", na.rm = TRUE)

task.colors = c("#3288BD", "#9E0142","#5E4FA2")


#plot
mean_tasks <- ggplot(all.mean, aes(x = factor(task,levels = c("successor", "next number",
                                                            "math_facts"),
                                            labels = c("Successor", "Next Number",
                                                       "Math Facts")),
                     y = mean, fill = factor(task,levels = c("successor", "next number",
                                                            "math_facts"),
                                            labels = c("Successor", "Next Number",
                                                       "Math Facts")))) +
  geom_boxplot() +
  theme_bw(base_size = 15) + 
  scale_fill_manual(values = task.colors) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) +
  labs(x = "Task", y = "Mean performance", 
       fill = "")
mean_tasks

####BOXPLOT
sf.limited <- sf.df %>%
  dplyr::select(SID, age, productive, trial, count_range, correct)%>%
  mutate(task = "Successor")

nn.limited <- nextnumber.df %>%
  dplyr::select(SID, age, productive, trial, count_range, correct)%>%
  mutate(task = "Next Number")

mf.limited <- mathfacts.df %>%
  dplyr::select(SID, age, productive, trial, count_range, correct)%>%
  mutate(task = "Math Facts")

three_limited <- bind_rows(sf.limited, nn.limited, mf.limited)

three_box <- ggplot(all.mean, aes(x = task, y = mean, fill = 
                                         factor(productive, levels = c("productive",
                                                                      "nonproductive"),
                                                labels = c("Productive",
                                                           "Nonproductive")))) +
   geom_boxplot() +
  theme_bw(base_size = 15) +
  theme(legend.text = element_text(size = 15), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  labs(x = "Within/outside unprompted count range", y = "Mean Successor performance",  
       fill = "") +
  scale_fill_manual(values = productivity.color)
```

###Mean task performance, within/outside count range
```{r}
#make dataframe for mean performance within/outside of count range
novel.nn <- nextnumber.df %>%
  filter(trial != 1) %>%
  mutate(count_range = factor(count_range, levels = c("within", "outside"),
                              labels = c("Within", "Outside")))%>%
  group_by(SID, age, count_range) %>%
  summarise(mean = mean(correct, na.rm = TRUE))%>%
  mutate(task = "next number")

novel.mf <- mathfacts.df %>%
  filter(trial != 1)%>%
  mutate(count_range = factor(count_range, levels = c("within", "outside"),
                              labels = c("Within", "Outside")))%>%
  group_by(SID, age, count_range) %>%
  summarise(mean = mean(correct, na.rm = TRUE))%>%
  mutate(task = "math_facts")

novel.sf <- sf.df %>%
  filter(trial != 1, 
         trial != 2)%>%
 mutate(count_range = factor(count_range, levels = c("within", "outside"),
                              labels = c("Within", "Outside")))%>%
  group_by(SID, age, count_range) %>%
  summarise(mean = mean(correct, na.rm = TRUE))%>%
  mutate(task = "successor")

#bind these together
tmp <- bind_rows(novel.nn, novel.mf)
all.novel <- bind_rows(tmp, novel.sf)

#take mean of means
all.novel.ms <- all.novel %>%
  group_by(task, count_range)%>%
  multi_boot_standard("mean", na.rm = TRUE)

novel_tasks <- ggplot(all.novel.ms, aes(x = count_range, 
                                  y = mean, fill = factor(task,levels = 
                                                            c("successor", "next number", "math_facts"),
                                                          labels = c("Successor", "Next Number", "Math Facts")))) +
  geom_bar(stat = "identity", position = position_dodge()) +
  geom_linerange(aes(ymin = ci_lower,
                      ymax = ci_upper),
                  size = .3,
                  show.legend = FALSE,
                 position=position_dodge(width = 0.9))+
  theme_bw(base_size = 15) + 
  scale_fill_manual(values = task.colors) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) +
  labs(x = "Within/outside count range", y = "Mean performance", 
       fill = "")
novel_tasks
```

###Mean performance by productivity
```{r}
#make df for mean performance
productive.nn <- nextnumber.df %>%
  filter(trial != 1)%>%
  mutate(productive = factor(productive, levels = c("productive", "nonproductive"), 
                             labels = c("Productive", "Nonproductive")))%>%
  group_by(SID, age, productive) %>%
  summarise(mean = mean(correct, na.rm = TRUE))%>%
  mutate(task = "next number")

productive.mf <- mathfacts.df %>%
  filter(trial != 1)%>%
  mutate(productive = factor(productive, levels = c("productive", "nonproductive"), 
                             labels = c("Productive", "Nonproductive")))%>%
  group_by(SID, age, productive) %>%
  summarise(mean = mean(correct, na.rm = TRUE))%>%
  mutate(task = "math_facts")

productive.sf <- sf.df %>%
  filter(trial != 1, 
         trial != 2)%>%
 mutate(productive = factor(productive, levels = c("productive", "nonproductive"), 
                             labels = c("Productive", "Nonproductive")))%>%
  group_by(SID, age, productive) %>%
  summarise(mean = mean(correct, na.rm = TRUE))%>%
  mutate(task = "successor")

#bind these together
tmp <- bind_rows(productive.nn, productive.mf)
all.productive <- bind_rows(tmp, productive.sf)

#take mean of means
all.productive.ms <- all.productive %>%
  group_by(task, productive)%>%
  multi_boot_standard("mean", na.rm = TRUE)

productive_tasks <- ggplot(all.productive.ms, aes(x = productive, 
                                  y = mean, fill = factor(task,levels = 
                                                            c("successor", "next number", "math_facts"),
                                                          labels = c("Successor", "Next Number", "Math Facts")))) +
  geom_bar(stat = "identity", position = position_dodge()) +
  geom_linerange(aes(ymin = ci_lower,
                      ymax = ci_upper),
                  size = .3,
                  show.legend = FALSE,
                 position=position_dodge(width = 0.9))+
  theme_bw(base_size = 15) + 
  scale_fill_manual(values = task.colors) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) +
  labs(x = "Productivity", y = "Mean performance", 
       fill = "")
productive_tasks
```

***

###Scatterplot of FHC and SF; MF mean performance and SF mean performance
```{r}
#sf and FHC
sf.hc.ms <- sf.df %>%
  filter(trial != 1, 
         trial != 2)%>%
  group_by(SID, highest_count)%>%
  summarise(mean = mean(correct), 
            log.mean = log10(mean))

model.weight <- glm(correct ~ final_highest, family = "binomial", data = model.df)
range(sf.df$highest_count)
xweight <- seq(10, 130, 10)

sf.hc.scatter <- ggplot(sf.df, aes(x = highest_count, y = correct)) +
  geom_point(stat = "identity") +
  geom_smooth(method = "lm", color = "blue") + 
  theme_bw(base_size = 15) +
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  labs(x = "Final Highest Count", y = "y")
sf.hc.scatter

##Math Facts and SF
#get mean math facts performance
mf.sf.ms <- model.df %>%
  dplyr::select(SID, mf_mean)

#Join with SF, get distinct SIDs
scatter.mf.sf <- right_join(sf.hc.ms, mf.sf.ms, by = "SID")%>%
  distinct(SID, mean, mf_mean)

mfsf.scatter <- ggplot(scatter.mf.sf, aes(x = mf_mean, y = mean)) +
  geom_point(stat = "identity") +
  geom_smooth(method = "lm", color = "blue") + 
  theme_bw(base_size = 15) +
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  labs(x = "Mean Math Facts performance", y = "Mean Successor performance")
mfsf.scatter
```