---
title: "SF-Math analysis"
author: "Rose Schneider"
date: "5/13/2018"
output:
  html_document: default
  pdf_document: default
---
```{r setup, include=FALSE}
rm(list = ls())
require("knitr")
library(tidyverse)
library(magrittr)
library(langcog)
library(lme4)
library(stringr)    
library(RColorBrewer)
library(ggthemes)

'%!in%' <- function(x,y)!('%in%'(x,y))
```

---

#Setup
##Loading data
Raw data is read in and processed in sf-math_dataProc.R, which is called here. Processed data are then read in
```{r}
source_r <- function(file, local = FALSE, ...){
  options(knitr.duplicate.label = 'allow')

  tempR <- tempfile(tmpdir = ".", fileext = ".R")
  on.exit(unlink(tempR))
  knitr::purl(file, output=tempR, quiet = TRUE)

  envir <- globalenv()
  source(tempR, local = envir, ...)
}

source_r("sf-math_dataProc.R")
#this results in a cleaned .csv, re-compiled every time from original data
```

##Read in processed data
```{r}
all.data <- read.csv("../Data/sf-math_data_processed.csv")
```
---

#Exclusions 
##Global exclusions
Children were excluded from the analysis only if a) they did not complete the highest count task or Give N, or b) their exclusion was noted by the experimenter. 

Pre-exclusion kids
```{r}
##How many kids pre-exclusions?
all.data %>%
  distinct(SID)%>%
  summarise(n = n())
```

Exclusion reasons
```{r}
#Why are kids being excluded?
all.data %>%
  filter(Exclude_analysis == 1)%>%
  distinct(SID, Exclude_analysis, Exclude_analysis_reason)%>%
  group_by(Exclude_analysis_reason)%>%
  summarise(n = n())%>%
  mutate(total.n = sum(n)) 
```

```{r}
#exclude these kids from analysis
all.data %<>%
  filter(Exclude_analysis != 1)
```

How many kids left after exclusions?
```{r}
#How many kids are left after exclusions
all.data %>%
  distinct(SID)%>%
  summarise(n = n())%>%
  mutate(total.n = sum(n))
```

Sex
```{r}
all.data %>%
  distinct(SID, Sex)%>%
  group_by(Sex)%>%
  summarise(n = n())
```

###Task exclusions
Children were excluded from a given task if they did not complete at least ONE trials of that task (in addition to the training trial). In order to be considered as having completed a trial of the task, a child must at least say "I don't know."  These children were excluded manually.
```{r}
#how many kids are excluded from which tasks
all.data %>%
  filter(Exclude_task == 1)%>%
  distinct(SID, Exclude_task, Excluded_task, Exclude_task_reason)

#exclude
all.data %<>%
  filter(Exclude_task != 1)
```

##Excluded trials
Trials where a participant gave no response were excluded from analysis.

How many trials were excluded? Why?
```{r}
#how many trials excluded, and for what reason
all.data %>%
  filter(Exclude_trial == 1)%>%
  group_by(Task, Exclude_trial_reason)%>%
  summarise(n = n())

#exclude these trials
all.data %<>%
  filter(Exclude_trial != 1)
```

How many kids failed training? In what task?
```{r}
#how many kids failed training
all.data %>%
  filter(Trial_number == "Training", #note that only one kid failed first trial of SF, no kid failed all
         Correct == 0)%>%
  group_by(Task)%>%
  summarise(n = n())

#filter out training trials 
all.data %<>%
  filter(Trial_number != "Training")

#how many trials do we have for each task?
all.data %>%
  filter(Task == "SF" | 
         Task == "WCN" |
           Task == "MF")%>%
  group_by(Task)%>%
  summarise(n = n()) 
```

---
#More classifications (post exclusion)

##Within/outside count range
Each trial on the Unit or WCN task was determined to be either within or outside a child's unprompted count range (IHC).
```{r}
all.data %<>%
  mutate(count_range = ifelse((Task == "SF" | Task == "WCN" | Task == "MF") & 
                                as.numeric(as.character(Task_item)) <= IHC, "Within", 
                              ifelse((Task == "SF" | Task == "WCN" | Task == "MF") &
                                       as.numeric(as.character(Task_item)) > IHC, "Outside", NA))) %>%
  mutate(count_range = factor(count_range, levels = c("Within", "Outside")))
```

##Mean performance for Indefinite, Unit, NN, and MF
Mean performance on the indefinite number task for children who received it.
```{r}
compute_means <- function(df, TaskName) {
  tmp.ms <- df %>%
    filter(Task == TaskName) %>%
    group_by(SID)%>%
    summarise(mean = mean(Correct, na.rm = TRUE))

  return(tmp.ms)
}

#run for indefinite, mf, unit, and wcn
indef.mean <- compute_means(all.data, "Indefinite") %>%
  dplyr::rename("mean.indef" = "mean")

mf.mean <- compute_means(all.data, "MF") %>%
  dplyr::rename("mean.mf" = "mean")

sf.mean <- compute_means(all.data, "SF") %>%
  dplyr::rename("mean.sf" = "mean")

nn.mean <- compute_means(all.data, "WCN") %>%
  dplyr::rename("mean.nn" = "mean")

#temporary df name to make reduce run
tmp.all.data <- all.data

#bind all these little guys to the main df
all.data <- list(tmp.all.data, indef.mean, mf.mean, sf.mean, nn.mean) %>% reduce(full_join, by = "SID")

```

---

##Demographics
```{r}
all.data %>%
  distinct(SID, Age)%>%
  summarise(mean = round(mean(Age), 2), 
            sd = round(sd(Age), 2))%>%
  kable()

#sex
all.data %>%
  distinct(SID, Sex)%>%
  group_by(Sex)%>%
  summarise(n = n())%>%
  kable()
```

---

#Primary analyses
First, make the model df
```{r}
model.df <- all.data %>%
  mutate(highest_contig = as.integer(highest_contig), 
         SID = factor(SID), 
         age.c = as.vector(scale(Age, center = TRUE, scale=TRUE)), #scale and center continuous variables for model fit
         fhc.c = as.vector(scale(FHC, center = TRUE, scale = TRUE)), 
         ihc.c = as.vector(scale(IHC, center = TRUE, scale = TRUE)),
         highest_contig.c = as.vector(scale(highest_contig, center = TRUE, scale=TRUE)), 
         highest_num.c = as.vector(scale(highest_num, center = TRUE, scale=TRUE)),
         mean.mf.c = as.vector(scale(mean.mf, center = TRUE, scale=TRUE)),
         mean.indef.c = as.vector(scale(mean.indef, center = TRUE, scale=TRUE)),
         starting_num.c = as.vector(scale(as.numeric(as.character(Task_item)), center = TRUE, scale = TRUE)))%>%
  filter(Task == "SF") # we're only predicting Unit Task performance, so filter down to this
```

##Step 1: Productivity models
Which measure of productivity best fits the data?
```{r}
#general function for making models so we don't have to re-write it a million times

make_single_model <- function(df, modelType) {
  if (modelType == "Base") { 
    model <- glmer(Correct ~ count_range + age.c + (1|SID), 
                   family = "binomial", data = df)
  } else {
    formula <- paste("Correct ~", modelType, "+ count_range + age.c + (1|SID)")
    model <- glmer(formula, data = df, family = "binomial")
  }
  return(model)
}

#base
sf.base <- make_single_model(model.df, "Base")

#IHC
sf.ihc <- make_single_model(model.df, "ihc.c")

#FHC
sf.fhc <- make_single_model(model.df, "fhc.c")

#Productive
sf.productive <- make_single_model(model.df, "Productive")

#HCNN
sf.hcnn <- make_single_model(model.df, "highest_contig.c")

#HNN
sf.hnn <- make_single_model(model.df, "highest_num.c")


model.df %<>%
  mutate(Task_item = as.numeric(as.character(Task_item)))

```

##Regression table
```{r}
library(memisc)
mtable.sf <- mtable('Base Model' = sf.base,
            'Model 1: IHC' = sf.ihc,
            'Model 2: FHC' = sf.fhc,
            'Model 3: Productivity.' = sf.productive,
            'Model 4: HCNN' = sf.hcnn,
            'Model 5: HNN' = sf.hnn,
            summary.stats = c('Nagelkerke R-sq.','Log-likelihood','AIC','N'))
mtable.sf
```

##Step 2: Now use LRTs to test whether a given productivity predictor is significantly improving the fit of the base
IHC: Chisq(1) = 19.65, *p* > .0001; AIC = 2701.3
```{r}
anova(sf.base, sf.ihc, test = 'LRT')
```

FHC: Chisq(1) = 32.508, *p* < .0001, AIC = 2688.5
```{r}
anova(sf.base, sf.fhc, test = 'LRT')
```

Productivity/Resilience: Chisq(1) = 11.213, *p* = .0008, AIC = 2709.8
```{r}
anova(sf.base, sf.productive, test = 'LRT')
```

Highest Contiguous NN: Chisq(1) = 70.798, *p* < .0001, AIC = 2650.2
```{r}
anova(sf.base, sf.hcnn, test = 'LRT')
```

Follow-up: Highest Next Number: Chisq(1) = 10.237, *p* = .001, AIC = 2710.8
```{r}
tmp <- anova(sf.base, sf.hnn, test = 'LRT')
```

##Ordering of productivity predictor by AIC: 
HCNN: 2650.2
FHC: 2688.5
IHC: 2701.3
Productive: 2709.8

(follow-up) HNN: 2710.8


##Step 3: Large model building
Does a given productivity predictor explain unique or overlapping variance? Begin with HCNN, which has the lowest AIC. 
```{r}
#this is a function that compares a given base model to a full model which is constructed by adding some number of predictors in a list
#function tests whether the FIRST item in a list significantly improves the fit of the model
#there's a bit of hardcoding in here, but it makes life a little easier

#Remember to update the base model used on the basis of the function output
test_predictor <- function(base_model, parameters, df) {
  reduced <- base_model
  if (length(parameters) == 1) {
    formula <- paste("Correct ~", parameters[1], "+ highest_contig.c + count_range + age.c + (1|SID)")
    full <- glmer(formula, data = model.df, family = "binomial")

  } else {
    mult_params <- str_flatten(unlist(parameters), collapse = " + ") #get params out of list into str
    
    formula <- paste("Correct ~", mult_params, "+ highest_contig.c + count_range + age.c + (1|SID)")
    full <- glmer(formula, data = model.df, family = "binomial")
  }
  
  lrt <- anova(reduced, full, test = 'LRT')
  
  if (lrt$`Pr(>Chisq)`[2] < .05) {
    print(paste(parameters[1], "significantly improves fit, update base model"))
    return(full)
    # return(lrt)
  }
  else {
    print(paste(parameters[1], "does NOT improve model, do NOT update base model"))
    return(full)
    # return(lrt)
  }
}
```

Build a base model with HCNN (lowest AIC)
```{r}
#HCNN - base model
sf.large.base <- glmer(Correct ~ highest_contig.c + count_range + age.c + (1|SID), 
                       family = "binomial", data = model.df)
```


Test if FHC improves base model: FHC has next lowest AIC
It does: Chisq(1) = 8.35, *p* = 0.004, AIC = 2648.3
```{r}
sf.large.fhc <- test_predictor(sf.large.base, c("fhc.c"))

#get actual anova
anova(sf.large.base, sf.large.fhc, test = 'LRT')
```

Test if IHC (next lowest AIC) improves this model
It does not: Chisq(1) = 0.422, *p* = 0.5, AIC = 2645.4
```{r}
sf.large.plusihc <- test_predictor(sf.large.fhc, c("ihc.c", "fhc.c"))

#anova for actual values
anova(sf.large.fhc, sf.large.plusihc, test= 'LRT')
```

Test if Productivity improves this model
It doesn't: Chisq(1) = 1.18, *p* = 0.27, AIC = 2644.7
```{r}
sf.large.plusprod <- test_predictor(sf.large.fhc, c("Productive", "fhc.c"))

#anova for actual values
anova(sf.large.fhc, sf.large.plusprod, test= 'LRT')
```

Full table with all models tested above
```{r}
##model table
mtable.sf.large <- mtable('Base Model: HCNN' = sf.large.base,
            'Model 1: FHC + HCNN' = sf.large.fhc,
            'Model 2: IHC + FHC + HCNN' = sf.large.plusihc,
            'Model 3: Prod. + FHC + HCNN' = sf.large.plusprod,
            summary.stats = c('Nagelkerke R-sq.','Log-likelihood','AIC','N'))
mtable.sf.large
```

###Final productivity predictors: HCNN and FHC

---

#Math Facts model 
##Step 1: Is mean Math Facts performance significantly predictive of SF knowledge? 
Yes: Chisq(1) = 54.66, p < .0001
```{r}
#build model
sf.mf <- make_single_model(model.df, "mean.mf.c")

#test whether mean math facts significantly predicts Unit Task accuracy
anova(sf.base, sf.mf, test = 'LRT')
mtable('Math Facts' = sf.mf)
```

##Step 2: Does Math Facts explain unique variance on top of HCNN and FHC? 
Yes: Chisq(1) = 15.04, *p* = .0001, AIC = 2630.6
```{r}
#reminder that our base is sf.large.fhc
sf.large.plusmf <- test_predictor(sf.large.fhc, c("mean.mf.c", "fhc.c")) 

#run LRT to get actual values
anova(sf.large.fhc, sf.large.plusmf, test = 'LRT') #math facts significantly adds to the model

library(sjPlot)
tab_model(sf.large.plusmf, transform = NULL)
```

***

#Visualization of model fits
```{r}
item.pal <- c("#173BAB", "#5CA7D8", "#DE8141")

##Math Facts
all.data %>%
  distinct(SID, mean.sf, mean.mf)%>%
  ggplot(aes(x = mean.mf, y = mean.sf, colour = "#DE8141")) + 
  geom_count(stat = "sum", show.legend = FALSE) + 
  geom_smooth(aes(fill = "#DE8141"), alpha = .3,  
              show.legend = FALSE) + 
  scale_color_manual(values = "#DE8141") + 
  scale_fill_manual(values = "#DE8141") +
  labs(x = "Mean Math Facts performance", 
       y = "Mean Unit Task performance") + 
  theme_bw(base_size = 8) + 
  theme(panel.grid = element_blank())

ggsave("Figures/unit_by_mf.png", height = 3, width = 4)

##Highest Contiguous Next Number
all.data %>%
  mutate(highest_contig = factor(highest_contig, levels = c("0", "1", 
                                                            "7", "24", "26", "30", "62", 
                                                            "71", "83", "95")))%>%
  mutate(highest_contig.num = as.numeric(highest_contig))%>%
  distinct(SID, mean.sf, highest_contig, highest_contig.num)%>%
  ggplot(aes(x = highest_contig.num, y = mean.sf, color = "#3AAADD")) + 
  geom_count(stat = "sum", 
             show.legend = FALSE) + 
  geom_smooth(aes(fill = "#3AAADD"), alpha = .3, 
              show.legend = FALSE) + 
  scale_color_manual(values = "#3AAADD") + 
  scale_fill_manual(values = "#3AAADD") + 
  theme_bw(base_size = 8) +
  theme(panel.grid = element_blank()) +
  labs(x = 'Highest Contiguous Next Number', 
       y = "Mean Unit Task performance") + 
    scale_x_continuous(breaks = (1:length(as.numeric(levels(factor(all.data$highest_contig, levels = c("0", "1", 
                                                            "7", "24", "26", "30", "62", 
                                                            "71", "83", "95")))))), 
                     labels = as.character(levels(factor(all.data$highest_contig, levels = c("0", "1", 
                                                            "7", "24", "26", "30", "62", 
                                                            "71", "83", "95")))))
ggsave("Figures/hcnn_unit.png", width = 4, height = 3)
```

***

#Secondary analyses
##Testing whether SF is generalized from Math Facts
If children are acquiring SF from MF, we should expect that these children should also be at ceiling in Math Facts. Here, I am testing whether there is a difference in accuracy across the Unit, NN, and Math Facts tasks

```{r}
#add quartiles for unit task
all.data %<>%
  mutate(sf.quartile = cut(mean.sf, 
                                breaks=quantile(mean.sf, na.rm=TRUE), 
                                include.lowest=TRUE))

##ANALYSIS
#Looking only at children who are at ceiling in the Unit Task, are they also at ceiling in the MF task?
all.tasks.model <- all.data %>%
  filter(Task == "SF" | 
           Task == "WCN" |
           Task == "MF")%>%
  droplevels()%>%
   mutate(highest_contig = as.integer(highest_contig), 
         SID = factor(SID), 
         age.c = as.vector(scale(Age, center = TRUE, scale=TRUE)), #scale and center continuous variables for model fit
         fhc.c = as.vector(scale(FHC, center = TRUE, scale = TRUE)), 
         ihc.c = as.vector(scale(IHC, center = TRUE, scale = TRUE)),
         highest_contig.c = as.vector(scale(highest_contig, center = TRUE, scale=TRUE)), 
         highest_num.c = as.vector(scale(highest_num, center = TRUE, scale=TRUE)),
         mean.mf.c = as.vector(scale(mean.mf, center = TRUE, scale=TRUE)),
         mean.indef.c = as.vector(scale(mean.indef, center = TRUE, scale=TRUE)),
         starting_num.c = as.vector(scale(as.numeric(as.character(Task_item)), center = TRUE, scale = TRUE)))%>%
  mutate(Task = factor(Task, levels = c("MF", "SF", "WCN")))
```

Test whether there are significant differences in accuracy by task
Yes: Chisq(2) = 178.15, *p* < .0001, AIC = 496.91
```{r}
#make base model
mf.sf.ceiling.base <- glmer(Correct ~ count_range + age.c + (1|SID), 
                            family = "binomial", data = subset(all.tasks.model, sf.quartile == "(0.875,1]"))
#now add task
mf.sf.ceiling.task <- glmer(Correct ~ Task + count_range + age.c + (1|SID), 
                            family = "binomial", data = subset(all.tasks.model, sf.quartile == "(0.875,1]"))

#test to see if there is a difference by task 
anova(mf.sf.ceiling.base, mf.sf.ceiling.task, test = 'LRT') #yes, there is a significant effect of task
summary(mf.sf.ceiling.task)
```

Planned contrasts: Does MF differ from either SF or NN? Is there a difference between SF and NN?
Yes: MF is significantly less accurate in comparison to SF or NN, but there is no difference between SF and NN
```{r}
##make appropriate contrasts
task.contrasts <- rbind(c(1, -0.5, -0.5),     # MF vs. (SF + NN) / 2
             c(0, 1, -1))                # SF vs. NN
cMat <- MASS::ginv(task.contrasts)

planned.contrasts <- glmer(Correct ~ Task + count_range + age.c + (1|SID), data = subset(all.tasks.model,
                                                                           sf.quartile == "(0.875,1]"), 
             family = "binomial",
            contrasts = list(Task = cMat))
summary(planned.contrasts)
```

Follow up on the ceiling unit task analysis with an analysis which has all children
```{r}
#make base model
mf.sf.all.base <- glmer(Correct ~ count_range + age.c + (1|SID), 
                            family = "binomial", data = all.tasks.model)
#now add task
mf.sf.all.task <- glmer(Correct ~ Task + count_range + age.c + (1|SID), 
                            family = "binomial", data = all.tasks.model)
#test to see if there is a difference by task 
anova(mf.sf.all.base, mf.sf.all.task, test = 'LRT') #yes, there is a significant effect of task


#Planned contrasts: Does MF differ from either SF or NN? Is there a difference between SF and NN?
##testing contrasts
task.contrasts <- rbind(c(1, -0.5, -0.5),     # MF vs. (SF + NN) / 2
             c(0, 1, -1))                # SF vs. NN
cMat <- MASS::ginv(task.contrasts)

planned.contrasts <- glmer(Correct ~ Task + count_range + age.c + (1|SID), data = all.tasks.model, 
             family = "binomial",
            contrasts = list(Task = cMat))

summary(planned.contrasts)
```

## Bar graph showing difference between MF and NN as function of SF
```{r}
three.pal <- c("#173BAB", "#5CA7D8", "#DE8141")

all.data %>%
  filter(Task == "SF" |
           Task == "WCN" |
           Task == "MF")%>%
  mutate(Task = factor(Task, levels = c("SF", "WCN", "MF"), labels = c("Unit Task", "Next Number", "Math Facts")), 
        sf.quartile = factor(sf.quartile, labels = c("25% — 44%", "44% — 63%", "63% — 88%", "88% — 100%")), 
        Correct= as.numeric(as.character(Correct)))%>%
  group_by(SID, Task, sf.quartile)%>%
  multi_boot_standard("Correct", na.rm = TRUE) %>%
  ggplot(aes(x = Task, y = mean, fill=Task)) +
  stat_summary(fun.y = mean, position = position_dodge(width = .95),
                      geom="bar", alpha = .5, colour = "black") +
  # geom_violin(alpha = .5) +
  geom_point(aes(x = Task, y = mean, colour = Task),
               position=position_jitter(width = .18, height = .035),
               size=1.5,
               show.legend=FALSE, 
             inherit.aes = FALSE) +
  stat_summary(fun.data = "mean_cl_boot", geom="linerange", 
               position = position_dodge(width=0.90), width = 0.2, size = 1)+
  ylab("Mean task performance") + 
  xlab('Unit Task performance quartiles') + 
  theme_bw(base_size = 13) + 
  theme(legend.position = "bottom") +
  theme(panel.grid.minor = element_blank(), 
        panel.grid.major = element_blank(),
        axis.text.x = element_blank(), 
        axis.ticks.x = element_blank(), 
        legend.position = "top") +
  ylim(0, 1.0) +
  scale_fill_manual(values = three.pal) +
  scale_colour_manual(values = three.pal, guide = "none") + 
  facet_wrap(~sf.quartile, strip.position = "bottom", ncol = 4)

ggsave("~/Documents/Projects/sf_math/Analysis/Figures/all_tasks.png", width = 7, height = 4)
```

---

##Looking at the dissociation between Math Facts and the Unit Task
Interested in a set-size effect for Math Facts
Is there an effect of set size on MF and SF?
Then see what percentage of kids who succeed at small MF fail at larger MF as a function of their success in SF

This isn't perfect,  but I'm doing a match on magnitude, because we tested novel & same items in every task. This allows us to do a number*task kind of analysis. Note that this is only including SF and MF, because we're interested in the difference between getting the conceptual structure and getting the rote-memorized math facts.
```{r}
#average magnitudes of all three items tested within a given range
sub.ten <- (5 + 6)/2
teens <- 15 #exclude because this wasn't tested in both sf and mf
twenties <- 20
thirties <- 32
forties <- 46 #exclude because this wasn't tested in both tasks
fifties <- 57
sixties <- (62+64)/2
seventies <- 73 #exclude because this wasn't tested in both tasks 
eighties <- (84+86)/2
nineties <- 93

#first, add a column indicating whether numbers were the same or different across tasks
sf.mf.comparison <- all.tasks.model %>%
  mutate(Task_item = as.numeric(as.character(Task_item)))%>%
  mutate(trial.type = ifelse((Task_item == 20 | Task_item == 32 | 
                               Task_item == 57 | Task_item == 93), "SF_MF_same",
                             ifelse((Task_item == 7 | Task_item == 62 | 
                                      Task_item == 95), "SF_NN_same", "Novel")), 
         magnitude.match = ifelse(Task_item < 10, sub.ten, 
                                  ifelse((Task_item >=10 & Task_item <20), teens, 
                                         ifelse((Task_item >= 20 & Task_item < 30), twenties, 
                                                ifelse((Task_item >= 30 & Task_item < 40), thirties, 
                                                       ifelse((Task_item >= 40 & Task_item < 50), forties, 
                                                              ifelse((Task_item >= 50 & Task_item < 60), fifties, 
                                                                     ifelse((Task_item >= 60 & Task_item < 70), sixties, 
                                                                            ifelse((Task_item >= 70 & Task_item < 80), seventies, 
                                                                                   ifelse((Task_item >=80 & Task_item < 90), eighties, nineties))))))))), 
         magnitude.match = round(magnitude.match, 2),
         magnitude.match.c = as.vector(scale(magnitude.match, center = TRUE, scale=TRUE)))%>%
  filter(magnitude.match != teens, 
         magnitude.match != forties, 
         magnitude.match != seventies, 
         Task == "SF" | Task == "MF")%>%
  droplevels()

##filter out 21 for MF
sf.mf.comparison.matched <- sf.mf.comparison %>%
  filter(Task_item != 21, 
         Task_item != 7, 
         Task_item != 51, 
         Task_item != 95, 
         Task_item != 15, 
         Task_item != 34, 
         Task_item != 60, 
         Task_item != 73, 
         Task_item != 81)
```

###Get a visualization of this for children overall
```{r}
two.pal <- c("#173BAB", "#DE8141")

sf.mf.comparison.matched %>%
  mutate(Task = factor(Task, levels = c("SF", "MF"), labels = c("Unit Task", "Math Facts")), 
         magnitude.match = factor(magnitude.match, levels = c("5.5", "20", 
                                                              "32", "57", "63", "85", "93")))%>%
  group_by(Task, magnitude.match)%>%
  multi_boot_standard("Correct", na.rm = TRUE) %>%
  ggplot(aes(x = magnitude.match, y = mean, colour = Task, group = Task)) +
  geom_point(size = 3) + 
  geom_line() +
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +
  theme_bw(base_size = 13) + 
  # facet_grid(~trial.type, scale = "free_x") +
  scale_colour_manual(values = two.pal) +
  theme(legend.position = "right", 
        panel.grid = element_blank()) +
  labs(x = "Magnitude of number queried", y = "Mean performance") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

ggsave("~/Documents/Projects/sf_math/Analysis/Figures/magnitude_overall_comp.png")
```

###Get a visualization of this for children by SF quartile
```{r}
sf.mf.comparison.matched %>%
  mutate(Task = factor(Task, levels = c("SF", "MF"), labels = c("Unit", "Math Facts")),
         magnitude.match = factor(magnitude.match, levels = c("5.5", "20", 
                                                              "32", "57", "63", "85", "93")),
         sf.quartile = factor(sf.quartile, labels = c("25% — 44%", "44% — 63%", "63% — 88%", "88% — 100%")))%>%
  group_by(sf.quartile, Task, magnitude.match)%>%
  multi_boot_standard("Correct", na.rm = TRUE) %>%
  ggplot(aes(x = magnitude.match, y = mean, colour = Task, group = Task)) +
  geom_point(size = 2) + 
  geom_line() +
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +
  theme_bw(base_size = 10) + 
  facet_grid(~sf.quartile) +
  scale_colour_manual(values = two.pal) +
  theme(legend.position = "bottom", 
        legend.title = element_blank()) +
  labs(x = "Magnitude match - number queried", y = "Mean performance") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

```

##Step 2: Is there an interaction between item magnitude and task for SF and MF?
Yes, there is an interaction; Chisq = 4.89, p = .03 
```{r}
sf.mf.comparison.matched %<>%
  mutate(Task = factor(Task, levels = c("SF", "MF")))

#magnitude match
comp.sf.mf.mag <- glmer(Correct ~ magnitude.match.c+Task + count_range + age.c + (1|SID), 
                data =  sf.mf.comparison.matched, 
                family = "binomial", 
                control=glmerControl(optimizer="bobyqa",
                         optCtrl=list(maxfun=2e4)))

#add interaction with magnitude match
comp.sf.mf.int <- glmer(Correct ~ magnitude.match.c*Task + count_range + age.c + (1|SID), 
                data =  sf.mf.comparison.matched, 
                family = "binomial", 
                control=glmerControl(optimizer="bobyqa",
                         optCtrl=list(maxfun=2e4)))

#compare
anova(comp.sf.mf.mag,comp.sf.mf.int, test = 'lrt')
summary(comp.sf.mf.int)
```

##Step 3: What percentage of children succeed on an item in Unit, and then fail on the same (or similar item) in MF? 
I think it might make sense to look at this through quartiles
```{r}
#Classify children as "addition understanders" or "addition non-understanders"
understanders <- sf.mf.comparison %>%
  filter(Task == "MF", 
         Task_item == 5)%>%
  group_by(SID)%>%
  mutate(equation.understander = ifelse(Correct == 1, "understander", "non-understander"))%>%
  dplyr::select(SID, equation.understander)

understander.df <- left_join(sf.mf.comparison, understanders, by = "SID")

##Descriptives for MF understanders and non-understanders
understander.df %>%
  group_by(equation.understander, Task)%>%
  summarise(mean = mean(Correct, na.rm = TRUE), 
            sd = sd(Correct, na.rm = TRUE))

#graph understanders and non-understanders performance for MF 
understander.df %>%
  filter(Task == "MF", 
         !is.na(equation.understander))%>%
  mutate(Task_item = factor(Task_item, levels=c(5, 20, 21, 32, 57, 64, 86, 93)))%>%
  group_by(equation.understander, Task_item)%>%
  multi_boot_standard("Correct", na.rm = TRUE)%>%
  ggplot(aes(x = Task_item, y = mean, color = equation.understander, group = equation.understander)) + 
  geom_point(size = 3) + 
  geom_line() + 
  geom_linerange(aes(ymin = ci_lower, ymax =  ci_upper)) + 
  theme_bw(base_size = 13) + 
  theme(panel.grid = element_blank()) + 
  scale_color_brewer(palette = "Set2")
```

GLMM: Testing whether, for addition knowers, they are more accurate on math facts for items on which they succeeded in Unit Task
```{r}
##analysis: testing whether, for addition knowers, they are more accurate on math facts for items that they succeeded on in Unit Task 
tmp <- understander.df %>%
  filter(equation.understander == "understander")%>%
  filter(Task_item == 20 | 
           Task_item == 32 | 
           Task_item == 57 | 
           Task_item == 93)%>%
  dplyr::select(SID, Task, Task_item, Correct, equation.understander)%>%
  filter(Task == "SF")%>%
  group_by(SID, Task_item)%>%
  mutate(sf.correct = ifelse((Correct == 1), "Unit Task Correct", "Unit Task Incorrect"))%>%
  dplyr::select(SID, Task_item, sf.correct)

tmp1 <- left_join(understander.df, tmp, by = c("SID", "Task_item"))%>%
  filter(!is.na(sf.correct), 
         Task == "MF")

model.test.base <- glmer(Correct ~ count_range + age.c + (1|SID), data = tmp1, family = "binomial")
model.test <- glmer(Correct ~ sf.correct + count_range + age.c + (1|SID), data = tmp1, family = "binomial")
anova(model.test.base, model.test, test= 'lrt')
summary(model.test)
```

What percentage of addition understanders go on to succeed or fail on later MF problems by whether they got that number correct or incorrect? 
```{r}
tmp1 %>%
  mutate(Task_item = factor(Task_item)) %>%
  mutate(mf.correct = factor(Correct, levels = c(0,1), 
                             labels = c("Math Facts Incorrect", "Math Facts Correct")), 
         sf.correct = factor(sf.correct, levels = c("Unit Task Incorrect", "Unit Task Correct")))%>%
  group_by(Task_item, sf.correct, mf.correct)%>%
  summarise(n = n()) %>%
  group_by(Task_item)%>%
  mutate(total.n = sum(n), 
         prop = n/total.n)%>%
  ggplot(aes(x = Task_item, y = prop, fill = mf.correct)) +
  geom_bar(stat = "identity", color = "black") + 
  facet_grid(mf.correct~sf.correct) + 
  labs(y = 'Proportion of children correct/incorrect on Math Facts', x = 'Item') + 
  scale_fill_brewer(palette = "Set1") + 
  theme_bw(base_size = 13) + 
  theme(panel.grid = element_blank(), 
        legend.position = "none")  
  
ggsave("~/Documents/Projects/sf_math/Analysis/Figures/addition_knowers_MathFacts.png", height = 5)

##Finally, mean performance by addition understander
tmp1 %>%
  filter(!is.na(equation.understander))%>%
  group_by(equation.understander, sf.correct)%>%
  summarise(mean = mean(Correct, na.rm = TRUE), 
            sd = sd(Correct, na.rm = TRUE))
```

---

#By-task descriptives & visualizations
##Highest Count

Descriptives
```{r}
#rename productivity
all.data %<>%
  mutate(Productive = factor(Productive, levels = c("Productive", "Nonproductive"), 
                             labels = c("Resilient", "Non-Resilient")))

productivity.pal <- c("#00b8e6", "#666666")

all.data %>%
  distinct(SID, Age, Productive, IHC, FHC)%>%
  group_by(Productive)%>%
  summarise(n = n(),
            mean_IHC = round(mean(IHC)), 
            sd_IHC = round(sd(IHC), 2), 
            median_IHC = round(median(IHC)), 
            mean_FHC = round(mean(FHC)), 
            sd_FHC = round(sd(FHC), 2), 
            median_FHC = round(median(FHC)))%>%
  kable()

#overall IHC and FHC
all.data %>%
  distinct(SID, Age, Productive, IHC, FHC)%>%
  summarise(n = n(),
            mean_IHC = round(mean(IHC)), 
            sd_IHC = round(sd(IHC), 2), 
            median_IHC = round(median(IHC)), 
            mean_FHC = round(mean(FHC)), 
            sd_FHC = round(sd(FHC), 2), 
            median_FHC = round(median(FHC)))%>%
  kable()
```

##Scatterplot/density of IHC/FHC
```{r}
initial_final <- all.data %>%
  filter(!is.na(Productive))%>%
  distinct(SID, IHC, FHC, Productive)%>%
  mutate(IHC = as.numeric(IHC), 
         FHC = as.numeric(FHC))

library(ggstance)
library(ggjoy)
library(cowplot)
####Cantonese####
pmain <- ggplot(initial_final, aes(x = IHC, y = FHC, color = Productive, shape = Productive)) + 
  geom_point(size = 3, alpha = .8, position = position_jitter()) + 
  # geom_jitter() + 
  scale_color_manual(values = productivity.pal) + 
  coord_fixed() + 
  theme_bw(base_size = 15) +
  scale_x_continuous(breaks = seq(0, 120, 10)) + 
  scale_y_continuous(breaks = seq(0, 120, 10)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        axis.title = element_text(size = 12),
        legend.position = c(.75, 0.2), 
        legend.text = element_text(size = 10), 
        legend.title = element_blank(), 
        axis.text = element_text(size = 11)) + 
  labs(x = "Initial Highest Count", y = "Final Highest Count") 

xdens <- axis_canvas(pmain, axis = "x") + 
  # geom_ridgeline(data = us_initial, aes(x = IHC, y = 0, height=..density.., 
  #                                       fill = Productive), 
  #                stat = 'xdensity', alpha = .5, size = .2, trim = FALSE) + 
  geom_density(data = initial_final, aes(x = IHC, y = ..density.., fill = Productive), 
               alpha=.4, adjust = .5) + 
  scale_fill_manual(values = productivity.pal) 

ydens <- axis_canvas(pmain, axis = "y", coord_flip = TRUE) + 
  # geom_vridgeline(data = us_initial, aes(y = FHC, x = 0, width=..density.., 
  #                                       fill = Productive), 
  #                stat = 'ydensity', alpha = .5, size = .2, trim = FALSE) + 
  geom_density(data = initial_final, aes(x = FHC, y = ..density.., fill = Productive), 
               alpha=.4, adjust = .5) +
  coord_flip() +
  scale_fill_manual(values = productivity.pal) 

p5 <- insert_xaxis_grob(pmain, xdens, grid::unit(.2, "null"), position = "top")
count_dens <- insert_yaxis_grob(p5, ydens, grid::unit(.2, "null"), position = "right")
ggdraw(count_dens)
ggsave("~/Documents/Projects/sf_math/Analysis/density.png")
# ggdraw(p6)
# png(filename = "hk_density.png")
# ggdraw(p6)
# dev.off()
```

###How many children who stopped before 120 were able to count at least a little beyond their IHC?
```{r}
#read in hc data
hc.df <- read.csv("~/Documents/Projects/sf_math/Data/sf_math_hc.csv")%>%
  dplyr::rename("IHC" = "IHC_final", 
                "FHC" = "FHC_final")%>%
  dplyr::select(SID, Last_successful, Error, Prompt, After_prompt, IHC, FHC, Special_count, Exclude_trial, 
                Exclude_trial_reason)%>%
  filter(Exclude_trial != 1)%>% 
  droplevels()

#merge with productivity classifications
productive <- all.data %>%
  distinct(SID, Productive)

hc.full <- left_join(hc.df, productive, by = "SID")%>%
  filter(!is.na(Productive))%>%#manual exclusion from data - these SIDS excuded from data, but did HC 
  mutate(IHC = as.numeric(as.character(IHC)), 
         FHC = as.numeric(as.character(FHC)))%>%
  mutate(IHC = ifelse(IHC > 120, 120, as.numeric(IHC)), 
         FHC = ifelse(FHC > 120, 120, as.numeric(FHC)))

#make error frequency df 
error.freq <- hc.full %>%
  filter(!is.na(Last_successful))%>%
  mutate(Error_type = ifelse(Last_successful %% 10 == 9 , "Decade end", 
                             ifelse(Last_successful %% 10 == 0, "Decade beginning", "Mid-decade")), 
         IHC = as.numeric(as.character(IHC)), 
         FHC = as.numeric(as.character(FHC)))

#how many kids counted spontaneously to 120?
all.data %>%
  distinct(SID, IHC)%>%
  filter(IHC == 120)%>%
  summarise(n=n())

#of the kids who stopped before 120, how many of them counting a little bit beyond?
#these ns are not right
hc.full %>%
  distinct(SID, IHC, FHC)%>%
  filter(IHC != 120)%>%
  mutate(delta.count = FHC-IHC, 
         counted.beyond = ifelse(delta.count > 0, "counted beyond IHC", "could not count beyond IHC"))%>%
  group_by(counted.beyond)%>%
  summarise(n = n(), 
            mean.delta = mean(delta.count, na.rm = TRUE), 
            sd.delta = sd(delta.count, na.rm = TRUE))
```


Prompts by productivity
```{r}
#how many prompts do productive and nonproductive counters need? and what kind of errors do they make?
#mean number of prompts by Productivity
error.freq %>%
  mutate(IHC = as.numeric(as.character(IHC)), 
         FHC = as.numeric(as.character(FHC)))%>%
  filter(IHC != 120)%>%
  group_by(Productive, Error_type)%>%
  summarise(n = n())%>%
  group_by(Productive)%>%
  mutate(total.n = sum(n), 
            prop = n/total.n)%>%
  kable()

```

##Where are errors happening?
```{r}
#create a df with individual error decades
error.freq.decade <- error.freq %>%
  mutate(error.decade = ifelse(Last_successful < 10, 0, 
                               ifelse(Last_successful >= 10 & Last_successful < 20, 10, 
                                      ifelse(Last_successful >= 20 & Last_successful < 30, 20, 
                                             ifelse(Last_successful >= 30 & Last_successful < 40, 30, 
                                                    ifelse(Last_successful >=40 & Last_successful < 50, 40, 
                                                           ifelse(Last_successful >= 50 & Last_successful < 60, 50, 
                                                                  ifelse(Last_successful >= 60 & Last_successful < 70, 60, 
                                                                         ifelse(Last_successful >= 70 & Last_successful < 80, 70, 
                                                                                ifelse(Last_successful >= 80 & Last_successful < 90, 80, 
                                                                                       ifelse(Last_successful >= 90 & Last_successful < 100, 90, 
                                                                                              ifelse(Last_successful >= 100 & Last_successful < 110, 100, 110)))))))))))) %>%
  mutate(error.base = (error.decade - Last_successful) * -1, 
         error.base = ifelse(Last_successful == 120, 0, as.numeric(error.base)))
```


###Error by decade
```{r}
error.freq.decade %>%
  filter(IHC != 120)%>%
  mutate(error.decade = factor(error.decade, levels = c(0, 10, 20, 30, 40, 50, 60, 70, 80, 
                                                        90, 100, 110)),
         error.base = factor(error.base))%>%
  group_by(Productive, error.decade)%>%
  summarise(n = n())%>% 
  group_by(Productive)%>%
  mutate(total.n = sum(n), 
         prop = n/total.n)%>%
  mutate(n = ifelse(is.na(n), 0, as.numeric(n)))%>%
  ggplot(aes(x = error.decade, y = Productive)) +
  geom_tile(aes(fill = round(prop, 2))) +
  geom_text(aes(label = as.character(round(prop, 2))), 
            size = 2.5) +
  coord_equal() +
  scale_fill_gradient2(low = "white", high = "red", "Proportion") + 
  theme_bw(base_size = 10) + 
  labs(x = "Decade of error") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        legend.position = "bottom", 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank())
```

###Error by unit
```{r}
error.freq.decade %>%
  filter(Last_successful <= 120)%>% #filter out trials where kid kept going beyond 140
  mutate(error.base = factor(error.base))%>%
  group_by(Productive, error.base)%>%
  summarise(n = n())%>% 
  group_by(Productive)%>%
  mutate(total.n = sum(n), 
         prop = n/total.n)%>%
  mutate(n = ifelse(is.na(n), 0, as.numeric(n)))%>%
  ggplot(aes(x = error.base, y = Productive)) +
  geom_tile(aes(fill = round(prop, 2))) +
  geom_text(aes(label = as.character(round(prop, 2))), 
            size = 2.5) +
  coord_equal() +
  scale_fill_gradient2(low = "white", high = "red", "Proportion") + 
  theme_bw(base_size = 10) + 
  labs(x = "Unit of error") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        legend.position = "bottom", 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank())
```



##How many errors?
```{r}
error.freq %>%
  filter(Last_successful != 120)%>%
  group_by(SID)%>%
  summarise(n = n())%>%
  group_by()%>%
  summarise(mean_prompts = round(mean(n, na.rm = TRUE), 2),
            sd_prompts = round(sd(n, na.rm = TRUE), 2), 
            median_prompts = round(median(n, na.rm = TRUE), 2),
            max_prompt = max(n, na.rm = TRUE), 
            min_prompt = min(n, na.rm = TRUE))%>%
  kable()
```

---

#Figure: NN accuracy by HNN and HCNN
```{r}
#get mean performance for highest NN
ms.hnn <- all.data %>%
  filter(Task == "WCN")%>%
  mutate(Number = factor(highest_num, levels = c("1", "7", 
                                                            "24", "26", "30", 
                                                            "62", "71", "83", "95")))%>%
  group_by(Number)%>%
  multi_boot_standard("Correct", na.rm = TRUE)%>%
  mutate(Type = "Highest Next Number")

#get mean performance for HCNN
ms.hcnn <- all.data %>%
  filter(Task == "WCN")%>%
  mutate(Number = factor(highest_contig, levels = c("0", "1", "7", 
                                                            "24", "26", "30", 
                                                            "62", "71", "83", "95")))%>%
  group_by(Number)%>%
  multi_boot_standard("Correct", na.rm = TRUE)%>%
  mutate(Type = "Highest Contiguous Next Number")

#bind_rows
ms.highest.all <- bind_rows(ms.hnn, ms.hcnn)%>%
  ungroup()%>%
  mutate(Number = factor(Number, levels = c("0", "1", "7", 
                                                            "24", "26", "30", 
                                                            "62", "71", "83", "95")))

#combine, plot
ms.highest.all %>%
  ggplot(aes(x = Number, y = mean, color = Type, group = Type)) + 
  geom_point(size = 3) + 
  geom_line(linetype = "dashed") +
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) + 
  theme_bw(base_size = 13) + 
  theme(panel.grid = element_blank(), 
        legend.title = element_blank(), 
        legend.position = "top") + 
  labs(x = "Number reached", y = "Mean Next Number performance") +
  scale_color_brewer(palette = "Paired")

 ggsave("~/Documents/Projects/sf_math/Analysis/nn_by_hcnn.png", width = 5.5, height = 4.5)

```

---

#Performance by Resilience: Unit, NN, Math Facts
##Descriptives
```{r}
all.data %>%
  filter(Task == "SF" | 
           Task == "WCN" | 
           Task == "MF")%>%
  group_by(Task, Productive)%>%
  summarise(mean = mean(Correct, na.rm = TRUE), 
            sd = sd(Correct, na.rm = TRUE))%>%
  kable()
```

##Figure: Mean performance by Resilience
```{r}
all.data %>%
  filter(Task == "SF" |
           Task == "WCN" |
           Task == "MF")%>%
  mutate(Task = factor(Task, levels = c("SF", "WCN", "MF"), labels = c("Unit Task", "Next Number", "Math Facts")))%>%
  group_by(SID, Productive, Task)%>%
  summarise(mean = mean(as.numeric(as.character(Correct), na.rm=TRUE)),
            sd = sd(as.numeric(as.character(Correct), na.rm=TRUE))) %>%
  ggplot(aes(x = Productive, y = mean, fill=Productive)) +
  stat_summary(fun.y = mean, position = position_dodge(width = .95),
                      geom="bar", alpha = .5, colour = "black") +
  # geom_violin(alpha = .5) +
  geom_point(aes(x = Productive, y = mean, colour = Productive),
               position=position_jitter(width = .18, height = .035),
               size=1.5,
               show.legend=FALSE, 
             inherit.aes = FALSE) +
  stat_summary(fun.data = mean_se, geom="errorbar", 
               position = position_dodge(width=0.90), width = 0.2, size = 1)+
  ylab("Mean task performance") + 
  xlab('') + 
  theme_bw(base_size = 13) + 
  theme(legend.position = "bottom") +
  theme(panel.grid.minor = element_blank(), 
        panel.grid.major = element_blank(),
        axis.text.x = element_blank(), 
        axis.ticks.x = element_blank(), 
        legend.position = "right", 
        legend.title = element_blank()) +
  ylim(0, 1.0) +
  scale_fill_manual(values = productivity.pal) +
  scale_colour_manual(values = productivity.pal, guide = "none") + 
  facet_grid(~Task)
# 
ggsave("~/Documents/Projects/sf_math/Analysis/mean_resilience.png", width = 7, height = 3.5)
```

##Analysis: Do Resilient Counters have better performance on tasks?
```{r}
#make model with mean performance
ms.task <- all.data %>%
  distinct(SID, Productive, Age, mean.unit, mean.mf, mean.nn)%>%
  gather(key = "Task", value = "mean", -SID, - Productive, -Age)%>%
  mutate(Task = ifelse(Task == "mean.unit", "SF", 
                       ifelse(Task == "mean.nn", "WCN", "MF")))%>%
  mutate(Task = factor(Task, levels = c("SF", "WCN", "MF")))%>%
  mutate(Productive = factor(Productive, levels = c("Non-Resilient", "Resilient")))

#mean performance
resil.lm <- lm(mean ~ Productive + Task + Age, 
               data = ms.task)

#summary
summary(resil.lm)

#re-order
summary(lm(mean ~ Productive + relevel(Task, ref = "WCN") + Age, 
               data = ms.task))
```

---

#Performance within/outside count range
Do Resilient counters do better for numbers outside their Initial Highest Count?
```{r}
#need to get only kids that have numbers which fall outside their count range
#filters out n = 28 kids
model.mf.sf.all.df %<>%
  mutate(Prod.tertiary = ifelse(IHC >= 95, "Resilient IHC >= 95", 
                                ifelse((IHC < 95 & Productive == "Resilient"), "Resilient IHC < 95", "Non-Resilient")))

model.mf.sf.all.df %>%
  group_by(SID, Prod.tertiary, count_range, Task)%>%
  summarise(n = n(), 
            mean = mean(Correct, na.rm = TRUE), 
            sd = sd(Correct, na.rm = TRUE), 
            se = sd/(sqrt(n)))%>%
  ggplot(aes(x = count_range, y = mean, fill = Prod.tertiary)) +
  stat_summary(fun.y = mean, position = position_dodge(width = .95),
                      geom="bar", alpha = .5, colour = "black") +
  # geom_violin(alpha = .5) +
  # geom_point(aes(x = count_range, y = mean, colour = Prod.tertiary),
  #              position=position_jitter(width = .18, height = .035),
  #              size=1.5,
  #              show.legend=FALSE, 
  #            inherit.aes = FALSE) +
  stat_summary(fun.data = mean_se, geom="errorbar", 
               position = position_dodge(width=0.90), width = 0.2, size = 1)+
  ylab("Mean task performance") + 
  xlab('') + 
  theme_bw(base_size = 13) + 
  theme(legend.position = "bottom") +
  theme(panel.grid.minor = element_blank(), 
        panel.grid.major = element_blank(),
        legend.position = "right", 
        legend.title = element_blank()) +
  ylim(0, 1.0) + 
  facet_grid(~Task)

#look at only participants who have items within/outside their IHC
range.base <- glmer(Correct ~ count_range*Productive + Task + age.c + (1|SID),
                    family = "binomial", data = subset(model.mf.sf.all.df, IHC < 95))

summary(range.base)#no significant interaction between count range and Productivity
```

---

#Exploratory: Indefinite Next Number 
Demographics 
```{r}
all.data %>%
  filter(!is.na(mean.indef))%>%
  distinct(SID, Age)%>%
  summarise(n = n(),
            mean = mean(Age, na.rm = TRUE), 
            sd = sd(Age, na.rm = TRUE))
```

Descriptives by item
```{r}
all.data %>%
  filter(Task == "Indefinite", 
         !is.na(Correct))%>%
  group_by(Task_item)%>%
  summarise(mean = mean(Correct, na.rm = TRUE), 
            sd = sd(Correct, na.rm = TRUE))%>%
  kable()
```

Note: This does not include all participants
```{r}
#filter out anyone who has NA for mean indef
indef.model <- model.df %>%
  filter(!is.na(mean.indef.c))

#does mean indefinite next number performance predict Unit Task
#is indefinite on its own predictive
indef.base.single <- glmer(Correct ~ count_range + age.c + (1|SID), 
                    family = "binomial", data = indef.model)
indef.indef.single <- glmer(Correct ~ mean.indef.c + count_range + age.c + (1|SID), 
                    family = "binomial", data = indef.model)

#compare
anova(indef.base.single, indef.indef.single, test = 'LRT')
summary(indef.indef.single)

##add to final model with prod and math
#model without indefinite
indef.base <- glmer(Correct ~ mean.mf.c + fhc.c + highest_contig.c + count_range + age.c + (1|SID), 
                    family = "binomial", data = indef.model)

#model with indefinite
indef.indef <- glmer(Correct ~ mean.indef.c + mean.mf.c + fhc.c + highest_contig.c + count_range + age.c + (1|SID), 
                     family = "binomial", data = indef.model)

#compare
anova(indef.base, indef.indef, test = 'LRT') #mean indefinite significantly adds

summary(indef.indef)

```

##Indefinite visualization by Resilience
```{r}
all.data %>%
  filter(!is.na(mean.indef))%>%
  filter(Task == "SF")%>%
  mutate(indef.bin = ifelse(mean.indef < .25, "0 correct", 
                            ifelse(mean.indef == .25, "1 correct", 
                                   ifelse(mean.indef == .5, "2 correct", 
                                          ifelse(mean.indef == .75, "3 correct", "4 correct")))))%>%
  group_by(SID, Task, indef.bin)%>%
  summarise(mean = mean(as.numeric(as.character(Correct), na.rm=TRUE)),
            sd = sd(as.numeric(as.character(Correct), na.rm=TRUE))) %>%
  ggplot(aes(x = indef.bin, y = mean, fill=Task)) +
  # geom_violin(aes(fill = Task), alpha = .4) + 
  stat_summary(fun.y = mean, position = position_dodge(width = .95),
                      geom="bar", alpha = .5, colour = "black") +
  geom_point(aes(x = indef.bin, y = mean, colour = Task),
               position=position_jitter(width=0.15,height=0.02),
               size=1.5,
               show.legend=TRUE, 
             inherit.aes = FALSE) +
  stat_summary(fun.data = mean_se, geom="errorbar", 
               position = position_dodge(width=0.90), width = 0.1, size = 1)+
  ylab("Mean Unit Task performance") + 
  xlab('Mean Indefinite Next Number performance') + 
  theme_bw(base_size = 15) + 
  theme(legend.position = "right") +
  theme(panel.grid.minor = element_blank(), 
        panel.grid.major = element_blank(), 
        legend.position = "none") +
  ylim(0, 1.0) +
  scale_fill_manual(values = "#091E5B") + 
  scale_color_manual(values = "#091E5B") + 
  coord_fixed(ratio = 1.8)

# ggsave("~/Documents/Projects/sf_math/Analysis/indef.png")
```

##What about indefinite NN along with other tasks? 
```{r}
all.data %>%
  filter(!is.na(mean.indef))%>%
  filter(Task == "SF" | 
           Task == "WCN" | 
           Task == "MF" | 
           Task == "Indefinite")%>%
  group_by(SID, Task, sf.quartile)%>%
  summarise(mean = mean(as.numeric(as.character(Correct), na.rm=TRUE)),
            sd = sd(as.numeric(as.character(Correct), na.rm=TRUE))) %>%
  ggplot(aes(x = sf.quartile, y = mean, fill=Task)) +
  # geom_violin(aes(fill = Task), alpha = .4) + 
  stat_summary(fun.y = mean, position = position_dodge(width = .95),
                      geom="bar", alpha = .5, colour = "black") +
  geom_point(aes(x = sf.quartile, y = mean, colour = Task),
               position=position_jitter(width=0.15,height=0.02),
               size=1.5,
               show.legend=TRUE, 
             inherit.aes = FALSE) +
  stat_summary(fun.data = mean_se, geom="errorbar", 
               position = position_dodge(width=0.90), width = 0.1, size = 1)+
  ylab("Mean Task performance") + 
  xlab("Unit Task quartiles") + 
  facet_grid(~sf.quartile, scale = "free_x") +
  theme_bw(base_size = 15) + 
  theme(legend.position = "right") +
  theme(panel.grid.minor = element_blank(), 
        panel.grid.major = element_blank(), 
        legend.position = "none") +
  ylim(0, 1.0) 
  # scale_fill_manual(values = "#091E5B") + 
  # scale_color_manual(values = "#091E5B") 
  
```

***

##Mean Math Facts and Next Number by Unit Task performance
```{r}
all.data %>%
  distinct(SID, mean.sf, mean.mf)%>%
  ggplot(aes(x = mean.mf, y = mean.sf, colour = "#ed7d31")) + 
  geom_count(stat = "sum", show.legend = FALSE) + 
  geom_smooth(aes(fill = "#ed7d31"), alpha = .3, method = 'lm', 
              show.legend = FALSE) + 
  scale_color_manual(values = "#ed7d31") + 
  scale_fill_manual(values = "#ed7d31") +
  labs(x = "Mean Math Facts performance", 
       y = "Mean Unit Task performance") + 
  coord_fixed()

ggsave("~/Documents/Projects/sf_math/Analysis/MF_Unit.png")
  
```

#Mean unit by FHC
```{r}
all.data %>%
  distinct(SID, mean.unit, FHC)%>%
  ggplot(aes(x = FHC, y = mean.unit, color = "#173BAB")) + 
  geom_count(stat = "sum", show.legend = FALSE) + 
  geom_smooth(aes(fill = "#173BAB"), method = 'lm', alpha = .3, show.legend = FALSE) + 
  scale_color_manual(values = "#173BAB") + 
  scale_fill_manual(values = "#173BAB") + 
  labs(x = 'Final Highest Count', y = 'Mean Unit Task performance') +
  scale_x_continuous(breaks = seq(0, 120, 10)) + 
  coord_fixed(ratio = 120)

ggsave("~/Documents/Projects/sf_math/Analysis/FHC_unit.png")
```

#Mean Unit by HCNN
```{r}
all.data %>%
  mutate(highest_contig = factor(highest_contig, levels = c("0", "1", 
                                                            "7", "24", "26", "30", "62", 
                                                            "71", "83", "95")))%>%
  mutate(highest_contig.num = as.numeric(highest_contig))%>%
  distinct(SID, mean.unit, highest_contig, highest_contig.num)%>%
  ggplot(aes(x = highest_contig.num, y = mean.unit, color = "#3AAADD")) + 
  geom_count(stat = "sum", 
             show.legend = FALSE) + 
  geom_smooth(aes(fill = "#3AAADD"), alpha = .3, method = "lm", 
              show.legend = FALSE) + 
  scale_color_manual(values = "#3AAADD") + 
  scale_fill_manual(values = "#3AAADD") + 
  labs(x = 'Highest Contiguous Next Number', 
       y = "Mean Unit Task performance") + 
    scale_x_continuous(breaks = (1:length(as.numeric(levels(factor(all.data$highest_contig, levels = c("0", "1", 
                                                            "7", "24", "26", "30", "62", 
                                                            "71", "83", "95")))))), 
                     labels = as.character(levels(factor(all.data$highest_contig, levels = c("0", "1", 
                                                            "7", "24", "26", "30", "62", 
                                                            "71", "83", "95"))))) + 
  coord_fixed(ratio = 10)

ggsave("~/Documents/Projects/sf_math/Analysis/hcnn_unit.png")

# all.data %>%
#   filter(Task == "SF")%>%
#   group_by(SID, factor(highest_contig)))%>%
#   summarise(mean.sub = mean(Correct, na.rm = TRUE), 
#             n = n(), 
#             sd = sd(Correct, na.rm = TRUE), 
#             se = sd/sqrt(n))%>%
#   group_by(highest_contig)%>%
#   summarise(mean = mean(mean.sub), 
#             mean.se = mean(se)) %>%
#   ggplot(aes(x = highest_contig, y = ))
```

#Unit Task
##With individual prod points
```{r}
all.data %>%
  filter(Task == "SF")%>%
  group_by(SID, Productive)%>%
  summarise(mean = mean(as.numeric(as.character(Correct), na.rm=TRUE)),
            sd = sd(as.numeric(as.character(Correct), na.rm=TRUE))) %>%
  ggplot(aes(x = Productive, y = mean, fill=Productive)) +
  geom_violin(aes(fill = Productive, colour = Productive), alpha = .1, size = 1) +
  geom_point(aes(x = Productive, y = mean, colour = Productive),
               position=position_jitter(width=0.15,height=0.02),
               size=1.5,
             alpha = .9,
               show.legend=TRUE, 
             inherit.aes = FALSE) +
  stat_summary(fun.data = mean_se, geom="errorbar", 
               position = position_dodge(width=0.90), width = 0.1, size = 1.5)+
  ylab("Mean Unit Task performance") + 
  xlab('') + 
  theme_bw(base_size = 13) + 
  theme(legend.position = "bottom") +
  theme(panel.grid.minor = element_blank(), 
        panel.grid.major = element_blank(), 
        legend.title = element_blank()) +
  ylim(0, 1.0) +
  scale_fill_manual(values = productivity.pal, guide = "none") +
  scale_colour_manual(values = productivity.pal, guide ="none")
```

##By productivity and count range
```{r}
all.data %>%
  filter(Task == "SF")%>%
  group_by(SID, count_range, Productive)%>%
  summarise(mean = mean(as.numeric(as.character(Correct)), na.rm=TRUE),
            sd = sd(as.numeric(as.character(Correct)), na.rm=TRUE)) %>%
ggplot(aes(x=count_range, y=mean, fill = Productive)) +
  geom_violin(aes(colour = Productive, fill = Productive), alpha = .1, size = 1, 
              show.legend = FALSE)  + 
  geom_point(position=position_jitterdodge(jitter.width = .3, 
                                           jitter.height = .1, 
                                           dodge.width = .9),
             aes(colour = Productive, group=Productive), 
             size=1.5,
             alpha = .9,
               show.legend=TRUE) +
  stat_summary(fun.data = mean_se, geom="errorbar", 
               position = position_dodge(width=0.90), width = 0.2, 
               show.legend = FALSE, size = 1) +
  ylab("Mean Unit Task performance") + 
  xlab('') + 
  theme_bw(base_size = 13) + 
  theme(legend.position = "bottom") + 
  ylim(0, 1.0) +
  scale_fill_manual(values = productivity.pal) + 
  scale_colour_manual(values = productivity.pal) +
  theme(panel.grid.minor = element_blank(), 
        panel.grid.major = element_blank(), 
        legend.position = "bottom", 
        legend.title = element_blank())
  
```

***

#NN Task
###With individual prod points
```{r}
all.data %>%
  filter(Task == "WCN")%>%
  group_by(SID, Productive)%>%
  summarise(mean = mean(as.numeric(as.character(Correct), na.rm=TRUE)),
            sd = sd(as.numeric(as.character(Correct), na.rm=TRUE))) %>%
  ggplot(aes(x = Productive, y = mean, fill=Productive)) +
  geom_violin(aes(fill = Productive, colour = Productive), alpha = .1, size = 1) +
  geom_point(aes(x = Productive, y = mean, colour = Productive),
               position=position_jitter(width=0.15,height=0.02),
               size=1.5,
             alpha = .9,
               show.legend=TRUE, 
             inherit.aes = FALSE) +
  stat_summary(fun.data = mean_se, geom="errorbar", 
               position = position_dodge(width=0.90), width = 0.1, size = 1.5)+
  ylab("Mean Next Number performance") + 
  xlab('') + 
  theme_bw(base_size = 13) + 
  theme(legend.position = "bottom") +
  theme(panel.grid.minor = element_blank(), 
        panel.grid.major = element_blank(), 
        legend.title = element_blank()) +
  ylim(0, 1.0) +
  scale_fill_manual(values = productivity.pal, guide = "none") +
  scale_colour_manual(values = productivity.pal, guide ="none")
```

##By productivity and count range
```{r}
all.data %>%
  filter(Task == "WCN")%>%
  group_by(SID, count_range, Productive)%>%
  summarise(mean = mean(as.numeric(as.character(Correct)), na.rm=TRUE),
            sd = sd(as.numeric(as.character(Correct)), na.rm=TRUE)) %>%
ggplot(aes(x=count_range, y=mean, fill = Productive)) +
  geom_violin(aes(colour = Productive, fill = Productive), alpha = .1, size = 1, 
              show.legend = FALSE)  + 
  geom_point(position=position_jitterdodge(jitter.width = .3, 
                                           jitter.height = .1, 
                                           dodge.width = .9),
             aes(colour = Productive, group=Productive), 
             size=1.5,
             alpha = .9,
               show.legend=TRUE) +
  stat_summary(fun.data = mean_se, geom="errorbar", 
               position = position_dodge(width=0.90), width = 0.2, 
               show.legend = FALSE, size = 1) +
  ylab("Mean Next Number performance") + 
  xlab('') + 
  theme_bw(base_size = 13) + 
  theme(legend.position = "bottom") + 
  ylim(0, 1.0) +
  scale_fill_manual(values = productivity.pal) + 
  scale_colour_manual(values = productivity.pal) +
  theme(panel.grid.minor = element_blank(), 
        panel.grid.major = element_blank(), 
        legend.position = "bottom", 
        legend.title = element_blank())
```

***

#Math Facts
##With individual prod points
```{r}
all.data %>%
  filter(Task == "MF")%>%
  group_by(SID, Productive)%>%
  summarise(mean = mean(as.numeric(as.character(Correct), na.rm=TRUE)),
            sd = sd(as.numeric(as.character(Correct), na.rm=TRUE))) %>%
  ggplot(aes(x = Productive, y = mean, fill=Productive)) +
  geom_violin(aes(fill = Productive, colour = Productive), alpha = .1, size = 1) +
  geom_point(aes(x = Productive, y = mean, colour = Productive),
               position=position_jitter(width=0.15,height=0.02),
               size=1.5,
             alpha = .9,
               show.legend=TRUE, 
             inherit.aes = FALSE) +
  stat_summary(fun.data = mean_se, geom="errorbar", 
               position = position_dodge(width=0.90), width = 0.1, size = 1.5)+
  ylab("Mean Math Facts performance") + 
  xlab('') + 
  theme_bw(base_size = 13) + 
  theme(legend.position = "bottom") +
  theme(panel.grid.minor = element_blank(), 
        panel.grid.major = element_blank(), 
        legend.title = element_blank()) +
  ylim(0, 1.0) +
  scale_fill_manual(values = productivity.pal, guide = "none") +
  scale_colour_manual(values = productivity.pal, guide ="none")
```

##By productivity and count range
```{r}
all.data %>%
  filter(Task == "MF")%>%
  group_by(SID, count_range, Productive)%>%
  summarise(mean = mean(as.numeric(as.character(Correct)), na.rm=TRUE),
            sd = sd(as.numeric(as.character(Correct)), na.rm=TRUE)) %>%
ggplot(aes(x=count_range, y=mean, fill = Productive)) +
  geom_violin(aes(colour = Productive, fill = Productive), alpha = .1, size = 1, 
              show.legend = FALSE)  + 
  geom_point(position=position_jitterdodge(jitter.width = .3, 
                                           jitter.height = .1, 
                                           dodge.width = .9),
             aes(colour = Productive, group=Productive), 
             size=1.5,
             alpha = .9,
               show.legend=TRUE) +
  stat_summary(fun.data = mean_se, geom="errorbar", 
               position = position_dodge(width=0.90), width = 0.2, 
               show.legend = FALSE, size = 1) +
  ylab("Mean Math Facts performance") + 
  xlab('') + 
  theme_bw(base_size = 13) + 
  theme(legend.position = "bottom") + 
  ylim(0, 1.0) +
  scale_fill_manual(values = productivity.pal) + 
  scale_colour_manual(values = productivity.pal) +
  theme(panel.grid.minor = element_blank(), 
        panel.grid.major = element_blank(), 
        legend.position = "bottom", 
        legend.title = element_blank())
  
```

***

#Indefinite
##With individual prod points
NB, this does not include all participants.
```{r}
all.data %>%
  filter(Task == "Indefinite")%>%
  group_by(SID, Productive)%>%
  summarise(mean = mean(as.numeric(as.character(Correct), na.rm=TRUE)),
            sd = sd(as.numeric(as.character(Correct), na.rm=TRUE))) %>%
  ggplot(aes(x = Productive, y = mean, fill=Productive)) +
  geom_violin(aes(fill = Productive, colour = Productive), alpha = .1, size = 1) +
  geom_point(aes(x = Productive, y = mean, colour = Productive),
               position=position_jitter(width=0.15,height=0.02),
               size=1.5,
             alpha = .9,
               show.legend=TRUE, 
             inherit.aes = FALSE) +
  stat_summary(fun.data = mean_se, geom="errorbar", 
               position = position_dodge(width=0.90), width = 0.1, size = 1.5)+
  ylab("Mean Indefinite performance") + 
  xlab('') + 
  theme_bw(base_size = 13) + 
  theme(legend.position = "bottom") +
  theme(panel.grid.minor = element_blank(), 
        panel.grid.major = element_blank(), 
        legend.title = element_blank()) +
  ylim(0, 1.0) +
  scale_fill_manual(values = productivity.pal, guide = "none") +
  scale_colour_manual(values = productivity.pal, guide ="none")
```

***

#Unit, NN, and MF together
##With individual prod points
```{r}


#now for just kids at ceiling
tmp <- all.data %>%
  mutate(sf.quartile = cut(mean.unit, 
                                breaks=quantile(mean.unit, na.rm=TRUE), 
                                include.lowest=TRUE))%>%
  mutate(mf.ceiling = ifelse(mean.mf >= .75, ">= 75% correct", "< 75% correct"))


tmp %>%
    filter(Task == "SF" |
           Task == "WCN" |
           Task == "MF", 
           sf.quartile == "(0.875,1]")%>%
  mutate(Task = factor(Task, levels = c("SF", "WCN", "MF"), labels = c("Unit Task", "Next Number", "Math Facts")))%>%
  group_by(SID, Task)%>%
  summarise(mean = mean(as.numeric(as.character(Correct), na.rm=TRUE)),
            sd = sd(as.numeric(as.character(Correct), na.rm=TRUE))) %>%
  ggplot(aes(x = Task, y = mean, fill=Task)) +
  stat_summary(fun.y = mean, position = position_dodge(width = .95),
                      geom="bar", alpha = .5, colour = "black") +
  # geom_violin(alpha = .5) +
  geom_point(aes(x = Task, y = mean, colour = Task),
               position=position_jitter(width = .18, height = .035),
               size=1.5,
               show.legend=TRUE, 
             inherit.aes = FALSE) +
  stat_summary(fun.data = mean_se, geom="errorbar", 
               position = position_dodge(width=0.90), width = 0.1, size = 1)+
  ylab("Mean task performance") + 
  xlab('Task') + 
  theme_bw(base_size = 13) + 
  theme(legend.position = "bottom") +
  theme(panel.grid.minor = element_blank(), 
        panel.grid.major = element_blank(), 
        legend.title = element_blank()) +
  ylim(0, 1.0) +
  scale_fill_manual(values = three.pal, guide = "none") +
  scale_colour_manual(values = three.pal, guide = "none")

ggsave("~/Documents/Projects/sf_math/Analysis/all_tasks_ceiling.png", width = 5.25, height = 4.5)
```

##By productivity and count range (this needs a little work)
```{r}
all.data %>%
  filter(Task == "SF" |
           Task == "WCN" |
           Task == "MF")%>%
  group_by(SID, count_range, Task, Productive)%>%
  summarise(mean = mean(as.numeric(as.character(Correct)), na.rm=TRUE),
            sd = sd(as.numeric(as.character(Correct)), na.rm=TRUE)) %>%
ggplot(aes(x=count_range, y=mean, fill = Productive)) +
  stat_summary(fun.y = mean, position = position_dodge(width = .95), 
                      geom="bar", alpha = .5, colour = "black") +
  geom_violin(alpha = .1, 
              show.legend = FALSE)  + 
  # geom_point(position=position_jitterdodge(jitter.width = .3, 
  #                                          jitter.height = .1, 
  #                                          dodge.width = .9),
  #            aes(colour = Productive, group=Productive), 
  #            size=1.5,
  #            alpha = .9,
  #              show.legend=TRUE) +
  stat_summary(fun.data = mean_se, geom="errorbar", 
               position = position_dodge(width=0.90), width = 0.2, 
               show.legend = FALSE) +
  ylab("Mean correct") + 
  xlab('') + 
  theme_bw(base_size = 10) + 
  theme(legend.position = "bottom") + 
  theme(text = element_text(size = 10)) +
  ylim(0, 1.0) +
  scale_fill_brewer(palette = "Dark2") + 
  # scale_colour_manual(values = productivity.pal) +
  theme(text = element_text(size = 12), 
        panel.grid.minor = element_blank(), 
        panel.grid.major = element_blank(), 
        legend.position = "bottom", 
        legend.title = element_blank()) + 
  facet_grid(~Task)
  
```

#Mean performance by item by task
```{r}
##For all tasks, by item
all.data %>%
  filter(Task == "MF" | 
         Task == "WCN" | 
           Task == "SF")%>%
  mutate(Task_item = factor(Task_item, levels = c("5", "6", "7", "15", "20", 
                                                  "21", "24", "26", "30", "32", 
                                                  "34", "46", "51", "57", "60", "62", "64", 
                                                  "71", "73", "81", "83", "84", "86",
                                                  "93", "95")), 
         Task = factor(Task, levels = c("SF", "WCN", "MF"), labels = c("Unit", 
                                                                       "Next Number", 
                                                                       "Math Facts")))%>%
  group_by(Task, Task_item)%>%
  summarise(mean = mean(Correct, na.rm = TRUE), 
            n = n(), 
            sd = sd(Correct, na.rm = TRUE), 
            se = sd/sqrt(n)) %>%
  ggplot(aes(x = Task_item, y = mean, colour = Task, group = Task)) +
  geom_point(size = 2) + 
  geom_line() +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), 
                width = .1) +
  theme_bw(base_size = 10) + 
  facet_grid(~Task, scale = "free_x") +
  # scale_colour_manual(values = productivity.pal) +
  theme(legend.position = "bottom", 
        legend.title = element_blank()) +
  labs(x = "Number queried", y = "Mean performance") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  
  
```


##Mean Unit task performance by HCNN
```{r}
all.data %>%
  filter(Task == "SF")%>%
  mutate(highest_contig = factor(highest_contig, levels = c("0", "1", "7", "24", "26", "30", 
                                                             "62", "71", "83", "95")))%>%
  group_by(highest_contig)%>%
  summarise(mean = mean(Correct, na.rm = TRUE), 
            n = n(), 
            sd = sd(Correct, na.rm = TRUE), 
            se = sd/sqrt(n)) %>%
  ggplot(aes(x = highest_contig, y = mean, group= highest_contig)) +
  geom_point(size = 2) + 
  geom_line() +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), 
                width = .1) +
  theme_bw(base_size = 10) + 
  langcog::scale_color_solarized() +
  theme(legend.position = "bottom", 
        legend.title = element_blank()) +
  labs(x = "Highest contiguous NN", y = "Mean Unit Task performance") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

##Unit task by mean MF
```{r}
all.data %>%
  filter(Task == "SF")%>%
  group_by(SID, mean.mf)%>%
  summarise(mean_unit = mean(Correct, na.rm = TRUE))%>%
  ggplot(aes(x = mean.mf, y = mean_unit)) +
  geom_point() + 
  geom_smooth(method = "lm") + 
  geom_jitter()
```


#Correlations
```{r}
sub.dat <- all.data %>%
  distinct(SID, Age, IHC, FHC, mean.unit, mean.mf, mean.nn, highest_contig, highest_num)%>%
  dplyr::select(-SID)%>%
  mutate(highest_contig = as.numeric(highest_contig))
  
library(corrplot)

M<-cor(sub.dat)
head(round(M,2))

#add p values
cor.mtest <- function(mat, ...) {
    mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat<- matrix(NA, n, n)
    diag(p.mat) <- 0
    for (i in 1:(n - 1)) {
        for (j in (i + 1):n) {
            tmp <- cor.test(mat[, i], mat[, j], ...)
            p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
        }
    }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}
# matrix of the p-value of the correlation
p.mat <- cor.mtest(sub.dat)
head(p.mat[, 1:8])

corrplot(M, type="upper", order="hclust", method = "color",
         addCoef.col = "white",
         tl.col="black", tl.srt=45,
         p.mat = p.mat, sig.level = 0.01)

```

---



---








##Attempting to visualize these models  
```{r}
# library(dotwhisker)
#visualizing regressions
full.model <- summary(sf.large.plusmf)

AIC <- as.numeric(full.model$AICtab[1])

full.model.df <- data.frame(coef(full.model)[,0:4])
full.model.df <- add_rownames(full.model.df, "Parameter")

full.model.df %<>%
  mutate(Parameter = ifelse(Parameter == "highest_contig.c", "HCNN",
                            ifelse(Parameter == "fhc.c", "FHC",
                                   ifelse(Parameter == "count_rangeOutside", "Beyond IHC",
                                          ifelse(Parameter == "age.c", "Age", 
                                                 ifelse(Parameter == "mean.mf.c", "Math Facts", "Intercept"))))))%>%
  mutate(Parameter = factor(Parameter, levels = c("Age","Beyond IHC", "Math Facts", "FHC", "HCNN", "Intercept"), 
                             labels = c("Age", "Within Initial Count",
                                        "Math Facts",  "Final Highest Count", "Highest Contig. Next Number",
                                         "Intercept")))

full.model.df %>%
ggplot(aes(x = Parameter, y = Estimate)) +
  geom_pointrange(aes(ymin = Estimate - 1.96 * Std..Error,
                      ymax = Estimate + 1.96 * Std..Error,
                      colour = Parameter), size = .85) +
  geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
  theme_bw(base_size = 13.5) +
  theme(panel.grid = element_blank(), 
        legend.position = "none") +
  coord_flip() +
  langcog::scale_color_solarized() +
  xlab("") +
  scale_y_continuous(name = "Coefficient Estimate (log likelihood)")
ggsave("~/Documents/Projects/sf_math/Analysis/coefficients.png", width = 5.75, height = 3.25)
```



#Post-hoc: SF and MF comparison


##Mean performance in each task by Unit Task quartile (SF, MF, NN)
```{r}


```

#Accuracy by Task - collapsed across all counters
```{r}
model.tasks.df <- tmp %>%
  filter(Task == "SF" | 
           Task == "WCN" | 
           Task == "MF") %>%
   mutate(highest_contig = as.integer(highest_contig), 
          Task = factor(Task, levels = c("SF", "WCN", "MF")),
         SID = factor(SID), 
         age.c = as.vector(scale(Age, center = TRUE, scale=TRUE)), #scale and center continuous variables for model fit
         fhc.c = as.vector(scale(FHC, center = TRUE, scale = TRUE)), 
         ihc.c = as.vector(scale(IHC, center = TRUE, scale = TRUE)),
         highest_contig.c = as.vector(scale(highest_contig, center = TRUE, scale=TRUE)), 
         # highest_num.c = as.vector(scale(highest_num, center = TRUE, scale=TRUE)),
         mean.mf.c = as.vector(scale(mean.mf, center = TRUE, scale=TRUE)),
         starting_num.c = as.vector(scale(as.numeric(as.character(Task_item)), center = TRUE, scale = TRUE)))

#LOOK AT JUST CEILING
#base model 
model.tasks.base <- glmer(Correct ~ count_range + age.c + (1|SID), family = "binomial",
                         data = subset(model.tasks.df, sf.quartile == "(0.875,1]"))

#does performance significant differ by task? 
model.tasks.1 <- glmer(Correct ~ Task + count_range + age.c + (1|SID), family = "binomial", 
                      data = subset(model.tasks.df, sf.quartile == "(0.875,1]"))

#compare
anova(model.tasks.base, model.tasks.1, test= 'LRT') #yes, tasks adds

summary(model.tasks.1)

#Now look at all kids
  #base model 
  model.tasks.base.all <- glmer(Correct ~ count_range + age.c + (1|SID), family = "binomial",
                           data = model.tasks.df)
  
  #does performance significant differ by task? 
  model.tasks.1.all <- glmer(Correct ~ Task + count_range + age.c + (1|SID), family = "binomial", 
                        data = model.tasks.df)
  
  #compare
  anova(model.tasks.base.all, model.tasks.1.all, test= 'LRT') #yes, tasks adds
  
  summary(model.tasks.1.all)

```

##For top quartile, t-test of mean MF and SF performance
```{r}
#for top quantile, t-test of mf and sf performance
ms <- tmp %>%
  group_by(SID, sf.quartile, Task)%>%
  summarise(mean = mean(Correct, na.rm = TRUE))%>%
  filter(sf.quartile == '(0.875,1]')

t.test(subset(ms, sf.quartile == '(0.875,1]' & Task == "MF")$mean, 
       subset(ms, sf.quartile == '(0.875,1]' & Task == "SF")$mean, var.equal = TRUE)
```

##T-test between mean SF and NN performance for top quartile
```{r}
#what about for next number?
ms <- tmp %>%
  group_by(SID, sf.quartile, Task)%>%
  summarise(mean = mean(Correct, na.rm = TRUE))%>%
  filter(sf.quartile == '(0.875,1]')

t.test(subset(ms, sf.quartile == '(0.875,1]' & Task == "WCN")$mean, 
       subset(ms, sf.quartile == '(0.875,1]' & Task == "SF")$mean, var.equal = TRUE)
```

##Exploratory - does mean indefinite performance predict SF? 
```{r}

task.pal2 <- c("#2aa198")
               
indef.model <- model.df %>%
  filter(!is.na(mean.indef.c))

indef.base <- glmer(Correct ~ mean.mf.c + highest_contig.c + count_range + age.c + (1|SID), 
                    family = "binomial", data = indef.model)

indef.indef <- glmer(Correct ~ mean.indef.c + mean.mf.c + highest_contig.c + count_range + age.c + (1|SID), 
                     family = "binomial", data = indef.model)

#compare
anova(indef.base, indef.indef, test = 'LRT')

summary(indef.indef)

##visualize
indef.model %>%
  group_by(SID, mean.indef)%>%
  summarise(mean.sf = mean(Correct, na.rm = TRUE))%>%
  ggplot(aes(x = mean.indef, y = mean.sf)) + 
  geom_point(position = position_jitter(width = .01, height = .01)) + 
  geom_smooth() +
  theme_bw()

#making the graph I want
tmp.df <- all.data %>%
  dplyr::select(SID, Age, Task, Correct)%>%
  filter(Task == "SF" | 
           Task == "MF" | 
           Task == "WCN")%>%
  group_by(SID, Task)%>%
  summarise(Mean = mean(Correct, na.rm = TRUE))

mean.indef <- all.data %>%
  filter(Task == "Indefinite")%>%
  group_by(SID)%>%
  summarise(mean.indef = mean(Correct))

ms.tmp <- right_join(tmp.df, mean.indef, by = "SID")

all.data %>%
  filter(!is.na(mean.indef))%>%
  filter(Task == "SF")%>%
  mutate(indef.bin = ifelse((mean.indef <.25 & mean.indef <= .5), "0% — 50%", "50% — 100"))%>%
  group_by(SID, Task, indef.bin)%>%
  summarise(mean = mean(as.numeric(as.character(Correct), na.rm=TRUE)),
            sd = sd(as.numeric(as.character(Correct), na.rm=TRUE))) %>%
  ggplot(aes(x = indef.bin, y = mean, fill=Task)) +
  geom_violin(aes(fill = Task), alpha = .4) + 
  geom_point(aes(x = indef.bin, y = mean, colour = Task),
               position=position_jitter(width=0.15,height=0.02),
               size=1.5,
               show.legend=TRUE, 
             inherit.aes = FALSE) +
  stat_summary(fun.data = mean_se, geom="errorbar", 
               position = position_dodge(width=0.90), width = 0.1, size = 1)+
  ylab("Mean Unit Task performance") + 
  xlab('Mean Indefinite Next Number performance') + 
  theme_bw(base_size = 15) + 
  theme(legend.position = "right") +
  theme(panel.grid.minor = element_blank(), 
        panel.grid.major = element_blank(), 
        legend.position = "none") +
  ylim(0, 1.0) +
  scale_fill_manual(values = "#091E5B") + 
  scale_color_manual(values = "#091E5B") + 
  coord_fixed(ratio = 1.8)

ggsave("~/Documents/Projects/sf_math/Analysis/indef.png")
```

##Post-hoc: Predicting mean performance by task, controlling for age and starting number
For children who are at ceiling in SF task, is there evidence that they draw upon one source of information more strongly than the other? If they generalize the SF from '+1' operation and from productive counting knowledge, then for children at ceiling there should no difference between these two tasks. If children weight one source of information more strongly, however, we should find a significant effect of task when controlling for starting number and age.
```{r}
ms1 <- tmp %>%
  filter(sf.quartile == "(0.875,1]", 
         Task=="MF" | 
           Task == "WCN")%>%
  mutate(starting_num.c = as.vector(scale(as.numeric(as.character(Task_item)), center = TRUE, scale = TRUE)))

summary(lmer(Correct ~ Task + starting_num.c + Age + (1|SID), 
              data = ms1))
```


```{r, include = FALSE}
indef.model <- model.df %>%
  filter(!is.na(mean.indef))

sf.indef.base <- glmer(Correct ~ count_range + age.c + (1|SID), 
                       family = "binomial", data = indef.model)
sf.indef <- glmer(Correct ~ mean.indef + count_range + age.c + (1|SID), 
                  family = "binomial", data = indef.model)

#test
anova(sf.indef.base, sf.indef, test = 'LRT')

#what about FHC and HCNN
sf.indef.fhc <- glmer(Correct ~ fhc.c + count_range + age.c + (1|SID), 
                  family = "binomial", data = indef.model)
anova(sf.indef.base, sf.indef.fhc, test = 'LRT')

sf.indef.highest_contig <- glmer(Correct ~ highest_contig.c + count_range + age.c + (1|SID), 
                  family = "binomial", data = indef.model)
anova(sf.indef.base, sf.indef.highest_contig, test = 'LRT')

#does HCNN add to this - yes
sf.indef.highest_contig.indef <- glmer(Correct ~ mean.indef + highest_contig.c + count_range + age.c + (1|SID), 
                  family = "binomial", data = indef.model)
anova(sf.indef.highest_contig, sf.indef.highest_contig.indef, test = 'LRT')

#does fhc add to this
sf.indef.highest_contig.indef.fhc <- glmer(Correct ~ fhc.c + mean.indef + highest_contig.c + count_range + age.c + (1|SID), 
                  family = "binomial", data = indef.model)
anova(sf.indef.highest_contig.indef, sf.indef.highest_contig.indef.fhc, test = 'LRT') #yes

#what about the addition of math facts
#individual model
sf.indef.mf <- glmer(Correct ~ mean.mf + count_range + age.c + (1|SID), 
                     family = "binomial", data = indef.model)
anova(sf.indef.base, sf.indef.mf, test = 'LRT')

#add to large model
sf.indef.highest_contig.indef.fhc.mf <- glmer(Correct ~ mean.mf + fhc.c + mean.indef + highest_contig.c + count_range + age.c + (1|SID), 
                  family = "binomial", data = indef.model)
anova(sf.indef.highest_contig.indef.fhc, sf.indef.highest_contig.indef.fhc.mf, test = 'LRT')
```



